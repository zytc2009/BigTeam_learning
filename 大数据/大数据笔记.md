[Toc]

## 概述

**大数据处理的主要应用场景包括数据分析、数据挖掘与机器学习**。数据分析主要使用 Hive、Spark SQL 等 SQL 引擎完成；数据挖掘与机器学习则有专门的机器学习框架 TensorFlow、Mahout 以及 MLlib 等，内置了主要的机器学习和数据挖掘算法。

![](bigdata\大数据平台.png)

## 大数据原理和架构

### 大规模数据存储核心问题

1. **数据存储容量的问题**。既然大数据要解决的是数以 PB 计的数据计算问题，而一般的服务器磁盘容量通常 1～2TB，那么如何存储这么大规模的数据呢？

2. **数据读写速度的问题**。一般磁盘的连续读写速度为几十 MB，以这样的速度，几十 PB 的数据恐怕要读写到天荒地老。
3. **数据可靠性的问题**。磁盘大约是计算机设备中最易损坏的硬件了，通常情况一块磁盘使用寿命大概是一年，如果磁盘损坏了，数据怎么办？

### HDFS

**HDFS 也许不是最好的大数据存储技术，但依然最重要的大数据存储技术**。

![](bigdata\HDFS的架构图.jpg)

HDFS 的**关键组件**有两个，一个是 DataNode，一个是 NameNode。

**DataNode** 负责文件数据的存储和读写操作，HDFS 将文件数据分割成若干数据块（Block），每个 DataNode 存储一部分数据块，这样文件就分布存储在整个 HDFS 服务器集群中。应用程序客户端（Client）可以并行对这些数据块进行访问，从而使得 HDFS 可以在服务器集群规模上实现数据并行访问，极大地提高了访问速度。

**NameNode** 负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名、数据块的 ID 以及存储位置等信息，相当于操作系统中文件分配表（FAT）的角色。

HDFS 一般的访问模式是通过 **MapReduce** 程序在计算时读取，MapReduce 对输入数据进行分片读取，通常一个分片就是一个数据块，每个数据块分配一个计算进程

#### HDFS 的高可用设计:

1. 数据存储故障容错

   磁盘介质在存储过程中受环境或者老化影响，其存储的数据可能会出现错乱。HDFS 的应对措施是，对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum）。在读取数据的时候，重新计算读取出来的数据的校验和，如果校验不正确就抛出异常，应用程序捕获异常后就到其他 DataNode 上读取备份数据。

2. 磁盘故障容错如果 

   DataNode 监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有 BlockID 报告给 NameNode，NameNode 检查这些数据块还在哪些 DataNode 上有备份，通知相应的 DataNode 服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求。

3. DataNode 故障容错

   DataNode 会通过心跳和 NameNode 保持通信，如果 DataNode 超时未发送心跳，NameNode 就会认为这个 DataNode 已经宕机失效，立即查找这个 DataNode 上存储的数据块有哪些，以及这些数据块还存储在哪些服务器上，随后通知这些服务器再复制一份数据块到其他服务器上，保证 HDFS 存储的数据块备份数符合用户设置的数目，即使再出现服务器宕机，也不会丢失数据。

4. NameNode 故障容错

   NameNode 是整个 HDFS 的核心，记录着 HDFS 文件分配表信息，所有的文件路径和数据块存储信息都保存在 NameNode，如果 NameNode 故障，整个 HDFS 系统集群都无法使用；如果 NameNode 上记录的数据丢失，整个集群所有 DataNode 存储的数据也就没用了。

NameNode 采用主从热备的方式提供高可用容错服务:

![](bigdata\NameNode高可用容错.png)

常用的保证系统可用性的策略有**冗余备份、失效转移和降级限流**

当要访问的程序或者数据无法访问时，需要将访问请求转移到备份的程序或者数据所在的服务器上，这也就是**失效转移**。失效转移你应该注意的是失效的鉴定，避免集群混乱。

当大量的用户请求或者数据处理请求到达的时候，可以拒绝部分请求，即进行限流；也可以关闭部分功能，降低资源消耗，即进行降级。

### MapReduce

**MapReduce 既是一个编程模型，又是一个计算框架**

词频统计：（python处理）

```python
# 文本前期处理
strl_ist = str.replace('\n', '').lower().split(' ')
count_dict = {}
# 如果字典里有该单词则加1，否则添加入字典
for str in strl_ist:
	if str in count_dict.keys():
    	count_dict[str] = count_dict[str] + 1
    else:
        count_dict[str] = 1
```

MapReduce：

```java
public class WordCount {
  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
}
```

#### MapReduce 作业启动和运行机制

MapReduce 运行过程涉及三类关键进程:

1. **大数据应用进程**。这类进程是启动 MapReduce 程序的主入口，主要是指定 Map 和 Reduce 类、输入输出文件路径等，并提交作业给 Hadoop 集群，也就是下面提到的 JobTracker 进程。这是由用户启动的 MapReduce 程序进程，比如我们上期提到的 WordCount 程序。

2. **JobTracker 进程**。这类进程根据要处理的输入数据量，命令下面提到的 TaskTracker 进程启动相应数量的 Map 和 Reduce 进程任务，并管理整个作业生命周期的任务调度和监控。这是 Hadoop 集群的常驻进程，需要注意的是，JobTracker 进程在整个 Hadoop 集群全局唯一。

3. **TaskTracker 进程**。这个进程负责启动和管理 Map 进程以及 Reduce 进程。因为需要每个数据块都有对应的 map 函数，TaskTracker 进程通常和 HDFS 的 DataNode 进程启动在同一个服务器。也就是说，Hadoop 集群中绝大多数服务器同时运行 DataNode 进程和 TaskTracker 进程。

MapReduce 的主服务器就是 JobTracker，从服务器就是 TaskTracker. 一主多从可谓是大数据领域的最主要的架构模式.

计算过程:

![](bigdata\MapReduce启动运行流程.png)

1. 应用进程 JobClient 将用户作业 JAR 包存储在 HDFS 中，将来这些 JAR 包会分发给 Hadoop 集群中的服务器执行 MapReduce 计算。

2. 应用程序提交 job 作业给 JobTracker。
3. JobTracker 根据作业调度策略创建 JobInProcess 树，每个作业都会有一个自己的 JobInProcess 树。
4. JobInProcess 根据输入数据分片数目（通常情况就是数据块的数目）和设置的 Reduce 数目创建相应数量的 TaskInProcess。
5. TaskTracker 进程和 JobTracker 进程进行定时通信。
6. 如果 TaskTracker 有空闲的计算资源（有空闲 CPU 核心），JobTracker 就会给它分配任务。分配任务的时候会根据 TaskTracker 的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据，以实现我们一开始就提到的“移动计算比移动数据更划算”。
7. TaskTracker 收到任务后根据任务类型（是 Map 还是 Reduce）和任务参数（作业 JAR 包路径、输入数据文件路径、要处理的数据在文件中的起始位置和偏移量、数据块多个备份的 DataNode 主机名等），启动相应的 Map 或者 Reduce 进程。
8. Map 或者 Reduce 进程启动后，检查本地是否有要执行任务的 JAR 包文件，如果没有，就去 HDFS 上下载，然后加载 Map 或者 Reduce 代码开始执行。
9. 如果是 Map 进程，从 HDFS 读取数据（通常要读取的数据块正好存储在本机）；如果是 Reduce 进程，将结果数据写出到 HDFS。

#### MapReduce 数据合并与连接机制

在 map 输出与 reduce 输入之间一般会有shuffle。

每个 Map任务快要计算完成的时候，MapReduce 计算框架会启动 shuffle 过程，在 Map 任务进程调用一个 Partitioner 接口，对 Map 产生的每个 <Key, Value> 进行 Reduce 分区选择，然后通过 HTTP 通信发送给对应的 Reduce 进程.

**分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是 shuffle**。

不管是 MapReduce 还是 Spark，只要是大数据批处理计算，一定都会有 shuffle 过程，只有让数据关联起来

### 资源调度框架Yarn

Hadoop 主要是由三部分组成，分布式文件系统 HDFS、分布式计算框架 MapReduce，分布式集群资源调度框架 Yarn

Yarn  - “Yet Another Resource Negotiator”

架构图：

![](bigdata\Yarn的架构.jpg)

Yarn 包括两个部分：一个是**资源管理器**（Resource Manager），一个是**节点管理器**（Node Manager）；资源管理器又包括两个主要组件：**调度器和应用程序管理器**。

**调度器**其实就是一个资源分配算法，根据应用程序（Client）提交的资源申请和当前服务器集群的资源状况进行资源分配。Yarn 内置了几种**资源调度算法**，包括 Fair Scheduler、Capacity Scheduler 等，你也可以开发自己的资源调度算法供 Yarn 调用。

Yarn 进行资源分配的单位是**容器**（Container），每个容器包含了一定量的内存、CPU 等计算资源，默认配置下，每个容器包含一个 CPU 核心。**容器**由 **NodeManager** 进程**启动和管理**，**NodeManger** 进程会监控本节点上容器的运行状况并**向 ResourceManger 进程汇报**。

**应用程序**启动后需要在集群中运行一个 **ApplicationMaster**，ApplicationMaster 也需要运行在容器里面。每个应用程序启动后都会先启动自己的 ApplicationMaster，由 ApplicationMaster 根据应用程序的资源需求进一步向 ResourceManager 进程申请容器资源，得到容器以后就会分发自己的应用程序代码到容器上启动，进而开始分布式计算。

**Yarn 的整个工作流程**：

1. 我们向 Yarn 提交应用程序，包括 MapReduce ApplicationMaster、我们的 MapReduce 程序，以及 MapReduce Application 启动命令。

2. ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个容器，并将 MapReduce ApplicationMaster 分发到这个容器上面，并在容器里面启动 MapReduce ApplicationMaster。

3. MapReduce ApplicationMaster 启动后立即向 ResourceManager 进程注册，并为自己的应用程序申请容器资源。

4. MapReduce ApplicationMaster 申请到需要的容器后，立即和相应的 NodeManager 进程通信，将用户 MapReduce 程序分发到 NodeManager 进程所在服务器，并在容器中运行，运行的就是 Map 或者 Reduce 任务。

5. Map 或者 Reduce 任务在运行期和 MapReduce ApplicationMaster 通信，汇报自己的运行状态，如果运行结束，MapReduce ApplicationMaster 向 ResourceManager 进程注销并释放所有的容器资源。

**框架遵循“依赖倒转原则”，依赖倒转原则是高层模块不能依赖低层模块，它们应该共同依赖一个抽象，这个抽象由高层模块定义，由低层模块实现。**

![](bigdata\一次互联网请求的旅程.png)



#### Hive 的架构

![](bigdata\hive架构.jpg)

通过 Hive 的 Client（Hive 的命令行工具，JDBC 等）向 Hive 提交 SQL 命令。如果是**创建数据表**的 DDL（数据定义语言），Hive 就会通过执行引擎 Driver 将数据表的信息记录在 Metastore 元数据组件中，这个组件通常用一个关系数据库实现，记录表名、字段名、字段类型、关联 HDFS 文件路径等这些数据库的 Meta 信息（元信息）。

如果是**查询分析数据**的 DQL（数据查询语句），Driver 就会将该语句提交给自己的编译器 Compiler 进行语法分析、语法解析、语法优化等一系列操作，最后生成一个 MapReduce 执行计划。然后根据执行计划生成一个 MapReduce 的作业，提交给 Hadoop MapReduce 计算框架处理。

Hive 内部预置了很多函数，Hive 的执行计划就是根据 SQL 语句生成这些函数的 DAG（有向无环图），然后封装进 MapReduce 的 map 和 reduce 函数中。

**Hive 如何实现 join 操作**：

![](bigdata\Hive如何实现join操作.jpg)

### Spark

除了速度更快，Spark 和 MapReduce 相比，还有更简单易用的编程模型

```scala
#字数统计 RDD
val textFile = sc.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")
```

RDD 是 Spark 的核心概念，是弹性数据集（Resilient Distributed Datasets）的缩写。RDD 既是 Spark 面向开发者的编程模型，又是 Spark 自身架构的核心元素。

MapReduce面向过程，Spark 面向对象的大数据计算

RDD 定义了很多转换操作函数，比如有计算 map(func)、过滤 filter(func)、合并数据集 union(otherDataset)、根据 Key 聚合 reduceByKey(func, [numPartitions])、连接数据集 join(otherDataset, [numPartitions])、分组 groupByKey([numPartitions]) 等十几个函数。

转换操作又分成两种，一种转换操作产生的 RDD 不会出现新的分片，比如 map、filter 等，另一种转换操作产生的 RDD 则会产生新的分片，比如reduceByKey。只有在产生新的 RDD 分片时候，才会真的生成一个 RDD，Spark 的这种特性也被称作**惰性计算**。

Spark生态体系:

![](bigdata\spark生态体系.png)

计算阶段:

![](bigdata\Spark运行DAG的不同阶段.png)

```scala
rddB = rddA.groupBy(key)
rddD = rddC.map(func)
rddF = rddD.union(rddE)
rddG = rddB.join(rddF)
```

Spark 作业调度执行的核心是 DAG, 负责 Spark 应用 DAG 生成和管理的组件是 DAGScheduler，DAGScheduler 根据程序代码生成 DAG，然后将程序分发到分布式计算集群，按计算阶段的先后关系调度执行。

**计算阶段划分的依据是 shuffle，不是转换函数的类型**

#### Spark 的作业管理

Spark 里面的 RDD 函数有两种，一种是转换函数，调用以后得到的还是一个 RDD，RDD 的计算逻辑主要通过转换函数完成。另一种是 action 函数，调用以后不再返回 RDD。比如 count() 函数，返回 RDD 中数据的元素个数；saveAsTextFile(path)，将 RDD 数据存储到 path 路径下。

Spark 的 DAGScheduler 在遇到 shuffle 的时候，会生成一个计算阶段，在遇到 action 函数的时候，会生成一个作业（job）。

RDD 里面的每个数据分片，Spark 都会创建一个计算任务去处理，所以一个计算阶段会包含很多个计算任务（task）。

DAGScheduler 根据代码生成 DAG 图以后，Spark 的任务调度就以任务为单位进行分配，将任务分配到分布式集群的不同机器上执行。

#### Spark 的执行过程

![](bigdata\Spark的运行流程.png)

首先，Spark 应用程序启动在自己的 JVM 进程里，即 Driver 进程，启动后调用 SparkContext 初始化执行配置和输入数据。SparkContext 启动 DAGScheduler 构造执行的 DAG 图，切分成最小的执行单位也就是计算任务。

然后 Driver 向 Cluster Manager 请求计算资源，用于 DAG 的分布式计算。Cluster Manager 收到请求以后，将 Driver 的主机地址等信息通知给集群的所有计算节点 Worker。

Worker 收到信息以后，根据 Driver 的主机地址，跟 Driver 通信并注册，然后根据自己的空闲资源向 Driver 通报自己可以领用的任务数。Driver 根据 DAG 图开始向注册的 Worker 分配任务。

Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。

**三个主要特性**：**RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效**



### HBase

#### HBase 可伸缩架构

HBase 的伸缩性主要依赖其可分裂的 HRegion 及可伸缩的分布式文件系统 HDFS 实现。

![](bigdata\HBase可伸缩架构.png)

Hbase**调用时序图**:

![](bigdata\Hbase调用时序图.png)

#### HBase 可扩展数据模型

面向列族的稀疏矩阵存储格式. 把字段的名称和字段的值，以 Key-Value 的方式一起存储在 HBase 中

#### HBase 的高性能存储

HBase 使用了一种叫作 LSM 树的数据结构进行数据存储。LSM 树的全名是 Log Structed Merge Tree，翻译过来就是 Log 结构合并树。LSM 树可以看作是一个 N 阶合并树。

数据写操作（包括插入、修改、删除）都在内存中进行，并且都会创建一个新记录.  数据写入的时候以 Log 方式连续写入，然后异步对磁盘上的多个 LSM 树进行合并。

读操作时，总是从内存中的排序树开始搜索，如果没有找到，就从磁盘 上的排序树顺序查找。

### 大数据流计算框架Storm、Spark Streaming、Flink

#### Storm架构

![](bigdata\Storm架构.png)

nimbus 是集群的 Master，负责集群管理、任务分配等。supervisor 是 Slave，是真正完成计算的地方，每个 supervisor 启动多个 worker 进程，每个 worker 上运行多个 task，而 task 就是 spout 或者 bolt。supervisor 和 nimbus 通过 ZooKeeper 完成任务分配、心跳检测等操作。

#### Spark Streaming

Spark Streaming 巧妙地利用了 Spark 的分片和快速计算的特性，将实时传输进来的数据按照时间进行分段，把一段时间传输进来的数据合并在一起，当作一批数据，再去交给 Spark引擎去处理

#### Flink

Flink 既可以流处理又可以批处理。Flink 可以初始化一个流执行环境，构建数据流，也可以初始化一个批处理执行环境 ，构建数据集 DataSet。然后在 DataStream 或者 DataSet 上执行各种数据转换操作。

Flink 对流处理的支持更加完善，可以对数据流执行 window 操作，将数据流切分到一个一个的 window 里，进而进行计算。

Flink架构：

![](bigdata\Flink架构.png)

JobManager 是 Flink 集群的管理者，Flink 程序提交给 JobManager 后，JobManager 检查集群中所有 TaskManager 的资源利用状况，如果有空闲 TaskSlot（任务槽），就将计算任务分配给它执行。

### ZooKeeper

HDFS 为了保证整个集群的高可用，需要部署两台 NameNode 服务器，一台作为主服务器，一台作为从服务器。当主服务器不可用的时候，就切换到从服务器上访问。但是如果不同的应用程序（Client）或者 DataNode 做出的关于主服务器是否可用的判断不同，那么就会导致 HDFS 集群混乱。

#### 分布式一致性原理

**CAP 原理**：一个提供数据服务的分布式系统无法同时满足数据一致性（Consistency）、可用性（Availibility）、分区耐受性（Patition Tolerance）这三个条件。

**一致性**是说，每次读取的数据都应该是最近写入的数据或者返回一个错误（Every read receives the most recent write or an error），而不是过期数据

**可用性**是说，每次请求都应该得到一个响应，而不是返回一个错误或者失去响应，不过这个响应不需要保证数据是最近写入的（Every request receives a (non-error) response, without the guarantee that it contains the most recent write）

**分区耐受性**是说，即使因为网络原因，部分服务器节点之间消息丢失或者延迟了，系统依然应该是可以操作的（The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes）。

#### Paxos 算法与 ZooKeeper 架构

Paxos 算法在多台服务器通过内部的投票表决机制决定一个数据的更新与写入。

当有多个请求同时修改某个数据的情况下，服务器的表决机制保证只有一个请求会通过执行，从而保证了数据的一致性。

ZooKeeper 系统的多台服务器存储相同数据，并且每次数据更新都要所有服务器投票表决，所以和一般的分布式系统相反，ZooKeeper 集群的**性能会随着服务器数量的增加而下降**。

**区块链采取的解决方案是工作量证明**。一台服务器要想在分布式集群中记录数据（即所谓分布式记账），必须进行一个规模庞大的计算，比如计算一个 256 Bit 的 hash 值，这个值的前若干位必须为 0。这种工作量证明方式，保证了恶意服务器要想伪造篡改数据，必须拥有强大的计算能力（占整个集群服务器计算能力的 51% 以上）。

### Spark的性能优化

**软件的主要性能指标**包括：

响应时间：完成一次任务（请求）花费的时间。

并发数：同时处理的任务数（请求数）。

吞吐量：单位时间完成的任务数（请求数、事务数、查询数……）。

性能计数器：System Load，线程数，进程数，CPU、内存、磁盘、网络使用率等。

**性能优化的过程**是：

1.  做性能测试，分析性能状况和瓶颈点。
2.  针对软件架构设计进行分析，寻找导致性能问题的原因。
3.  修改相关代码和架构，进行性能优化。
4.  做性能测试，对比是否提升性能，并寻找下一个性能瓶颈。

#### 大数据软件性能优化原则

##### 1. SQL 语句优化。

##### 2. 数据倾斜处理。

数据倾斜是指当两张表进行 join 的时候，其中一张表 join 的某个字段值对应的数据行数特别多，导致这个任务长时间无法完成。如：null字段过滤处理

##### 3. MapReduce、Spark 代码优化

##### 4. 配置参数优化。

##### 5. 大数据开源软件代码优化



#### Spark 性能优化

1. 性能测试，观察 Spark 性能特性和资源（CPU、Memory、Disk、Net）利用情况。

2.  分析、寻找资源瓶颈。

3.  分析系统架构、代码，发现资源利用关键所在，思考优化策略。

4.  代码、架构、基础设施调优，优化、平衡资源利用。

5.  性能测试，观察系统性能特性，是否达到优化目的，以及寻找下一个瓶颈点。

   

### Doris 的架构方案和创新设计

阿里的这个方案可以为我们提供一种解决问题的思路

![](bigdata\Doris整体架构.png)

独特的**分区路由算法、失效转移策略、集群伸缩设计**方案

#### 分区路由算法

![](bigdata\Doris分区路由算法.png)

![](bigdata\Doris扩节点.png)

#### 失效转移策略

![](bigdata\Doris冗余备份.png)

Doris 将失效分为三种情况：**瞬时失效、临时失效、永久失效**，不同情况采用不同的失效转移策略。

![](bigdata\Doris临时失效策略.png)

**永久失效**的节点，需要添加新的服务器以代替下线的服务器，基本策略就是将另一个 group 正常使用的服务器数据拷贝到新添加的服务器上即可。

#### 集群伸缩设计

![](bigdata\Doris扩容.png)

具体过程为：

1. 向集群中一个分组 group 添加新的物理服务器，部署并启动 Doris 服务器进程。
2.  将这个 group 的所有服务器设置为临时失效。
3.  使用路由算法重新计算加入服务器后的虚拟节点分布，并把需要迁移的虚拟节点对应的物理文件拷贝到新服务器上。
4. 设置 group 所有服务器临时失效恢复，将扩容期间的数据更新写回到这些服务器。

### 基准测试工具 HiBench

HiBench 内置了若干主要的大数据计算程序作为基准测试的负载（workload）。

- Sort，对数据进行排序大数据程序。
- WordCount，前面多次提到过，词频统计大数据计算程序。
- TeraSort，对 1TB 数据进行排序，最早是一项关于软件和硬件的计算力的竞赛，所以很多大数据平台和硬件厂商进行产品宣传的时候会用 TeraSort 成绩作为卖点。
- Bayes 分类，机器学习分类算法，用于数据分类和预测。
- k-means 聚类，对数据集合规律进行挖掘的算法。逻辑回归，数据进行预测和回归的算法。
- SQL，包括全表扫描、聚合操作（group by）、连接操作（join）几种典型查询 SQL。
- PageRank，Web 排序算法。

此外还有十几种常用大数据计算程序，支持的大数据框架包括 MapReduce、Spark、Storm 等。

HiBench 使用非常简单，只需要三步：

1. 配置，配置要测试的数据量、大数据运行环境和路径信息等基本参数。
2.  初始化数据，生成准备要计算的数据，比如要测试 1TB 数据的排序，那么就生成 1TB 数据。
3.  执行测试，运行对应的大数据计算程序。

### Akka 原理与应用

Akka 使用一种叫 Actor 的编程模型，Actor 编程模型是和面向对象编程模型平行的一种编程模型。面向对象认为一切都是对象，对象之间通过消息传递。而 Actor 编程模型认为一切都是 Actor，Actor 之间也是通过消息传递实现复杂的功能，但是这里的消息是真正意义上的消息。不同于面向对象编程时，方法调用是同步阻塞的；给 Actor 发送消息不需要等待 Actor 处理，消息是异步的。

一个 Actor 类最重要的就是实现 receive 方法，在 receive 里面根据 Actor 收到的消息类型进行对应的处理。Actor 之间互相发送消息全部都是异步的。这种全部消息都是异步，通过异步消息完成业务处理的编程方式也叫**响应式编程**



## 大数据平台

### 淘宝大数据平台

![](bigdata\淘宝前期架构.png)

### 美团大数据平台

![](bigdata\美团大数据平台.png)

### 滴滴大数据平台

滴滴大数据平台分为实时计算平台（流式计算平台）和离线计算平台（批处理计算平台）两个部分。

![](bigdata\滴滴大数据平台.png)

滴滴还对 HBase 重度使用，并对相关产品（HBase、Phoenix）做了一些自定义的开发，维护着一个和实时、离线两个大数据平台同级别的 HBase 平台，它的架构图如下

![](bigdata\滴滴HBase平台架构.png)

### 腾讯的大数据平台架构

![](bigdata\腾讯的大数据平台架构.png)

### 大数据解决方案提供商

#### CDH

![](bigdata\CDH架构.png)

CDH 是一个大数据集成平台，将主流大数据产品都集成到这个平台中，企业可以使用 CDH 一站式部署整个大数据技术栈。从架构分层角度，CDH 可以分为 4 层：系统集成，大数据存储，统一服务，过程、分析与计算。

1. 系统集成：数据库导入导出用 Sqoop，日志导入导出用 Flume，其他实时数据导入导出用 Kafka。
2.  大数据存储：文件系统用 HDFS，结构化数据用 Kudu，NoSQL 存储用 HBase，其他还有对象存储。
3. 统一服务：资源管理用 Yarn，安全管理用 Sentry 和 RecordService 细粒度地管理不同用户数据的访问权限。
4.  过程、分析与计算：批处理计算用 MapReduce、Spark、Hive、Pig，流计算用 Spark Streaming，快速 SQL 分析用 Impala，搜索服务用 Solr。

#### TDH

![](bigdata\TDH架构图.png)

### 大数据云计算服务商

云计算厂商的主要大数据服务：

1. 数据集成：提供大数据同步服务，通过提供 reader 和 writer 插件，可以将不同数据源（文本、数据库、网络端口）的数据导入、导出。
2. E-MapReduce：集成了 Hadoop、Spark、Hive 等主要大数据产品，用户可以直接将自己的 MapReduce、Spark 程序或者 Hive QL 提交到 E-MapReduce 上执行。
3.  分析性数据库 AnalyticDB：提供快速低延迟的数据分析服务，类似 Cloudera 的 Impala。
4.  实时计算：基于 Flink 构建的流计算系统。

阿里云将这些大数据基础服务和其他大数据应用服务整合起来，构成一个大数据产品家族，这就是阿里云的数加。**数加**功能体系如下：

### 大数据 SaaS 服务商

大数据的采集、分析、展现，把大数据服务当作软件提供给企业（软件即服务，SaaS）

对于像友盟、神策、百度统计这样的大数据 SaaS 服务商来说，你只需要在系统中调用它提供的数据采集 SDK，甚至不需要调用，只要将它提供的 SDK 打包到自己的程序包中，就可以自动采集各种数据，传输到他们的大数据平台。

如果需要精细化、定制化地进一步采集数据、分析数据，还是需要自己调用接口进行开发。

### 大数据开放平台

除了上面提到的这几类商业大数据平台，还有一类大数据商业服务，就是大数据开放平台。

还有一种风控大数据开放平台，结合用户数据和自身数据进行大数据计算。