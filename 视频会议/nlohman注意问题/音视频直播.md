## WebRtc1对1通话

### WebRTC 处理过程

![](image\1对1音视频实时通话过程示意图.png)

分为4 部分，即两个 WebRTC 终端（上图中的两个大方框）、一个 Signal（信令）服务器和一个 STUN/TURN 服务器。

- WebRTC 终端，负责音视频采集、编解码、NAT 穿越、音视频数据传输。
- Signal 服务器，负责信令处理，如加入房间、离开房间、媒体协商消息的传递等。
- STUN/TURN 服务器，负责获取 WebRTC 终端在公网的 IP 地址，以及 NAT 穿越失败后的数据中转。

### 视频采集

#### 视频采集UML

![](image\video_capture.jpg)

DeviceInfo接口提供了设备枚举相关功能，其平台相关子类实例以组合的形式提供给VideoCapture。

- 枚举设备个数，获取某个设备名称。、
- 枚举某个设备所支持的所有能力（VideoCaptureCapability: 分辨率，最大帧率，颜色空间，是否逐行扫描)
- 获取某个设备的所有能力中与外部设置的能力最匹配的那个能力。

VideoCaptureModule视频采集模块的虚基类，它定义一系列视频采集的通用接口函数：

- Start/StopCapture用来开始/结束视频采集（平台相关）；
- CaptureStarted用来判断当前capture运行状态（平台相关）；
- Register/DeCaptureDataCallback用来注册/注销数据回调模块（平台无关）；
- Set/GetApplyRotation用来设置视频旋转角度（平台无关）。

VideoCaptureImpl类是VideoCaptureModule的实现子类。做了3个事：

- 声明静态Ctreate方法，用于创建平台相关的VideoCaptureImpl子类，在Windows平台上为VideoCaptureDS，在Linux平台上实现的子类是VideoCaptureV4L2。该方法一处声明，多处实现，在相应平台编译时，只会加载对应平台的实现代码；
- 平台相关的接口，留待平台相关的子类中实现，主要是开始/结束视频采集；
- 实现平台无关的接口：注册视频数据回调，应用视频旋转相关函数。其中注册数据回调将一个实现了VideoSinkInterface&lt;VideoFrame&gt;接口的对象赋予VideoCaptureImpl::_dataCallBack成员。当采集模块得到一帧视频数据，就可以通过该对象的OnFrame()方法推送出来。

#### 采集模块的内部数据流

1. 以VideoCaptureDS为例，平台相关的采集模块采集到一帧视频后，平台相关的函数ProcessCapturedFrame()方法进行处理。ProcessCapturedFrame()将视频帧直接传递给VideoCaptureImpl::IncomingFrame()方法
2. VideoCaptureImpl::IncomingFrame()方法将对视频帧按需求进行旋转，并利用libyuv库转换成I420类型，再给视频帧加上ntp时间戳。经过上述处理后，IncomingFrame()将视频帧进一步传递给VideoCaptureImpl::DeliverCapturedFrame()

3. VideoCaptureImpl::DeliverCapturedFrame()将调用VideoSinkInterface::OnFrame()，将视频帧传递给回调对象_dataCallBack，即数据的下一站，从而将视频帧推送出采集模块。

#### 流水线的建立

##### videocapture-》videotrack

![](image\videocapture_videotrack.png)

相关类

![](image\video_capture流水线.jpg)

##### VideoSource->VideoBroadcaster

![](image\video_source.png)

一方面VideoBroadcaster实现了VideoSinkInterface接口，成为一个Sink，这样VideoSource得到采集模块的视频帧后，首先会流入到内部的VideoBroadcaster成员对象，而非直接从VideoSource流出；另一方面VideoSource和VideoBroadcaster都实现了VideoSourceInterface接口，对外VideoSource作为视频源存在，向数据流下一站提供注册方法AddOrUpdateSink()；该方法内部调用VideoBroadcaster的AddOrUpdateSink()，从而将数据流下一站VideoSink注册到VideoBroadcaster，存入成员std::vector&lt;SinkPair&gt; sinks_中。到此，应该不难想到VideoBroadcaster既有了数据流入，还知道数据的下一站(可能多个)，那么VideoBroadcaster::OnFrame()中就可以通过循环调用下一站的OnFrame方法将视频帧广播出去。
为什么要如此设计？因为，在WebRTC 1.0的官方规范中说明了一个视频源是可以被多个视频轨共用的。通过上述方式可以实现共用的概念

##### VideoSource-VideoTrackSource

VideoTrackSource没有实现VideoSinkInterface接口，因此，实质上视频数据是不会流入到VideoTrackSource中的，但其组合了VideoSource对象，并且实现了VideoSourceInterface接口，添加到VideoTrackSource中的VideoSink会被添加到VideoSource，然后进一步添加到VideoBroadcast中。对外部来说，VideoTrackSource就是视频源。
VideoTrackSource另外实现了视频源状态相关的接口，以及状态通告相关的接口NotifierInterface，用于向更高一层（VideoTrack）通告视频源的状态。由于与数据流的讨论无关，此处只提及，不详述。

##### VideoTrackSource-VideoTrack

如同VideoTrackSource一般，VideoTrack也没有实现VideoSinkInterface接口，因此，视频数据也不会流入到VideoTrack中，但其组合了VideoTrackSource，并且间接实现了VideoSourceInterface接口。想要从VideoTrack中获取视频流的站点，只要实现VideoSinkInterface接口，通过VideoTrack的AddOrUpdateSink()注册进来即可，因为该VideoSink会经过VideoTrackSource-VideoSource-VideoBroadcaster，最终可以从VideoBroadcaster获得视频流。
VideoTrack另外实现了ObserverInterface接口，用于以观察者的身份来接收响应VideoTrackSource关于视频源状态的报告。
VideoTrack还实现了VideoTrackInterface接口，其中提供了一个重要的属性：ContentHint。这个属性告知编码器在码率降低时，应该如何应对：降低帧率？降低分辨率？对于桌面采集应用来说，我们应该设置该属性为kDetailed或者是kText，这样编码器编码该视频流的时候不会降低分辨率，量化参数qp值也不会设置的过大。

#### VideoTrack到本地渲染

如何从VideoTrack中获取视频数据：1）实现VideoSinkInterface接口，2）通过VideoTrack的AddOrUpdateSink()注册进去即可。事实上，本地渲染就是如此做的：要么直接使用WebRTC提供的平台相关的渲染类，这些类都实现了VideoSinkInterface接口；要么可以自己实现Renderer类，并实现VideoSinkInterface接口，在OnFrame方法中获取视频帧，并进行渲染操作。render通过VideoTrack的AddOrUpdateSink()注册进去时，会一直被投递到VideoBroadcaster被其持有，从VideoBroadcaster处直接得到视频帧。

WebRTC中提供的渲染类相关的UML类图：

![](image\VideoSinkInterface.jpg)

#### VideoTrack到编码器

WebRTC中VideoStreamEncoder类表征着一个视频编码器，接收原始视频帧作为输入，产生编码后的比特流作为输出。该类位于src/video/video_stream_encoder.h中

VideoTrack-&gt;VideoStreamEncoder:

![](image\VideoTrack-VideoStreamEncoder.png)

这几个对象的UML类图及其关系如下所示：要正真获得视频帧，该类需要实现VideoSinkInterface接口，在OnFrame()在该方法中得到上一站传来的视频帧。通过下面类图，我们可以看到实质上只有VideoStreamEncoder是一个VideoSink对象。而VideoTrack通过以对象成员的方式一直被传递到VideoStreamEncoder。由于VideoTrack实现了VideoSourceInterface，VideoStreamEncoder又可以反向设置到VideoTrack中，根据之前的结论，VideoStreamEncoder最终会存储在VideoBroadcaster中，由VideoBroadcaster将视频帧直接传递给VideoStreamEncoder。

![](image\VideoTrack-VideoStreamEncoder_UML.jpg)

从控制流来讲，如果不深入研究细节，仅从WebRTC的外层API来看，通过

PeerConnection-&gt;AddTrack()；

PeerConnection-&gt;CreateOffer()；

PeerConnection-&gt;SetLocalDescription()这三步就建立起了这条流水线。

##### 1.AddTrack()

在创建出VideoTrack后，通过PeerConnection-&gt;AddTrack()接口会为每个要发送的视频Track创建一个VideoRtpSender对象，视频Track成为VideoRtpSender的成员，实现逻辑上视频流向VideoTrack-&gt;VideoRtpSender流动。 另外，如果SDP使用kUnifiedPlan方式，还会为每个track创建一个独立的

RtpTranceiver对象，组合包含该track的VideoRtpSender，并添加到PC的成员RtpTranceiver数组中。

VideoRtpSender对象有两个重要的成员是与本文的讨论相关的track_和media_channel_。分别就是VideoTrack和WebRtcVideoChannel对象，是视频流的上一站和下一站。执行AddTrack()并不会将二者关联起来，只会将VideoTrack添加到VideoRtpSender中。但最终VideoRtpSender-SetSsrc()方法被调用时完成二者绑定。

- VideoRtpSender-&gt;SetSsrc()被调用的时机？
- 如果SDP使用kUnifiedPlan方式，VideoRtpSender被创建时，media_channel_并没有跟随一起被创建，那么何时何地media_channel_会被创建。

##### 2.CreateOffer()

PeerConnection-&gt;CreateOffer()方法的详细过程是非常复杂的，它收集本地的音视频能力和网络层传输能力形成SDP描述结构。虽然该方法没有直接参与视频流水线构建，但是其为下一步PeerConnection-&gt;SetLocalDescription()操作提供了必要信息，使得其能完成视频流水线的构建.

下面简要分析PeerConnection-&gt;CreateOffer()的过程中与视频相关的部分，大致的调用过程如下：

![](image\createOffer.jpg)

PeerConnection::GetOptionsForUnifiedPlanOffer()会遍历PeerConnection中所有的RtpTransceiver，为每个RtpTransceiver创建一个媒体描述信息对象MediaDescriptionOptions，在最终的生成的SDP对象中，一个MediaDescriptionOptions就是一个m-line。 根据由于之前的分析，一个Track对应一个RtpTransceiver，实质上在SDP中一个track就会对应到一个m-line。上述遍历形成所有媒体描述信息MediaDescriptionOptions会存入到MediaSessionOptions对象中，该对象在后续过程中一路传递，最终在MediaSessionDescriptionFactory::CreateOffer()方法中被用来完成SDP创建。

另外MediaSessionDescriptionFactory::CreateOffer()创建SDP过程中，会为每个媒体对象，即每个track：audio、video、data创建对应的MediaContent。上图右边展示了为视频track创建VideoContent过程，标黄的静态方法CreateStreamParamsForNewSenderWithSsrcs()会为每个RtpSender生成唯一的ssrc值。ssrc是个关键信息，正如之前分析，但需要说明的一点是此处并不会调用RtpSender-&gt;SetSsrc()方法，ssrc当前只存在于SDP信息中，等待SetLocalDescription()的解析。

##### 3.SetLocalDescription()

在CreateOffer()成功的回调中，一方面，我们会通过信令将Offer SDP发送给对端；另一方面调用SetLocalDescription()进行本地设置操作。

SetLocalDescription()的大致步骤如下：

![](image\setLocalDescription.jpg)

流水线上对象的创建：
1）PeerConnection::UpdateTransceiverChannel()方法中检查PC中的每个RtpTranceiver是存在MediaChannel，不存在的会调用WebRtcVideoEngine::CreateMediaChannel()创建WebRtcVideoChannel对象，并赋值给RtpTranceiver的RtpSender和RtpReceiver，这儿解决了VideoRtpSender的media_channel_成员为空的问题；
2）PeerConnection::UpdateSessionState()方法中，将SDP中的信息应用到上一步创建的视频媒体通道对象WebRtcVideoChannel上，调用WebRtcVideoChannel::AddSendStream()方法为通道创建WebRtcVideoSendStream，如果有多个视频Track，会有多个WebRtcVideoSendStream分别与之对应。WebRtcVideoSendStream对象存入WebRtcVideoChannel的std::map&lt;uint32_t, WebRtcVideoSendStream*&gt; send_streams_成员，以ssrc为key。创建WebRtcVideoSendStream，其构造函数中会进一步创建VideoSendStream，VideoSendStream的构造中会进一步创建

VideoStreamEncoder对象。 到此，所有有关的对象都已经创建完成。

流水线的建立:

之前就分析过VideoRtpSender-SetSsrc()方法非常重要，该方法在PeerConnection::ApplyLocalDescription()中最后被调用。会触发Track被传递，从VideoRtpSender传递到WebRtcVideoChannel，再传递到WebRtcVideoSendStream，成为WebRtcVideoSendStream的成员source。 从而实现了逻辑上VideoRtpSender-&WebRtcVideoChannel-WebRtcVideoSendStream流水线的建立；

WebRtcVideoSendStream::SetVideoSend()方法紧接着又触发调用VideoSendStream的SetSource()方法，以WebRtcVideoSendStream为视频源参数（看之前的类图，WebRtcVideoSendStream实现了VideoSourceInterface接口）一路传递给VideoStreamEncoder的成员VideoSourceProxy。在这个VideoSourceProxy::SetSource方法中，反向调用WebRtcVideoSendStream::AddOrUpdateSink()方法将VideoStreamEncoder作为VideoSink（看之前的类图，VideoStreamEncoder实现了VideoSinkInterface接口）添加到了WebRtcVideoSendStream。注意，在WebRtcVideoSendStream::AddOrUpdateSink()中会调用source-AddOrUpdateSink()进一步将VideoStreamEncoder添加到了VideoTrack（如之前的描述VideoTrack已经被传递到WebRtcVideoSendStream成为WebRtcVideoSendStream的成员source）。在逻辑上实现了视频流从WebRtcVideoSendStream-VideoSendStream-VideoStreamEncoder这段流水线。











参考文档:

1. [WebRTC源码分析——视频流水线建立](https://note.youdao.com/ynoteshare1/index.html?id=31dc0d17be34515e86b844e0186c1ce7&type=note)
2. 

