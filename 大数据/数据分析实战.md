[Toc]

## 数据分析基础

### 数据分析总体概述

#### 数据采集

![](images\数据采集.jpg)

#### 数据挖掘

数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能 BI。

![](images\数据挖掘.jpg)

#### 数据可视化

![](images\数据可视化.jpg)

#### 修炼指南

我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西

学习理论知识，提升认知；熟悉工具操作；多练习实践

### 数据挖掘学习路径

数据挖掘的基本流程、十大算法和数学原理

#### 基本流程

1. 商业理解：从商业的角度理解项目需求，再对数据挖掘的目标进行定义。
2. 数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等。这有助于你对收集的数据有个初步的认知。
3. 数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。
4. 模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
5. 模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。
6. 上线发布：呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程

#### 十大算法

分成四类:

- 分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CARTl 
- 聚类算法：K-Means，EMl 
- 关联分析：Aprioril 
- 连接分析：PageRank

##### 1. C4.5

C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理

##### 2.朴素贝叶斯（Naive Bayes）

对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。

##### 3. SVM

SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型

##### 4. KNN

KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。

##### 5. AdaBoost

Adaboost 在训练中建立了一个联合的分类模型，是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。

##### 6. CART

CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。

##### 7. Apriori

Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

##### 8. K-Means

K-Means 算法是一个聚类算法。

##### 9. EM

EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。

EM 算法经常用于聚类和机器学习领域中。

##### 10. PageRank

PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。PageRank 被 Google 创造性地应用到了网页权重的计算中，当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高

### 数据挖掘的数学原理

#### 1. 概率论与数理统计

比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。

#### 2. 线性代数

基于矩阵的各种运算，以及基于矩阵的理论，如PCA 方法、SVD 方法，以及 MF、NMF 方法

#### 3. 图论

#### 4. 最优化方法

最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。最优化方法的提出就是用更短的时间得到收敛，取得更好的效果。

总结：

![](images\数据挖掘知识清单.jpg)



### Python基础

#### NumPy

##### ndarray 对象

ndarray 实际上是多维数组的含义。在 NumPy 数组中，维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个轴（axes），秩就是描述轴的数量。

##### 结构数组

类似C中结构体的定义

```python
import numpy as np
#定义类型
persontype = np.dtype({ 'names':['name', 'age', 'chinese', 'math', 'english'], 'formats':['S32','i', 'i', 'i', 'f']})
#结构数组，每个元素都是一个persontype类型
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5), ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)], dtype=persontype)
#平均年龄
ages = peoples[:]['age']
print np.mean(ages)
```

##### ufunc 运算

能对数组中每个元素进行函数操作

**连续数组的创建**

```python
x1 = np.arange(1,11,2) #等差数组，初始值、终值、步长，默认是不包括终值
x2 = np.linspace(1,9,5) #初始值、终值、元素个数，默认是包括终值的
#x1,x2 都是[1 3 5 7 9]
```

**算数运算**

通过 NumPy 可以自由地创建等差数组，同时也可以进行加、减、乘、除、求 n 次方和取余数。

```python
x1 = np.arange(1,11,2)
x2 = np.linspace(1,9,5)
print np.add(x1, x2)
print np.subtract(x1, x2)
print np.multiply(x1, x2)
print np.divide(x1, x2)
print np.power(x1, x2)
#以下两个方法结果相同
print np.remainder(x1, x2)
print np.mod(x1, x2)
```

**统计函数**

```python
import numpy as np
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
#全部元素的最小值
print np.amin(a)
#amin(a,0) 是延着 axis=0 轴的最小值，也就是3个元素的最小值[1,2,3]
print np.amin(a,0)
#是延着axis=1 轴的最小值,[1,4,7]
print np.amin(a,1)
print np.amax(a)
print np.amax(a,0)
print np.amax(a,1)
```

统计最大值与最小值之差 **ptp()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.ptp(a)
print np.ptp(a,0)
print np.ptp(a,1)
```

统计数组的百分位数 **percentile()**

p 的取值范围是 0-100，如果 p=0，那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。

axis=0 是跨行（纵向），axis=1 是跨列（横向）

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.percentile(a, 50)
print np.percentile(a, 50, axis=0)
print np.percentile(a, 50, axis=1)
```

统计数组中的中位数 **median()**、平均数 **mean()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
#求中位数
print np.median(a)
print np.median(a, axis=0)
print np.median(a, axis=1)
#求平均数
print np.mean(a)
print np.mean(a, axis=0)
print np.mean(a, axis=1)
```

统计数组中的加权平均值 **average()**

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
#默认每个元素的权重是相同的
print np.average(a)
#加权平均
print np.average(a,weights=wts)
```

统计数组中的**标准差** std()、**方差 var()**

```
a = np.array([1,2,3,4])
print np.std(a)
print np.var(a)
```

#### NumPy 排序

sort(a, axis=-1, kind=‘quicksort’, order=None)

默认情况下使用的是快速排序；在 kind 里，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。同样 axis 默认是 -1，即沿着数组的最后一个轴进行排序，也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。另外 order 字段，对于结构化的数组可以指定按照某个字段进行排序。

```python
a = np.array([[4,3,2],[2,4,1]])
print np.sort(a)
print np.sort(a, axis=None)
print np.sort(a, axis=0)  
#[[2 3 1] [4 4 2]]
print np.sort(a, axis=1)  
#[[2 3 4] [1 2 4]]

persontype = np.dtype({ 'names':['name',  'chinese', 'math', 'english'], 'formats':['S32', 'i', 'i', 'f']})
#结构数组，每个元素都是一个persontype类型
peoples = np.array([("ZhangFei",75,100, 90),("GuanYu",85,96,88.5)], persontype)
#按总成绩排序
ranking = sorted(peoples,key=lambda x:x[1]+x[2]+x[3], reverse=True)
print(ranking)
```

### Pandas

Series 和 DataFrame 这两个核心数据结构，他们分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、统计和输出。

**Series 是个定长的字典序列。**说是定长是因为在存储的时候，相当于两个 ndarray.  Series 有两个基本属性：index 和 values。在 Series 结构中，index 默认是 0,1,2,……递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’, ‘d’]。

```python
import pandas as pd
#pip install pandas
from pandas import Series, DataFrame
x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
print x1
print x2
```

**DataFrame 类型数据结构类似数据库表。**它包括了行索引和列索引

```python
import pandas as pd
from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
df1= DataFrame(data)
df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])
print df1
print df2
```

**数据导入和输出** , 直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件

```python
import pandas as pd
from pandas import Series, DataFrame
#如果报缺少xlrd 和 openpyxl，使用pip install安装
score = DataFrame(pd.read_excel('data.xlsx'))
score.to_excel('data1.xlsx')
print score
```

#### 数据清洗

##### 1. 删除 DataFrame 中的不必要的列或行

```python
df2 = df2.drop(columns=['Chinese'])
df2 = df2.drop(index=['ZhangFei'])
```

##### 2. 重命名列名 columns，让列表名更容易识别

```python
df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)
```

##### 3. 去重复的值

```python
df = df.drop_duplicates() #去除重复行
```

##### 4. 格式问题

```python
#更改数据格式
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 

#删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.strip)
#删除左边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip)
#删除右边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip)
#删除特殊符号
df2['Chinese']=df2['Chinese'].str.strip('$')

#全部大写
df2.columns = df2.columns.str.upper()
#全部小写
df2.columns = df2.columns.str.lower()
#首字母大写
df2.columns = df2.columns.str.title()

#对数据表 df 进行 df.isnull()，可以打印所有位置是否空值
print(df.isnull())
#查看哪列存在空值，可以使用df.isnull().any()
print(df.isnull().any())
```

#### 使用 apply 函数对数据进行清洗

```python
#大写转化
df['name'] = df['name'].apply(str.upper)
#使用函数
def double_df(x):
           return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)

#新增两列，其中’new1’列是“语文”和“英语”成绩之和的 m 倍，'new2’列是“语文”和“英语”成绩之和的 n 倍
def plus(df,n,m):
    df['new1'] = (df[u'语文']+df[u'英语']) * m
    df['new2'] = (df[u'语文']+df[u'英语']) * n
    return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
```

#### 数据统计

![](images\统计函数.jpg)

describe() 函数最简便

```python
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print df1.describe()
```

#### 数据表合并

一个 DataFrame 相当于一个数据库的数据表，那么多个 DataFrame 数据表的合并就相当于多个数据库的表合并。

```python

df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})
```

##### 1. 基于指定列进行连接

```python
df3 = pd.merge(df1, df2, on='name')
```

##### 2. inner 内连接

```python
df3 = pd.merge(df1, df2, how='inner')
```

##### 3. left 左连接

左连接是以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充。

```python
df3 = pd.merge(df1, df2, how='left')
```

##### 4. right 右连接

右连接是以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充。

```python
df3 = pd.merge(df1, df2, how='right')
```

##### 5. outer 外连接

外连接相当于求两个 DataFrame 的并集。

```python
df3 = pd.merge(df1, df2, how='outer')
```

#### pandasql:用SQL方式打开Pandas

pandasql 中的主要函数是 sqldf，它接收两个参数：一个 SQL 查询语句，还有一组环境变量 globals() 或 locals()。

```python
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

![](images\Pandas.jpg)

#### 商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系

##### 元数据 VS 数据元

**元数据**（MetaData）：描述其它数据的数据，也称为“中介数据”。

**数据元**（Data Element）：就是最小数据单元。

元数据可以很方便地应用于数据仓库.

#### 数据挖掘(KDD)的流程

##### 1. 分类

就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类

##### 2. 聚类

##### 3. 预测

##### 4. 关联分析

![](images\kdd过程.jpg)

数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换

1. 数据清洗

   主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。

2. 数据集成

   是将多个数据源中的数据存放在一个统一的数据存储中。

3.  数据变换

   将数据转换成适合数据挖掘的形式. 比如：归一化操作

4. 数据后处理。 是将模型预测的结果进一步处理后，再导出

### 用户画像

![](images\用户画像建模.jpg)

#### 用户唯一标识是整个用户画像的核心

设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。

#### 给用户打标签。

1. 用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。
2. 消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。
3. 行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。
4. 内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣

用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题

#### 业务价值

1. 获客：如何进行拉新，通过更精准的营销获取客户。
2. 粘客：个性化推荐，搜索排序，场景运营等。
3. 留客：流失率预测，分析关键节点降低流失率。

![](images\标签化的流程.jpg)

#### 美团外卖分析

打标签：

1. 用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。
2. 消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。
3. 行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。
4. 内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

业务价值：

1. 在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。
2. 在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。
3. 在留客上，预测用户是否可能会从平台上流失。

### 数据采集

![](images\数据源.jpg)

开放数据源：单位维度，行业维度

![](images\单位维度的数据源.jpg)

#### 爬虫爬取

经历三个过程：使用 Requests 爬取内容；使用 XPath 解析内容；使用 Pandas 保存数据；

**抓取工具**

**火车采集器**：

数据抓取，清洗、数据分析、数据挖掘和可视化等工作。数据源适用于绝大部分的网页。

**八爪鱼**：

免费的采集模板实际上就是内容采集规则，包括了电商类、生活服务类、社交媒体类和论坛类的网站都可以采集，用起来非常方便。当然你也可以自己来自定义任务。

> 输入网页，设计流程，启动采集
>
> 自定义采集：打开网页、点击元素、循环翻页、提取数据
>
> 两个重要的工具一定要用好：流程视图和 XPath。

云采集收费，配置好采集任务，通过云端多节点并发采集，采集速度远远超过本地采集。可以自动切换多个 IP，避免 IP 被封。

**集搜客**：

完全可视化操作，无需编程。没有云采集功能

#### 日志采集

Web 服务器采集，自定义采集用户行为（js，AJAX）

#### 埋点

第三方的工具，比如友盟、Google Analysis、Talkingdata 等

#### 爬虫的流程

##### Requests 访问页面

```python
r = requests.get('http://www.douban.com')
r = requests.post('http://xxx.com', data = {'key':'value'})
```

##### XPath 定位

![](images\XPath路径表达式.jpg)

```python
#例子：
xpath(‘node’) #选取了 node 节点的所有子节点；
xpath(’/div’) #从根节点上选取 div 节点；
xpath(’//div’) #选取所有的 div 节点；
xpath(’./div’) #选取当前节点下的 div 节点；
xpath(’…’) #回到上一个节点；
xpath(’//@id’) #选取所有的 id 属性；
xpath(’//book[@id]’)#选取所有拥有名为 id 的属性的book元素；
xpath(’//book[@id=“abc”]’)#选取所有book元素，且这些book元素拥有id="abc"的属性；
xpath(’//book/title|//book/price’)#选取book元素的所有title和price元素。

from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')#所有列表项目
```

JSON对象转化

```python
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)#Json对象转Python对象
json.dumps(input)#Python对象转JSON对象
print input
```

**如何使用 JSON 数据自动下载海报**：

```python
# coding:utf-8
import requests
import json
query = '王祖贤'
headers = {'User-Agent': 'Mozilla/5.0 ...'}
''' 下载图片 '''
def download(src, id):
  #去除字符串空格，最好也去掉特殊符号，如果目录不确定存在，最好用os方法判断创建
  dir = './' + str(id).replace(' ','') + '.jpg'
  try:
    pic = requests.get(src, timeout=10)
    fp = open(dir, 'wb')
    fp.write(pic.content)
    fp.close()
  except requests.exceptions.ConnectionError:
    print('图片无法下载')
            
''' for 循环 请求全部的 url '''
for i in range(0, 22471, 20):
  url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
  html = requests.get(url,  header).text    # 得到返回结果
  response = json.loads(html,encoding='utf-8') # 将 JSON 格式转换成 Python 对象
  for image in response['images']:
    print(image['src']) # 查看当前下载的图片网址
    download(image['src'], image['id']) # 下载一张图片
```

**使用 XPath 自动下载电影海报**

一个快速定位 XPath 的方法就是采用浏览器的 XPath Helper 插件，使用 Ctrl+Shift+X 快捷键的时候，用鼠标选中你想要定位的元素，就会得到类似下面的结果

有时候你需要Selenium 库工具，来进行网页加载的模拟，直到完成加载后它才给你完整的 HTML。

```python
from lxml import etree
from selenium import webdriver

request_url ='https://search.douban.com/movie/subject_search?search_text=%E7%8E%8B%E7%A5%96%E8%B4%A4&cat=1002'
#这个参数规则看网页结构调整
src_xpath = "//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src"
title_xpath = "//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']"

driver = r"E:\pythonworkspace\drivers\chromedriver.exe"

driver = webdriver.Chrome(driver)
driver.get(request_url)
html = etree.HTML(driver.page_source)
driver = webdriver.Chrome()
driver.get(request_url)

#获取到完整的HTML时，就可以对HTML中的 XPath 进行提取
''' for 循环 请求全部的 url '''
for i in range(0, 45, 15):
    url = request_url + '&start=' + str(i)
    driver.get(url)
    html = etree.HTML(driver.page_source)
    srcs = html.xpath(src_xpath)
    titles = html.xpath(title_xpath)
    print(srcs, titles)
    for image, title in zip(srcs, titles):
        download(image, title.text)  # 下载一张图片
```

ChromeDriver镜像站：https://chromedriver.storage.googleapis.com/index.html

另外，**Puppeteer** 是个很好的选择，可以控制 Headless Chrome，这样就不用 Selenium 和 PhantomJS。与 Selenium 相比，Puppeteer **直接调用 Chrome 的 API 接口，不需要打开浏览器**，直接在 V8 引擎中处理，同时这个组件是由 Google 的 Chrome 团队维护的，所以兼容性会很好。

**加注**：如果需要一万个手机号。。。那怎么更换呢？也要一万台设备吗？

1 万个手机号，主要用于账号注册，通常采用的是“卡池”这个设备。简单来说，卡池可以帮你做收发短信。一个卡池设备 512 张卡，并发 32 路。有了卡池，还需要算法。你不能让这 512 张卡每次操作都是有规律可循的，卡池可以帮你做短信验证码，以便账号登录用。MIFI+SIM 帮你做手机流量上网用。这是两套不同的设备。



### 数据清洗

#### 数据清洗规则：

1. 完整性：单条数据是否存在空值，统计的字段是否完善。
2. 全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
3. 合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。
4. 唯一性：数据是否存在重复记录，行数据、列数据都需要是唯一的。

#### 使用 Pandas 来进行清洗

##### 1. 完整性

**缺失值处理**：

- 删除：删除数据缺失的记录；
- 均值：使用当前列的均值；
- 高频：使用当前列出现频率最高的数据

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)

age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True)
```

**空行处理**：

```python
# 删除全空的行
df.dropna(how='all',inplace=True) 
```

##### 2. 全面性

**单位统一**：

```python
# 获取 weight 数据列中单位为lbs的数据，磅（lbs）转化为千克（kgs）
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
  # 截取从头开始到倒数第三个字符之前，即去掉lbs。
  weight = int(float(lbs_row['weight'][:-3])/2.2)
  df.at[i,'weight'] = '{}kgs'.format(weight) 
```

##### 3. 合理性

**非 ASCII 字符处理**

```python
# 删除非 ASCII 字符
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

##### 4. 唯一性

一列有多个参数，做拆分：

```python
# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)
```

重复数据：

```python
# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)
```

**没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障**。



**数据清洗小作业**：

分析：NaN，负值删除记录，大小写统一，同名数据合并，

| food        | ounces | animal |
| ----------- | ------ | ------ |
| bacon       | 4.0    | pig    |
| pulled pork | 3.0    | pig    |
| bacon       | NaN    | pig    |
| Pastrami    | 6.0    | cow    |
| corned beef | 7.5    | cow    |
| Bacon       | 8.0    | pig    |
| pastrami    | -3.0   | cow    |
| honey ham   | 5.0    | pig    |
| nova lox    | 6.0    | salmon |

### 数据集成

#### 两种架构：ELT 和 ETL

ETL 的过程为提取 (Extract)——转换 (Transform)——加载 (Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取 (Extract)——加载 (Load)——变换 (Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

**目前**数据集成的**主流架构是 ETL**，但**未来**使用 **ELT** 作为数据集成架构的将越来越多。这样做会带来多种好处：

1. ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
2. 在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程

#### ETL 工具

典型的 ETL 工具有:

1. 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等
2. 开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等

国内很多公司都在使用 Kettle 用来做数据集成

##### Kettle 工具的使用

Kettle，纯 Java 编写，在 2006 年并入了开源的商业智能公司 Pentaho, 正式命名为 Pentaho Data Integeration，简称“PDI”。因此 Kettle 现在是 Pentaho 的一个组件，下载地址：https://community.hitachivantara.com/docs/DOC-1009855

Kettle 采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。

Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。

Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。

**创建 Transformation**：

Transformation 可以分成三个步骤，它包括了输入、中间转换以及输出。

在 Transformation 中包括两个主要概念：Step 和 Hop。Step 的意思就是步骤，Hop 就是跳跃线的意思。

- Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这 4 个步骤；
- Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。

**创建 Job（作业）**：

完整的任务，实际上是将创建好的转换和作业串联起来。 Job 包括两个概念：Job Entry、Hop。

- Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。
- Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。

在 Kettle 中，你可以使用 **Spoon**，它是一种一种图形化的方式，来让你设计 Job 和 Transformation，并且可以保存为文件或者保存在数据库中。

###### 案例 1：将文本文件的内容转化到 MySQL 数据库

| create_time | name       | Chinese | English | Math |
| ----------- | ---------- | ------- | ------- | ---- |
| 2018/12/22  | ZhangFei   | 66      | 65      | 30   |
| 2018/12/22  | GuanYu     | 95      | 85      | 98   |
| 2018/12/22  | ZhaoYun    | 93      | 92      | 96   |
| 2018/12/22  | HuangZhong | 90      | 88      | 77   |
| 2018/12/22  | DianWei    | 80      | 90      | 90   |

Step1：创建转换，右键“转换→新建”；

Step2：在左侧“核心对象”栏目中选择“文本文件输入”控件，拖拽到右侧的工作区中；

Step3：从左侧选择“表输出”控件，拖拽到右侧工作区；

Step4：鼠标在“文本文件输入”控件上停留，在弹窗中选择图标，鼠标拖拽到“表输出”控件，将一条连线连接到两个控件上；这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。

Step5：双击“文本文件输入”控件，导入已经准备好的文本文件；

Step6：双击“表输出”控件，这里你需要配置下 MySQL 数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个 wucai 数据库，以及 score 数据表。字段包括了 name、create_time、Chinese、English、Math，与文本文件的字段一致）。

Step7：创建数据库字段的对应关系，这个需要双击“表输出”，找到数据库字段，进行字段映射的编辑；

Step8：点击左上角的执行图标

Kettle 的开源社区：http://www.ukettle.org

##### 阿里开源软件：DataX

DataX 可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。

DataX 的模式是基于框架 + 插件完成的，DataX 的框架如下图：

![](images\DataX的框架.jpg)

将数据从源头装载到 DataXStorage，然后在“写”模块，数据从 DataXStorage 导入到目的地。

这样的好处就是，在整体的框架下，我们可以对 Reader 和 Writer 进行插件扩充，比如我想从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和 OracleWriter 插件，装在框架上使用即可。

##### Apache 开源软件:Sqoop

由 Apache 基金会所开发的分布式系统基础架构，它主要用来在 Hadoop 和关系型数据库中传递数据

### 数据变换

数据变换是数据准备的重要环节，它通过**数据平滑、数据聚集、数据概化和规范化**等方式将数据转换成适用于数据挖掘的形式

常见的变换方法：

1. 数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑；
2. 数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；
3. 数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。
4. 数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等；
5. 属性构造：构造出新的属性并添加到属性集中。

#### 数据规范化的几种方法

##### 1. Min-max 规范化

Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：

新数值 =（原数值 - 极小值）/（极大值 - 极小值）。

##### 2. Z-Score 规范化

优点：算法简单，不受数据量级影响，结果易于比较。不足：它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较

新数值 =（原数值 - 均值）/ 标准差

##### 3. 小数定标规范化

小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。

#### Python 的 SciKit-Learn 库使用

SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块

1. Min-max 规范化

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征，按列计算
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化，MinMaxScaler原始数据投射到指定的空间[min, max]，默认0-1
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

2. Z-Score 规范化

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x

#结果：
[[-0.70710678 -1.41421356  0.26726124]
 [ 1.41421356  0.70710678  1.06904497]
 [-0.70710678  0.70710678 -1.33630621]]
```

3. 小数定标规范化

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 小数定标规范化
j = np.ceil(np.log10(np.max(abs(x))))
scaled_x = x/(10**j)
print scaled_x
```

**注意**：这些算法都需要是多行，如果一行多列会是0，如果一维会报错

### 数据可视化

可视化视图超过 20 种，分别包括：文本表、热力图、地图、符号地图、饼图、水平条、堆叠条、并排条、树状图、圆视图、并排圆、线、双线、面积图、双组合、散点图、直方图、盒须图、甘特图、靶心图、气泡图等。

9 种情况：

![](images\可视化的视图使用.png)

#### 商业智能分析

最著名的当属 Tableau 和 PowerBI 了，另外中国帆软出品的 FineBI 也受到国内很多企业的青睐。

Tableau 是国外的商业软件，收费不低。它适合 BI 工程师、数据分析分析师。

PowerBI 是微软出品的，可以和 Excel 搭配使用，你可以通过 PowerBI 来呈现 Excel 的可视化内容。

#### 可视化大屏类

##### DataV

DataV 是一款可视化的工具，天猫双十一大屏就是用它呈现的。你要做的就是选择相应的控件，配置控件的样式、数据传输和交互效果等。当然 DataV 本身有一些免费的模板，你可以直接通过模板来创建

##### FineReport

FineReport 是帆软出品的工具，可以做数据大屏，也可以做可视化报表，在很多行业都有解决方案，操作起来也很方便。可以实时连接业务数据，对数据进行展示。

#### 前端可视化组件

Canvas 和 SVG 是 HTML5 中主要的 2D 图形技术，WebGL 是 3D 框架。

**Canvas** 适用于位图，也就是给了你一张白板，需要你自己来画点。Canvas 技术可以绘制比较复杂的动画。不过它是 HTML5 自带的，所以低版本浏览器不支持 Canvas。ECharts 这个可视化组件就是基于 Canvas 实现的。

**SVG** 的中文是可缩放矢量图形，它是使用 XML 格式来定义图形的。相当于用点和线来描绘了图形，相比于位图来说文件比较小，而且任意缩放都不会失真。SVG 经常用于图标和图表上。它最大的特点就是支持大部分浏览器，动态交互性实现起来也很方便，比如在 SVG 中插入动画元素等。

**WebGL** 是一种 3D 绘图协议，能在网页浏览器中呈现 3D 画面技术，并且可以和用户进行交互。你在网页上看到的很多酷炫的 3D 效果，基本上都是用 WebGL 来渲染的。下面介绍的 Three.js 就是基于 WebGL 框架的

**可视化组件**： **Echarts、D3、Three.js 和 AntV**

ECharts 是基于 H5 canvas 的 Javascript 图表库，是百度的开源项目，一直都有更新，使用的人也比较多。它作为一个组件，可以和 DataV、Python 进行组合使用。你可以在 DataV 企业版中接入 ECharts 图表组件。也可以使用 Python 的 Web 框架（比如 Django、Flask）+ECharts 的解决方案

D3 的全称是 Data-Driven Documents，是一个 JavaScript 的函数库。它提供了各种简单易用的函数，大大简化了 JavaScript 操作数据的难度。你只需要输入几个简单的数据，就能够转换为各种绚丽的图形

Three.js，顾名思义，就是 Three+JS 的意思。“Three.js”使用 JavaScript 来实现 3D 效果。Three.js 是一款 WebGL 框架，封装了大量 WebGL 接口。

AntV 是蚂蚁金服出品的一套数据可视化组件，包括了 G2、G6、F2 和 L7 一共 4 个组件。其中 G2 应该是最知名的，它的意思是 The grammar Of Graphics，也就是一套图形语法。它集成了大量的统计工具，而且可以让用户通过简单的语法搭建出多种图表。G6 是一套流程图和关系分析的图表库。F2 适用于移动端的可视化方案。L7 提供了地理空间的数据可视化框架。

#### 编程语言

在 Python 里包括了众多可视化库，比如 Matplotlib、Seaborn、Bokeh、Plotly、Pyecharts、Mapbox 和 Geoplotlib。其中使用频率最高，最需要掌握的就是 Matplotlib 和 Seaborn。

**Matplotlib** 是 Python 的可视化基础库，作图风格和 MATLAB 类似，所以称为 Matplotlib。一般学习 Python 数据可视化，都会从 Matplotlib 入手，然后再学习其他的 Python 可视化库。

**Seaborn** 是一个基于 Matplotlib 的高级可视化效果库，针对 Matplotlib 做了更高级的封装，让作图变得更加容易。你可以用短小的代码绘制更多维度数据的可视化效果图

在 R语言中也有很多可视化库可供选择。其中包括了 R 自带的绘图包 Graphics 以及工具包 ggplot2、ggmap、timevis 和 plotly 等。其中 **ggplot2** 是 R 语言中重要的绘图包，这个工具包将数据与绘图操作进行了分离，所以使用起来清晰明了，画出的图也漂亮。其实在 **Python** 里后来也引入了 **ggplot** 库

#### 数据可视化的学习

1. 重点推荐 TableauTableau 

   在可视化灵活分析上功能强大，主要目标用户更多是较专业的数据分析师。同时在工作场景中使用率高，因此掌握 Tableau 对于晋升和求职都很有帮助。不过 Tableau 是个商业软件，收费不低。而且上手起来有一些门槛，需要一定数据基础

2. 使用微图、DataV

   微图和八爪鱼是一家公司的产品，使用起来非常方便，而且免费。当你用八爪鱼采集数据之后，就直接可以用微图进行数据可视化。

   DataV 是阿里推出的数字大屏技术，不过它是收费的产品。它最大的好处，就是可以分享链接，让别人可以在线浏览，不需要像 Tableau 一样安装客户端才能看到数据可视化的结果。另外 DataV 有一些模板，你直接可以使用。

3. Python 可视化

### Python 的可视化技术

#### 可视化视图

按照数据之间的关系，我们可以把可视化视图划分为 4 类，它们分别是比较、联系、构成和分布。我来简单介绍下这四种关系的特点：

1. 比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；
2. 联系：查看两个或两个以上变量之间的关系，比如散点图；
3. 构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；
4. 分布：关注单个变量，或者多个变量的分布情况，比如直方图。同样，按照变量的个数，我们可以把可视化视图划分为单变量分析和多变量分析。

可视化的视图可以说是分门别类，多种多样，主要介绍常用的 10 种视图，这些视图包括了散点图、折线图、直方图、条形图、箱线图、饼图、热力图、蜘蛛图、二元变量分布和成对关系。

![](images\可视化视图.png)

##### 散点图

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
N = 1000
x = np.random.randn(N)
y = np.random.randn(N)
# 用Matplotlib画散点图,默认情况下呈现出来的是个长方形
plt.scatter(x, y,marker='x')
plt.show()
# 用Seaborn画散点图,呈现的是个正方形，还给了这两个变量的分布密度情况
df = pd.DataFrame({'x': x, 'y': y})
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

##### 折线图

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用Matplotlib画折线图,需要提前把数据按照x轴的大小进行排序
plt.plot(x, y)
plt.show()
# 使用Seaborn画折线图
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

##### 直方图

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
a = np.random.randn(100)
s = pd.Series(a) 
#pyplot.hist(x, bins=10)函数，参数x是一维数组，bins代表直方图中的箱子数量，默认是 10
plt.hist(s)
plt.show()
#Seaborn.distplot(x, bins=10, kde=True) 函数。其中参数x是一维数组，bins 代表直方图中的箱子数量，kde是否显示核密度估计，默认是 True
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True)
plt.show()
```

##### 条形图

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
#Matplotlib.bar(x, height) 函数，其中参数x代表x轴的位置序列，height是y轴的数值序列
plt.bar(x, y)
plt.show()
#Seaborn.barplot(x=None, y=None, data=None) 函数。其中参数data为 DataFrame 类型，x、y是data 中的变量
sns.barplot(x, y)
plt.show()
```

##### 箱线图

箱线图，又称盒式图，由五个数值点组成：最大值 (max)、最小值 (min)、中位数 (median) 和上下四分位数 (Q3, Q1)。它可以帮我们分析出数据的差异性、离散程度和异常值等。

```python
# 数据准备
# 生成10*4维度数据
data=np.random.normal(size=(10,4)) 
lables = ['A','B','C','D']
#Matplotlib.boxplot(x, labels=None) 函数，其中参数x代表要绘制箱线图的数据，labels是缺省值，可以为箱线图添加标签。
plt.boxplot(data,labels=lables)
plt.show()
# 用Seaborn.boxplot(x=None, y=None, data=None)函数。其中参数data为DataFrame类型，x、y 是data中的变量
df = pd.DataFrame(data, columns=lables)
sns.boxplot(data=df)
plt.show()
```

##### 饼图

```python
import matplotlib.pyplot as plt
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用Matplotlib.pie(x, labels=None) 函数，其中参数 x 代表要绘制饼图的数据，labels 是缺省值，可以为饼图添加标签
plt.pie(x = nums, labels=labels)
plt.show()
```

##### 热力图

英文叫 heat map，是一种矩阵表示方法，其中矩阵中的元素值用颜色来代表，不同的颜色代表不同大小的值。通过颜色就能直观地知道某个位置上数值的大小。另外你也可以将这个位置上的颜色，与数据集中的其他位置颜色进行比较。热力图是一种非常直观的多元变量分析方法。

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备,Seaborn中自带的数据集flights
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用Seaborn画热力图
sns.heatmap(data)
plt.show()
```

##### 蜘蛛图

蜘蛛图是一种显示一对多关系的方法。在蜘蛛图中，一个变量相对于另一个变量的显著性是清晰可见的。

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties  
# 数据准备
labels=np.array([u"推进","KDA",u"生存",u"团战",u"发育",u"输出"])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用Matplotlib画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14)  
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

##### 二元变量分布

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图(直方图的二维模拟)）
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

##### 成对关系

多个成对双变量的分布, 可以使用sns.pairplot() 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况。

```python
import matplotlib.pyplot as plt
import seaborn as sns
# 数据准备，鸢尾花数据集
iris = sns.load_dataset('iris')
# 用Seaborn画成对关系
sns.pairplot(iris)
plt.show()
```

> 鸢尾花可以分成 Setosa、Versicolour 和 Virginica 三个品种，在这个数据集中，针对每一个品种，都有 50 个数据，每个数据中包括了 4 个属性，分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。

**python可视化总结**：

![](images\python可视化总结.png)



## 数据分析算法

### 决策树

#### 构造和剪枝

**构造**

构造就是生成一棵完整的决策树，构造的过程就是选择什么属性作为节点的过程

三个重要的问题：

1. 选择哪个属性作为根节点；
2. 选择哪些属性作为子节点；
3. 什么时候停止并得到目标状态，即叶节点。

**剪枝** 

防止“过拟合”。造成过拟合的原因之一就是因为训练集中样本量较小。太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。

剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。

**预剪枝**是在决策树构造时就进行剪枝。**方法**是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

**后剪枝**就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。**方法**：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

将哪个属性作为根节点是个关键问题，两个指标：**纯度**和**信息熵**。

把决策树的构造过程理解成为寻找纯净划分的过程。可以用纯度来表示，就是让目标变量的分歧最小。

信息熵（entropy），它表示了信息的不确定度。在信息论中，随机离散事件出现的概率存在着不确定性。

**信息熵越大，纯度越低**。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。

经典的 “不纯度”的指标有三种，分别是**信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）**。

将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树。

#### 在 ID3 算法上进行改进的 C4.5 算法

##### 1. 采用信息增益率

ID3 在计算的时候，倾向于选择取值多的属性。C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵

##### 2. 采用悲观剪枝

ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。

##### 3. 离散化处理连续属性

选择具有最高信息增益的划分所对应的阈值。

##### 4. 处理缺失值



#### CART 决策树

英文全称叫做 Classification And Regression Tree，中文叫做分类回归树

分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。

##### 分类树的工作流程

CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。在 CART 算法中，基于基尼系数对特征属性进行二元分裂

##### 创建分类树

```python
# encoding=utf-8
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
# 准备数据集，鸢尾花卉数据集
iris=load_iris()
# 获取特征集和分类标识
features = iris.data
labels = iris.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)
# 创建CART分类树
clf = DecisionTreeClassifier(criterion='gini')
# 拟合构造CART分类树
clf = clf.fit(train_features, train_labels)
# 用CART分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print("CART分类树准确率 %.4lf" % score)
```

##### 回归树的工作流程

CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。

样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。

可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值

```python
#CART 回归树对波士顿房价进行预测
# encoding=utf-8
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
from sklearn.tree import DecisionTreeRegressor
# 准备数据集
boston=load_boston()
# 探索数据
print(boston.feature_names)
# 获取特征集和房价
features = boston.data
prices = boston.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)
# 创建CART回归树
dtr=DecisionTreeRegressor()
# 拟合构造CART回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
```

##### 决策树的剪枝

采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。

节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量

#### sklearn 中的决策树模型

```python
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
```

![](images\DecisionTreeClassifier参数.jpg)

##### 决策树的关键流程

**特征选择是分类模型好坏的关键**

1. 准备阶段：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；
2. 分类阶段：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。

![](images\决策树关键流程.jpg)

**模块 1：数据探索**

有一些函数：

使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；

使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；

使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；

使用 head 查看前几行数据（默认是前 5 行）；

使用 tail 查看后几行数据（默认是最后 5 行）。

```python
import pandas as pd
# 数据加载
train_data = pd.read_csv('./Titanic_Data/train.csv')
test_data = pd.read_csv('./Titanic_Data/test.csv')
# 数据探索
print(train_data.info())
print('-'*30)
print(train_data.describe())
print('-'*30)
print(train_data.describe(include=['O']))
print('-'*30)
print(train_data.head())
print('-'*30)
print(train_data.tail())
```

**模块 2：数据清洗**

```python
# 使用平均年龄来填充年龄中的nan值
train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)
test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)
# 使用票价的均值填充票价中的nan值
train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)
test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)
#观察下 Embarked 字段的取值，计数
print(train_data['Embarked'].value_counts())
```

**模块 3：特征选择**

```python
# 特征选择
features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
#获取这些列数据
train_features = train_data[features]
train_labels = train_data['Survived']
test_features = test_data[features]

#将符号转成数字 0/1 进行表示
dvec=DictVectorizer(sparse=False)
#fit_transform将特征向量转化为特征值矩阵
train_features=dvec.fit_transform(train_features.to_dict(orient='record'))
#打印转化后的特征属性，可以看到原本一列的Embarked变成了3列，Sex 列变成了两列
print(dvec.feature_names_)
```

**模块 4：决策树模型**

```python
from sklearn.tree import DecisionTreeClassifier
# 构造 ID3 决策树
clf = DecisionTreeClassifier(criterion='entropy')
# 决策树训练
clf.fit(train_features, train_labels)
```

**模块 5：模型预测 & 评估**

```python
test_features=dvec.transform(test_features.to_dict(orient='record'))
# 决策树预测
pred_labels = clf.predict(test_features)

# 得到决策树准确率
acc_decision_tree = round(clf.score(train_features, train_labels), 6)
print(u'score 准确率为 %.4lf' % acc_decision_tree)
```

统计决策树分类器的准确率：

K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值

K 折交叉验证的原理是这样的：

1. 将数据集平均分割成 K 个等份；
2. 使用 1 份数据作为测试数据，其余作为训练数据；
3. 计算测试准确率；
4. 使用不同的测试集，重复 2、3 步骤。

**模块 6：决策树可视化**

使用 Graphviz 可视化工具帮我们把决策树呈现出来

安装 Graphviz 库需要下面的几步：

1. 安装 graphviz 工具，这里是它的下载地址；http://www.graphviz.org/download/
2. 将 Graphviz 添加到环境变量 PATH 中；
3. 需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。

### 朴素贝叶斯

![](images\朴素贝叶斯分类.png)

几个概念：

- 先验概率：通过经验来判断事情发生的概率
- 后验概率：后验概率就是发生结果之后，推测原因的概率。
- 条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”
- 似然函数（likelihood function）：你可以把概率模型的训练过程理解为求参数估计的过程。

**朴素贝叶斯**

它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。

朴素贝叶斯模型由两种类型的概率组成：

- 每个类别的概率P(Cj)；
- 每个属性的**条件概率**P(Ai|Cj)。这是我们最关心的

贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。

#### 分类工作原理

朴素贝叶斯分类是常用的贝叶斯分类方法。

**连续数据案例**: 

考虑正态分布的密度函数，如：

  EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：

1. x：正态分布中，需要计算的数值；
2. Mean：正态分布的平均值；
3. Standard_dev：正态分布的标准差；
4. Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。

#### 分类器工作流程

朴素贝叶斯分类常用于文本分类、情感分析和垃圾邮件识别，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。

![](images\朴素贝叶斯分类器工作流程.jpg)

**准备阶段**

需要确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定

**训练阶段**

生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。输入是特征属性和训练样本，输出是分类器。

**应用阶段**

使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。

#### sklearn 机器学习包

**高斯朴素贝叶斯**（GaussianNB）：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。

**多项式朴素贝叶斯**（MultinomialNB）：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。

**伯努利朴素贝叶斯**（BernoulliNB）：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。

#### TF-IDF 如何计算

理论知识见推荐系统笔记，这里只说实战

在 sklearn 中我们直接使用 TfidfVectorizer 类

TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)

构造参数：自定义停用词 stop_words（列表） 和规律规则 token_pattern（正则）

```python
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vec = TfidfVectorizer()

documents = [
    'this is the bayes document',
    'this is the second second document',
    'and the third one',
    'is this the document'
]
#拟合模型，返回文本矩阵
tfidf_matrix = tfidf_vec.fit_transform(documents)

print('不重复的词:', tfidf_vec.get_feature_names())
#返回词汇表，字典类型
print('每个单词的ID:', tfidf_vec.vocabulary_)
#返回停用词列表
print('stop_words:', tfidf_vec.stop_words_)
#顺序是按照词语的 id 顺序
print('每个单词的tfidf值:', tfidf_matrix.toarray())
```

#### 对文档进行分类

![](images\朴素贝叶斯文档分类流程.jpg)

1. 基于分词的数据准备，包括分词、单词权重计算、去掉停用词；
2. 应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率

**模块 1：对文档进行分词**

```python
#英文文档用NLTK
import nltk
#NLTK 包中包含了英文的停用词 stop words、分词和标注方法。
word_list = nltk.word_tokenize(text) #分词
nltk.pos_tag(word_list) #标注单词的词性

#中文分词用jieba
#jieba包含了中文的停用词 stop words 和分词方法
import jieba
word_list = jieba.cut (text) #中文分词
```

**模块 2：加载停用词表**

停用词保存到stop_words.txt，利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。

```python
stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]
```

**模块 3：计算单词的权重**

```python
#max_df单词在文档中的最高出现率
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
features = tf.fit_transform(train_contents)
```

**模块 4：生成朴素贝叶斯分类器**

采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。不能因为一个事件没有观察到，就认为整个事件的概率为 0。

当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。

当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。

```python
# 多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB  
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
```

**模块 5：使用生成的分类器做预测**

```python
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
#计算测试集的特征矩阵
test_features=test_tf.fit_transform(test_contents)
#预测
predicted_labels=clf.predict(test_features)
```

**模块 6：计算准确率**

```python
from sklearn import metrics
#对实际结果和预测的结果做对比
print(metrics.accuracy_score(test_labels, predicted_labels))
```

**总结：**

![](images\朴素贝叶斯文档分类工具.png)

小作业：文档分类练习

https://github.com/cystanford/text_classification

### SVM

#### SVM 的工作原理

SVM 就是帮我们**找到一个超平面**，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。

**支持向量**就是离**分类超平面**最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。

#### 硬间隔、软间隔和非线性 SVM

假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。另外还存在一种情况，就是非线性支持向量机。

**核函数**：它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分。