[Toc]

### 视频流程：

![视频流程示意图](image\视频流程示意图.jpg)

以一路视频会话为例，主要分为以下几个线程：

1）视频源产生线程：Camera生产视频画面，封装成视频帧，以一定帧率投递到下一个模块。；

2）采集线程：由Capturer负责采集视频帧，并对视频帧进行一定处理，如调整画面亮度。并送入各个发送链路的编码模块编码并发送出去；

3）接收线程：Rtp/Rtcp负责接收RTP/RTCP数据包，并进行包解析；

4）解码线程：Decoder解码编码后的视频帧；

5）投递线程：Render接收解码后的视频帧并缓存，投递给显示设备；

6）显示线程：Player负责绘制或输出视频画面，可以做多个窗口显示或者一个窗口多画面显示。

  视频解码时间相对编码时间要长，因此开辟了单独线程完成解码；另外多路视频画面接收后没有类似多路音频混音的操作，而是分别进行渲染显示。



# WebRTC的视频解码原理简析

WebRTC的视频部分，包含采集、编解码(I420/VP8)、加密、媒体文件、图像处理、显示、网络传输与流控(RTP/RTCP)等功能。

视频采集---video_capture：

   源代码在webrtc\modules\video_capture\main目录下，包含接口和各个平台的源代码。在windows平台上，WebRTC采用的是dshow技术，来实现枚举视频的设备信息和视频数据的采集，这意味着可以支持大多数的视频采集设备；对那些需要单独驱动程序的视频采集卡（比如海康高清卡）就无能为力了。

视频采集支持多种媒体类型，比如I420、YUY2、RGB、UYUY等，并可以进行帧大小和帧率控制。

视频编解码---video_coding：

​    源代码在webrtc\modules\video_coding目录下。

   WebRTC采用I420/VP8编解码技术。VP8是google收购ON2后的开源实现，并且也用在WebM项目中。VP8能以更少的数据提供更高质量的视频，特别适合视频会议这样的需求。

视频加密--video_engine_encryption：

​    视频加密是WebRTC的video_engine一部分，相当于视频应用层面的功能，给点对点的视频双方提供了数据上的安全保证，可以防止在Web上视频数据的泄漏。

​    视频加密在发送端和接收端进行加解密，密钥由视频双方协商，代价是会影响视频数据处理的性能；也可以不使用视频加密功能，这样在性能上会好些。

​    视频加密的数据源可能是原始的数据流，也可能是编码后的数据流。估计是编码后的数据流，这样加密代价会小一些，需要进一步研究。

视频媒体文件--media_file：

   源代码在webrtc\modules\media_file目录下。

该功能是可以用本地文件作为视频源，有点类似虚拟摄像头的功能；支持的格式有Avi。

另外，WebRTC还可以录制音视频到本地文件，比较实用的功能。

视频图像处理--video_processing：

   源代码在webrtc\modules\video_processing目录下。

   视频图像处理针对每一帧的图像进行处理，包括明暗度检测、颜色增强、降噪处理等功能，用来提升视频质量。

视频显示--video_render

源代码在webrtc\modules\video_render目录下。

   在windows平台，WebRTC采用direct3d9和directdraw的方式来显示视频，

网络传输与流控：

   对于网络视频来讲，数据的传输与控制是核心价值。WebRTC采用的是成熟的RTP/RTCP技术。

音频：

​    WebRTC的音频部分，包含设备、编解码(iLIBC/iSAC/G722/PCM16/RED/AVT、NetEQ)、加密、声音文件、声音处理、声音输出、音量控制、音视频同步、网络传输与流控(RTP/RTCP)等功能。

音频设备---audio_device：

​    源代码在webrtc\modules\audio_device\main目录下，包含接口和各个平台的源代码。

​    在windows平台上，WebRTC采用的是Windows Core Audio和Windows Wave技术来管理音频设备，还提供了一个混音管理器。利用音频设备，可以实现声音输出，音量控制等功能。

音频编解码---audio_coding：

​    源代码在webrtc\modules\audio_coding目录下。

​     WebRTC采用iLIBC/iSAC/G722/PCM16/RED/AVT编解码技术。WebRTC还提供NetEQ功能---抖动缓冲器及丢包补偿模块，能够提高音质，并把延迟减至最小。另外一个核心功能是基于语音会议的混音处理。

声音加密--voice_engine_encryption：

​     和视频一样，WebRTC也提供声音加密功能。

声音文件：

​    该功能是可以用本地文件作为音频源，支持的格式有Pcm和Wav。同样，WebRTC也可以录制音频到本地文件。

声音处理--audio_processing

​     源代码在webrtc\modules\audio_processing目录下。

​     声音处理针对音频数据进行处理，包括回声消除(AEC)、AECM(AEC Mobile)、自动增益(AGC)、降噪(NS)、静音检测(VAD)处理等功能，用来提升声音质量。

网络传输与流控：

​      和视频一样，WebRTC采用的是成熟的RTP/RTCP技术。

\---------------------

IM和视频聊天的，可以参考下这个 https://github.com/starrtc/starrtc-android-demo

原文：https://blog.csdn.net/fanyun_01/article/details/88936945

转载于:https://www.cnblogs.com/elesos/p/10678681.html



# WEBRTC 视频接收原理及流程

创建解码器
VideoChannel::SetRemoteContent_w->BaseChannel::UpdateRemoteStreams_w->
WebRtcVideoChannel2::AddRecvStream->WebRtcVideoChannel2::AddRecvStream[new WebRtcVideoReceiveStream]->
WebRtcVideoReceiveStream::ConfigureCodecs->WebRtcVideoChannel2::WebRtcVideoReceiveStream::CreateOrReuseVideoDecoder->
VideoDecoder::Create->H264Decoder::Create|VP8Decoder::Create|VP9Decoder::Create()->
WebRtcVideoReceiveStream::RecreateWebRtcStream->Call::CreateVideoReceiveStream[new VideoReceiveStream]->


VCMGenericDecoder::RegisterDecodeCompleteCallback->H264DecoderImpl::RegisterDecodeCompleteCallback
/*
VideoCodingModuleImpl::RegisterReceiveCallback->VideoReceiver::RegisterReceiveCallback->
VCMDecodedFrameCallback::SetUserReceiveCallback->*/
注册用户接收已解码视频帧
1.VideoStreamDecoder::VideoStreamDecoder->VideoReceiver::RegisterReceiveCallback[注册VideoStreamDecoder为回调]->
2.VCMDecodedFrameCallback::SetUserReceiveCallback

解码线程
VideoReceiveStream::DecodeThreadFunction->VideoReceiveStream::Decode->
VideoReceiver::Decode(uint16_t maxWaitTimeMs)->VCMReceiver::FrameForDecoding[返回VCMEncodedFrame*]->
VideoReceiveStream::Encoded(EncodedImage& encoded_image)[从网络接收处理后完整的已编码帧传入]->
VideoReceiver::Decode->VCMCodecDataBase::GetDecoder(const VCMEncodedFrame& frame,VCMDecodedFrameCallback*)
[传入解码后的回调类VCMDecodedFrameCallback]->VCMGenericDecoder::RegisterDecodeCompleteCallback->
H264DecoderImpl::RegisterDecodeCompleteCallback->
VCMCodecDataBase::CreateAndInitDecoder->
H264DecoderImpl::Decode(const EncodedImage& input_image)[H264解码成功后调用之前注册的回调]->
VCMDecodedFrameCallback::Decoded->VCMReceiveCallback::FrameToRender->
VideoStreamDecoder::FrameToRender->
IncomingVideoStream::OnFrame(const VideoFrame& video_frame)->
IncomingVideoStreamProcess[线程]->IncomingVideoStream::DeliverFrame->
VideoReceiveStream::OnFrame(const VideoFrame& video_frame)[从编码传入又到解码输出]
[config_.renderer->OnFrame(video_frame);renderer为何物]
在构造函数初始化WebRtcVideoChannel2::WebRtcVideoReceiveStream::WebRtcVideoReceiveStream
config_.renderer = this renderer为视频引擎流接收对象WebRtcVideoReceiveStream,流回到视频引擎]
->WebRtcVideoChannel2::WebRtcVideoReceiveStream::OnFrame(const webrtc::VideoFrame& frame)
->VideoSinkInterface::OnFrame(const VideoFrameT& frame)[WebRtcVideoReceiveStream::SetSink接收者]
VideoRtpReceiver::VideoRtpReceiver[provider_->SetVideoPlayout(ssrc_, true, &broadcaster_),
即实际接收者为 VideoBroadcaster::OnFrame][最终用户通过VideoBroadcaster::AddOrUpdateSink 注册观察者]->


WebRtcSession::SetVideoPlayout->VideoChannel::SetSink->VideoMediaChannel::SetSink->



关键类图

![](image\关键类图.jpg)



# WebRTC 数据安全机制

我们假设一个场景， A 与 B 通信，但此时 B 并不是真正的 B ，而是冒充的，这样 A 与 B 通信时，冒充的 B 就获得了 A 的重要信息。其实这种情况更多发生在会议系统或在线教育的小班课中，此时会议中有多人进行互动，如果黑客进入了会议中，他只需听别人说话，自己不发言，这样就将关键的信息窃取走了。所以现在的问题又来了，我们该**如何辨别对方的身份是否合法呢**？

了解https协议的人都知道，https是通过**非对称加密**、**数字签名**和**数字证书**来确保数据传输的安全，这其实是SSL协议的一部分，在webrtc中也会采用类似SSL的机制来保证数据安全。这就是了**DTLS。**

我们先来看一下，webrtc是如何识别双方身份的。

它首先通过信令服务器交换 SDP，SDP 信息中包括了以下几个重要信息：

```text
...
a=ice-ufrag:khLS
a=ice-pwd:cxLzteJaJBou3DspNaPsJhlQ
a=fingerprint:sha-256 FA:14:42:3B:C7:97:1B:E8:AE:0C2:71:03:05:05:16:8F:B9:C7:98:E9:60:43:
...
```



SDP 交换完成后，A 与 B 都获取到了对方的 ice-ufrag、ice-pwd 和 fingerprint 信息，有了这些信息后，就可验证对方是否是一个合法用户了。

其中， **ice-ufrag 和 ice-pwd 是用户名和密码**。当 A 与 B 建立连接时，A 每次发送数据到B，都要带着它的用户名和密码过来，此时 B 端就可以通过验证 A 带来的用户名和密码与 SDP 中的用户名和密码是否一致的，来判断 A 是否是一个合法用户了。

除此之外，**fingerprint**也是验证合法性的关键一步，它是存放公钥证书的**指纹（或叫信息摘要）**，在通过 ice-ufrag 和 ice-pwd 验证用户的合法性之余，还要对它发送的证书做验证，看看证书在传输的过程中是否被窜改了。



![img](https://pic3.zhimg.com/80/v2-c54301ff3be19be38511ca9e6d9e2141_720w.jpg)

从这张图中你可以看到， A 与 B 在传输数据之前，需要经历如下几个步骤。

- 首先通过信令服务器交换 SDP 信息，也就是进行媒体协商。在 SDP 中记录了用户的用户名、密码和指纹，有了这些信息就可以对用户的身份进行确认了。
- 然后就是双方通过 STUN\TURN 协议获得自己的host、srflx 和 relay，交换candidate，这样双方才能建立连接。但是在语言视频直播中，客户端B一般是流媒体服务器，所以客户端A发一个STUN request ，服务端B响应一个STUN response，双方就可以建立连接了，但是发送STUN 消息的时候要带上用户名和密码。如果 STUN 消息中的用户名和密码与交换的 SDP 中的用户名和密码一致，则说明是合法用户。
- A、B建立连接后，则需要进行 DTLS 协商（协商过程下面会讲），交换公钥证书并协商密码相关的信息。同时还要通过 fingerprint 对证书进行验证，确认其没有在传输中被窜改。
- 最后，再使用协商后的密码信息和公钥对数据进行加密，开始传输音视频数据。

以上就是 WebRTC 保证数据安全的整套机制。

WebRTC 是通过使用 DTLS、SRTP 等几个协议的结合来达到数据安全的，那接下来我们就来分别看一下这几个协议是如何实现的。



## DTLS协议

说到网络上的数据安全你可能首先想到的是 HTTPS， HTTPS 可以简单理解为“HTTP 协议 + SSL数据加密”，当然实际上它要复杂得多。HTTPS 的底层最初是使用 SSL协议对数据加密。当 SSL 更新到 3.0 时，IETF 对 SSL 3.0 进行了标准化，并增加了一些新的功能，不过基本与 SSL 3.0 没什么区别，标准化后的 SSL 更名为 TLS 1.0，所以可以说 TLS 1.0 就是 SSL 的 3.1 版本。



由于 TLS 底层是基于 TCP 协议的，而 WebRTC 音视频数据的传输主要基于 UDP 协议，因此 WebRTC 对数据的保护无法直接使用 TLS 协议。但 TLS 协议在数据安全方面做得确实非常完善，所以人们就想到是否可以将 TLS 协议移植到 UDP 协议上呢？ 因此 DTLS 就应运而生了。

所以你可以认为**DTLS 就是运行在 UDP 协议之上的简化版本的 TLS**，它使用的安全机制与 TLS 几乎一模一样。



在 WebRTC 中为了更有效地保护音视频数据，所以需要使用 DTLS 协议交换公钥证书，并确认使用的密码算法，这个过程在 DTLS 协议中称为**握手协议**。

![img](https://picb.zhimg.com/80/v2-8e4917e8f792e69c2b80ef9c132c25dc_720w.jpg)



DTLS 的握手过程如下：

- 首先 DTLS 协议采用 C/S 模式进行通信，其中发起请求的一端为客户端，接收请求的为服务端。
- 客户端向服务端发送 ClientHello 消息，服务端收到请求后，回 ServerHello 消息，并将自己的证书发送给客户端，同时请求客户端证书。
- 客户端收到证书后，将自己的证书发给服务端，并让服务端确认加密算法。
- 服务端确认加密算法后，发送 Finished 消息，至此握手结束。



**DTLS解决的问题**

- 交换密钥
- 约定使用的加密算法

（DTLS不解决加密、解密的问题 ， 加密解密的问题是通过 SRTP/SRTCP 协议解决的）

## SRTP/SRTCP 协议

RTP/RTCP 协议并没有对它的负载数据进行任何保护。因此，如果你通过抓包工具，如 Wireshark，将音视频数据抓取到后，通过该工具就可以直接将音视频流播放出来，这是非常恐怖的事情。

在 WebRTC 中，为了防止这类事情发生，没有直接使用 RTP/RTCP 协议，而是使用了 SRTP/SRTCP 协议 ，即安全的 RTP/RTCP 协议。

**WebRTC 使用了非常有名的 libsrtp 库将原来的 RTP/RTCP 协议数据转换成 SRTP/SRTCP 协议数据**。



**SRTP要解决的问题**

- 对数据加密，保证数据安全
- 保证数据完整性

### 音频处理流程

![](image\音频处理流程.jpg)



## WebRTC之PeerConnection的建立过程

基于第三方webrtc开源平台开发视频会议难度不是很大，主要是业务方面的问题。但是，一旦涉及核心的底层问题就需要阅读源代码，找出bug了，难度不小。

分析了一下peerconnection的创建过程。

![](image\peerconnection创建过程.png)



假设clientA，clientB分为为offer和answer.

1. Offer端

pc =new RTCPeerConnection(null);

pc.onicecandidate=handleIceCandidate;

pc.onaddstream=handleRemoteStreamAdded;

pc.onremovestream=handleRemoteStreamRemoved;

pc.addStream(localStream);

**pc.createOffer(setLocalAndSend,handleCreateOfferError);**



function handleIceCandidate(event){

//将本地产生的candidate发送给对方

sendMessage({

type:”candidate”;

candidate:event.candidate.candidate;

……;

});

}



function setLocalAndSend(sdp){

pc.setLocalDescription(sdp);//设置本地sdp，完成设置后onicecandidate事件会调用。

sendMessage(sdp);//将offer发送给对方

}



当offer提供端接收到来自对方的answer时: pc.setRemoteDescription(new RTCSessionDescription(message));

当offer端接收到来自对方的candidate时，pc.addIceCandidate(candidate);//将来自对方的candidate设置给本地



2． Answer端的代码与offer端类似，红色部分代码不同

pc =new RTCPeerConnection(null);

pc.onicecandidate=handleIceCandidate;

pc.onaddstream=handleRemoteStreamAdded;

pc.onremovestream=handleRemoteStreamRemoved;

pc.addStream(localStream);



**注意区别**：offer端是主动调用createOffer函数并将offer发送给对方。Answer端接受到offer后，才会创建peerConnection等一系列操作：

pc.setRemoteDescription(sdp);//设置接收到的远端offer

**pc.createAnswer(setLocalAndSend,null,sdpConstraints);** //创建answer并发送给对方。

**setLocalAndSend**中会设置本地sdp，完成设置后onicecandidate事件会调用。然后将candidate发送给对方，对方收到candidate后调用addIceCandidate函数完成peerconnection的创建。

 

对于**onaddstream事件的调用时机，对于offer端，在接收到offer之后就可能onaddstream事件就被触发了。**







相关文章：

https://blog.csdn.net/neustar1/article/month/2014/02

https://smileyqp.github.io/webrtc_book/

https://zhuanlan.zhihu.com/c_1184402743954948096

https://blog.csdn.net/jakezhang1990/article/details/104690022

https://www.cnblogs.com/cther/p/myPeerConnection.html