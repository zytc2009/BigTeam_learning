[Toc]

（代码下载见环境搭建文档）

三个问题：

1. 客户端之间如何建立连接？
2. 客户端之间如何实现数据传输？
3. 音视频数据的采集、预览、编码、传输、解码、渲染完整流程。

# 安卓相机采集实现分析

WebRTC-Android 的相机采集主要涉及到以下几个类：Enumerator，Capturer，Session，SurfaceTextureHelper。

其中 Enumerator 创建 Capturer，Capturer 创建 Session，实现对相机的操作，SurfaceTextureHelper 实现用 SurfaceTexture 接收数据。

### Enumerator

`CameraEnumerator` 接口如下：

```
public interface CameraEnumerator {
  public String[] getDeviceNames();
  public boolean isFrontFacing(String deviceName);
  public boolean isBackFacing(String deviceName);
  public List<CaptureFormat> getSupportedFormats(String deviceName);
  public CameraVideoCapturer createCapturer(
      String deviceName, CameraVideoCapturer.CameraEventsHandler eventsHandler);
}
```

主要是获取设备列表、检查朝向、创建 Capturer。

相关类主要是Camera2Enumerator和Camera1Enumerator

### Capturer

WebRTC 视频采集的接口定义为 `VideoCapturer`，其中定义了初始化、启停、销毁等操作，以及接收启停事件、数据的回调。相机采集的实现是 `CameraCapturer`，针对不同的相机 API 又分为 `Camera1Capturer` 和 `Camera2Capturer`。相机采集大部分逻辑都封装在 `CameraCapturer` 中，只有创建 `CameraSession` 的代码在两个子类中有不同的实现。

下面分别看看 `VideoCapturer` 几个重要的 API 实现逻辑。

#### initialize

initialize 比较简单，只是保存一下传入的相关对象。

#### startCapture

startCapture 则会先检查当前是否正在创建 session，或者已有 session 正在运行，这里保证了不会同时存在多个 session 在运行。而众多状态成员的访问都通过 `stateLock` 进行保护，避免多线程安全问题。

如果需要创建 session，则在相机操作线程创建 session，同时在主线程检测相机操作的超时。所有相机的操作都切换到了单独的相机线程，以避免造成主线程阻塞，而检查超时自然不能在相机线程，否则相机线程被阻塞住之后超时回调也不会执行。

我们发现 capturer 中并没有实际相机操作的代码，开启相机、预览的代码都封装在了 `CameraSession` 中，那这样 capturer 的逻辑就得到了简化，切换摄像头、失败重试都只需要创建 session 即可，capturer 可以专注于状态维护和错误处理的逻辑。

**CameraCapturer** 状态维护和错误处理的逻辑还是非常全面的：相机开启状态、相机运行状态、切换摄像头状态、错误重试、相机开启超时，全部都考虑到了。另外相机切换、开关相机、错误事件，统统都有回调通知。这里就充分体现出了 demo 和产品的差别，开启相机预览的 demo 十行代码就能搞定，而要全面考虑各种异常情况，就需要费一番苦心了。

不过这里仍有一点小瑕疵，错误回调的参数是字符串，虽然可以很方便的打入日志，但不利于代码判断错误类型。最好是参数使用错误码，然后准备一个错误码到错误信息的转换函数。

#### stopCapture

`stopCapture` 时会先判断是否正在创建 session，如果正在创建，那就需要等待其创建完毕。通过检查后，如果当前有 session 正在运行，就在相机线程关闭 session。

#### changeCaptureFormat

改变采集格式需要重启采集，即先 `stopCapture`，再 `startCapture`。这俩操作都是异步的，会不会有问题？这就涉及到 Handler 的一点知识了，向 Handler 提交的消息、任务，都会被加入到同一个队列中，提交到队列中的任务会保证按序执行，即先提交一定会先执行，所以这里我们不必担心关闭相机和开启相机顺序错乱。

#### switchCamera

`switchCamera` 也会先停止老的 session，再创建新的 session，只不过还需要检查相机个数、实现切换状态通知逻辑。

这块代码应该有个小问题：`startCapture` 会把 `openAttemptsRemaining` 设置为 `MAX_OPEN_CAMERA_ATTEMPTS`，但切换摄像头时只会将其设置为 1，这个不对称应该没什么道理，所以我认为应该保持一致。

### Session

前面我们已经知道，和相机 API 实际打交道的代码都在 `CameraSession` 中，这里我们就一探其究竟。

开启相机、开启预览、设置事件回调的代码都在创建 session 的工厂方法 `Camera1Session.create` 和 `Camera2Session.create` 中。停止相机和预览则定义了一个 `stop` 接口。

具体的相机 API 使用就比较简单了。

#### Camera1

- 创建 `Camera` 对象：`Camera.open`；
- 设置预览 SurfaceTexture，用来接收帧数据（位于显存中）：`camera.setPreviewTexture`；
- 选择合适的相机预览参数（尺寸、帧率、对焦）：`Parameters` 和 `camera.setParameters`；
- 如果需要获取内存数据回调，则需要设置 buffer 和 listener：`camera.addCallbackBuffer` 和 `camera.setPreviewCallbackWithBuffer`；
- 如果需要相机服务为我们调整数据方向，则可以设置旋转角度：`camera.setDisplayOrientation`；
- 开启预览：`camera.startPreview`；
- 停止预览：`camera.stopPreview` 和 `camera.release`；

#### Camera2

- 创建 `CameraManager` 对象，相机操作始于“相机管家”：`context.getSystemService(Context.CAMERA_SERVICE)`；
- 创建 `CameraDevice` 对象：`cameraManager.openCamera`；
- 和 Camera1 不同，Camera2 的操作都是异步的，调用 `openCamera` 时我们会传入一个回调，在其中接收相机操作状态的事件；
- 创建成功：`CameraDevice.StateCallback#onOpened`；
- 创建相机对象后，开启预览 session，设置数据回调：`camera.createCaptureSession`，同样，这个操作也会传入一个回调；
- session 开启成功：`CameraCaptureSession.StateCallback#onConfigured`；
- 开启 session 后，设置数据格式（尺寸、帧率、对焦），发出数据请求：`CaptureRequest.Builder` 和 `session.setRepeatingRequest`；
- 停止预览：`cameraCaptureSession.stop` 和 `cameraDevice.close`；

#### 图像方向

通常前置摄像头输出的图像方向是逆时针旋转 270° 的，后置摄像头是 90°，但存在一些意外情况，例如 Nexus 5X 前后置都是 270°。

在 Camera1 里我们可以通过 `camera.setDisplayOrientation` 接口来控制相机的输出图像角度，但实际上无论是获取内存数据，还是获取显存数据（SurfaceTexture），这个调用都不会改变数据，它只是影响了相机输出数据时携带的变换矩阵的方向。Camera2 里没有相应的接口，但相机服务会自动为我们合理调整变换矩阵方向，所以相当于我们正确地调用了类似的接口。

如果利用 `camera.setPreviewDisplay` 或者 `camera.setPreviewTexture` 实现预览，那 `camera.setDisplayOrientation` 确实会让预览出来的图像方向发生变化，因为相机服务在渲染到 SurfaceView/TextureView 时会应用变换矩阵，使得预览画面是旋转之后的画面。

除了方向还有一个镜像的问题，Camera1 在前置摄像头时会自动为我们翻转一下画面（当然也只是修改了变换矩阵），例如前置摄像头输出的图像方向是逆时针旋转 270° 时，那就会把图像上下翻转，如果我们再设置一个旋转 90°，把图像旋正，那就相当于是左右翻转，也就达到了镜像的效果，即：前置摄像头我们用左手摸左边的脸，预览里也是显示在屏幕左边。

至于怎么设置 `camera.setPreviewDisplay` 的参数，使得直接预览可以方向正确，可以使用以下代码：

```
private static int getRotationDegree(int cameraId) {
    int orientation = 0;
    WindowManager wm = (WindowManager) applicationContext
        .getSystemService(Context.WINDOW_SERVICE);
    switch (wm.getDefaultDisplay().getRotation()) {
      case Surface.ROTATION_90:
        orientation = 90;
        break;
      case Surface.ROTATION_180:
        orientation = 180;
        break;
      case Surface.ROTATION_270:
        orientation = 270;
        break;
      case Surface.ROTATION_0:
      default:
        orientation = 0;
        break;
    }

    if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
        return (720 - (cameraInfo.orientation + orientation)) % 360;
    } else {
        return (360 - orientation + cameraInfo.orientation) % 360;
    }
}
```

### SurfaceTextureHelper

`SurfaceTextureHelper` 负责创建 `SurfaceTexture`，接收 `SurfaceTexture` 数据，相机线程的管理。

创建 `SurfaceTexture` 有几点注意事项：

- 创建 OpenGL texture 时所在的线程需要准备好 GL 上下文，WebRTC 中将这部分逻辑封装在 `EglBase` 类中；
- 创建 `SurfaceTexture` 所在的线程，将是其数据回调 `onFrameAvailable` 发生的线程；不过 API 21 引入了一个新的重载版本，支持指定回调所在线程的 Handler；

### 问题

- 利用 SurfaceTexture 接收帧数据，有些机型可能获取到的数据是黑屏（MX5 遇到过）：[需要设置 SurfaceTexture 的 buffer size，`surfaceTexture.setDefaultBufferSize`](http://stackoverflow.com/a/34337226/3077508)

- 利用 SurfaceTexture 接收帧数据，通过 `SurfaceTexture.getTimestamp` 接口获取时间戳，这个时间戳是相对时间，而且前面会有几帧值为 0：相对时间的问题可以在首帧记录下和物理时间的差值，然后计算后续每帧的物理时间戳，但头几帧时间戳为 0，所以我们记下差值就得等到非零时，而头几帧则可以直接使用物理时间作为时间戳；

- `surfaceTexture.updateTexImage` 和 `eglSwapBuffers` 会发生死锁，我们需要自行加锁：

  ```
    // SurfaceTexture.updateTexImage apparently can compete and deadlock with eglSwapBuffers,
    // as observed on Nexus 5. Therefore, synchronize it with the EGL functions.
    // See https://bugs.chromium.org/p/webrtc/issues/detail?id=5702 for more info.
    synchronized (EglBase.lock) {
    surfaceTexture.updateTexImage();
    }
  
    synchronized (EglBase.lock) {
    EGL14.eglSwapBuffers(eglDisplay, eglSurface);
    }
  ```

- 有些机型上，用 `TextureView` 实现预览，`onSurfaceTextureAvailable` 回调不会被调用，导致无法开启预览，这个问题有可能可以通过开启硬件加速得以解决（参考 [StackOverflow 这个问题](https://stackoverflow.com/a/28895727/3077508)），但有可能这个办法也不管用。在一款 OPPO 4.3 的手机上，发现延迟一会儿重设一次 `LayoutParams` 就能触发，所以就先这么搞了；

# 安卓预览实现

### 获取数据

WebRTC 的数据采集由 `VideoCapturer` 完成，`VideoCapturer` 定义了一个 `CapturerObserver` 来接收采集到的数据。而相机数据的输出，无外乎两个途径：`Camera.PreviewCallback`（Camera1） 和 `SurfaceTexture`（Camera1 和 Camera2）。当然，[Camera2 也可以获取 YUV 内存数据](https://github.com/Piasy/CameraCompat/blob/master/CameraCompat/src/main/java/com/github/piasy/cameracompat/compat/Camera2PreviewCallback.java#L45)。

`camera.setPreviewCallbackWithBuffer` 的调用在 `Camera1Session` 中，取得内存数据后将一路回调通知到 `VideoCapturer#onByteBufferFrameCaptured`。

为 `SurfaceTexture` 设置数据回调 `surfaceTexture.setOnFrameAvailableListener` 的调用则在 `SurfaceTextureHelper` 中，显存数据更新后将一路回调通知到`VideoCapturer#onTextureFrameCaptured`。

`CapturerObserver` 只有一个实现类，那就是 `AndroidVideoTrackSourceObserver`，而它在收到数据之后负责把数据抛到 native 层，WebRTC 在 native 层做了很多事情，包括图像处理、编码（软）、传输等，`AndroidVideoTrackSourceObserver` 是帧数据从 Java 层到 native 层的起点。

帧数据在 native 层的旅途我们暂且跳过，下面我们从渲染的终点继续探寻数据流动的线索。

### 渲染

WebRTC 里用的是 `SurfaceView`，没有使用 `GLSurfaceView`。 如果使用 `GLSurfaceView` 我们的渲染模式就只有 continously 和 when dirty 了，而如果我们自己管理 OpenGL 环境，那我们的渲染将是完全自定义的。

WebRTC 的渲染接口定义为 `VideoRenderer`，它用于预览的实现就是 `SurfaceViewRenderer`。

#### OpenGL 和 EGL

OpenGL 环境管理，其实就是 EGL 环境的管理：`EGLContext`，`EGLSurface` 和 `EGLDisplay`。

- `EGLContext` 是一个容器，里面存储着各种内部的状态（view port，texture 等）以及对这个 context 待执行的 GL 指令，可以说它存储着渲染的输入（配置和指令）；
- `EGLSurface` 则是一个 buffer，存储着渲染的输出（a color buffer, a depth buffer, and a stencil buffer），它有两种类型，`EGL_SINGLE_BUFFER` 和 `EGL_BACK_BUFFER`，single 就是只有一个 buffer，在里面画了就立即显示到了 display 上，而 back 则有两个 buffer，一个用于在前面显示，一个用于在后面绘制，绘制完了就用 `eglSwapBuffers` 进行切换；
- `EGLDisplay` 是和“操作系统的窗口系统”的一个连接，它代表了一个显示窗口，我们最常用的是系统默认的显示窗口（屏幕）；

### SurfaceViewRenderer 和 EglRenderer

WebRTC 把 EGL 的操作封装在了 `EglBase` 中，并针对 EGL10 和 EGL14 提供了不同的实现，而 OpenGL 的绘制操作则封装在了 `EglRenderer` 中。视频数据在 native 层处理完毕后会抛出到 `VideoRenderer.Callbacks#renderFrame` 回调中，在这里也就是 `SurfaceViewRenderer#renderFrame`，而 `SurfaceViewRenderer` 又会把数据交给 `EglRenderer` 进行渲染。所以实际进行渲染工作的主角就是 `EglRenderer` 和 `EglBase14`（EGL14 实现）了。

`EglRenderer` 实际的渲染代码在 `renderFrameOnRenderThread` 中，前面已经提到，GLES API 的调用都需要在创建了 `EGLContext` 的线程调用，在 `EglRenderer` 中这个线程就是 RenderThread，也就是 `renderThreadHandler` 对应的线程。

由于这里出现了异步，而且提交的 `Runnable` 并不是每次创建一个匿名对象，所以我们就需要考虑如何传递帧数据，`EglRenderer` 的实现：它先把需要渲染的帧保存在 `pendingFrame` 成员变量中，保存好后异步执行 `renderFrameOnRenderThread`，在其中首先把 `pendingFrame` 的值保存在局部变量中，然后将其置为 null，这样就实现了一个“接力”的效果，利用一个成员变量，把帧数据从 `renderFrame` 的参数传递到了 `renderFrameOnRenderThread` 的局部变量中。当然这个接力的过程需要加锁，以保证多线程安全，一旦完成接力，双方的操作就无需加锁了，这样能有效减少加锁的范围，提升性能。

`renderFrameOnRenderThread` 中会调用 `GlDrawer` 的 `drawOes`/`drawYuv` 来绘制 OES 纹理数据/YUV 内存数据。绘制完毕后，调用 `eglBase.swapBuffers` 交换 Surface 的前后 buffer，把绘制的内容显示到屏幕上。

### GlRectDrawer

`GlDrawer` 的实现是 `GlRectDrawer`，在这里我们终于见到了期待已久的 shader 代码、vertex 坐标和 texture 坐标。

```java
private static final String VERTEX_SHADER_STRING =
      "varying vec2 interp_tc;\n"
    + "attribute vec4 in_pos;\n"
    + "attribute vec4 in_tc;\n"
    + "\n"
    + "uniform mat4 texMatrix;\n"
    + "\n"
    + "void main() {\n"
    + "    gl_Position = in_pos;\n"
    + "    interp_tc = (texMatrix * in_tc).xy;\n"
    + "}\n";

private static final String OES_FRAGMENT_SHADER_STRING =
      "#extension GL_OES_EGL_image_external : require\n"
    + "precision mediump float;\n"
    + "varying vec2 interp_tc;\n"
    + "\n"
    + "uniform samplerExternalOES oes_tex;\n"
    + "\n"
    + "void main() {\n"
    + "  gl_FragColor = texture2D(oes_tex, interp_tc);\n"
    + "}\n";

private static final FloatBuffer FULL_RECTANGLE_BUF = GlUtil.createFloatBuffer(new float[] {
    -1.0f, -1.0f, // Bottom left.
    1.0f, -1.0f, // Bottom right.
    -1.0f, 1.0f, // Top left.
    1.0f, 1.0f, // Top right.
});
```

如果我们需要对图像进行旋转操作，`Matrix.rotateM`搭配 `Matrix.translateM` 才能正常。



# 视频硬编码实现

### MediaCodec 基础

![](image\media_codec_workflow.png)

调用流程：

- 选择编码器：根据 `mimeType` 和 `colorFormat`，以及是否为编码器，选择出一个 `MediaCodecInfo`；
- 创建编码器：`MediaCodec.createByCodecName(codecInfo.getName())`；
- 对于 API 21 以上的系统，我们可以选择异步消费输出：`mVideoCodec.setCallback`；
- 配置编码器：设置各种编码器参数（`MediaFormat`），再调用 `mVideoCodec.configure`（*文档也没有明确说 setCallback 应该在 configure 之前，但既然示例是这样写的，我们还是保持这样好了，[毕竟相机采集也是踩过坑了的](https://blog.piasy.com/2017/07/24/WebRTC-Android-Camera-Capture/#section-1)*）；
- 对于 API 19 以上的系统，我们可以选择 Surface 输入：`mVideoCodec.createInputSurface`；
- 启动编码器：`mVideoCodec.start`；
- 输入数据到编码器：输入数据到来时，Surface 输入模式下（*提前用 Surface 创建 EGLSurface*），调用 GLES API 绘制，最后 `eglSwapBuffers` 即可；普通模式下我们需要 `dequeueInputBuffer`、填入数据、`queueInputBuffer`；
- 消费编码器输出数据：异步模式下，我们在 `onOutputBufferAvailable` 中使用 buffer 内的数据，然后 `releaseOutputBuffer` 即可；同步模式下我们需要 `dequeueOutputBuffer`、使用 buffer 内的数据、`releaseOutputBuffer`；
- 停止并销毁编码器：先告知编码器我们要结束编码，Surface 输入时调用 `mVideoCodec.signalEndOfInputStream`，普通输入则可以为在 `queueInputBuffer` 时指定 `MediaCodec.BUFFER_FLAG_END_OF_STREAM` 这个 flag；告知编码器后我们就可以等到编码器输出的 buffer 带着 `MediaCodec.BUFFER_FLAG_END_OF_STREAM` 这个 flag 了，等到之后我们调用 `mVideoEncoder.release` 销毁编码器；

### 数据送进编码器

WebRTC 的硬编码封装在 `MediaCodecVideoEncoder` 类中, 接口的调用都发生在 native 层，就在 `webrtc/sdk/android/src/jni/androidmediaencoder_jni.cc` 这个文件

- 初始化编码器、EGL 环境：`initEncode`，其中会选择合适的编码器配置并启动，使用 Surface 模式输入时，我们会创建 `EglBase14` 和 `GlRectDrawer`，用于把 texture 数据绘制到编码器的输入 Surface 中；
- 输入 texture 数据到编码器：`encodeTexture`，其中的代码比较简单，GL 绘制 `drawer.drawOes`，交换 buffer `eglBase.swapBuffers`；但这里有一个 `checkKeyFrameRequired` 调用，我们会在流控部分展开讲；
- 输入内存数据到编码器：`dequeueInputBuffer` 和 `encodeBuffer`，其中的代码和官方样例没什么区别；

### 从编码器取数据

- `dequeueOutputBuffer` 取出一帧数据；
- `releaseOutputBuffer` 归还输出 buffer；

但这里有几点值得一提：

- 通常编码传输时每个关键帧头部都需要带上编码配置数据（PPS，SPS），但 MediaCodec 会在首次输出时专门输出编码配置数据，后面的关键帧里是不携带这些数据的，所以需要我们手动做一个拼接；
- 这里调用 `mediaCodec.dequeueOutputBuffer` 时第二个参数 timeout 传的是 0，表示不会等待，由于这里并没有一个单独的线程不停调用，反倒可以防止阻塞，但如果我们单独起了一个线程专门取输出数据，那这就会导致 CPU 资源的浪费了，可以加上一个合适的值，例如 3~10ms；

### 安卓硬编码流控

MediaCodec 流控相关的接口并不多，一是配置时设置目标码率和码率控制模式，二是动态调整目标码率（API 19+）。

**配置时指定目标码率和码率控制模式**：

```
mediaFormat.setInteger(MediaFormat.KEY_BIT_RATE, bitRate);
mediaFormat.setInteger(MediaFormat.KEY_BITRATE_MODE,
        MediaCodecInfo.EncoderCapabilities.BITRATE_MODE_VBR);
// 其他配置

mVideoCodec.configure(mediaFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
```

码率控制模式在 `MediaCodecInfo.EncoderCapabilities` 类中定义了三种，在 [framework 层有另一套名字](http://androidxref.com/7.1.1_r6/xref/frameworks/native/include/media/openmax/OMX_Video.h#237)和它们的值一一对应：

- CQ 对应于 `OMX_Video_ControlRateDisable`，它表示完全不控制码率，尽最大可能保证图像质量；
- CBR 对应于 `OMX_Video_ControlRateConstant`，它表示编码器会尽量把输出码率控制为设定值，即我们前面提到的“不为所动”；
- VBR 对应于 `OMX_Video_ControlRateVariable`，它表示编码器会根据图像内容的复杂度（实际上是帧间变化量的大小）来动态调整输出码率，图像复杂则码率高，图像简单则码率低；

**动态调整目标码率**：

```
Bundle param = new Bundle();
param.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, bitrate);
mediaCodec.setParameters(param);
```

WebRTC 使用的是 CBR，稳定可控是 CBR 的优点

- VBR 在画面内容保持静止时，码率会降得很低，一旦画面内容开始动起来，码率上升会跟不上，就会导致画面质量很差；
- VBR 上调码率后，有可能导致中间网络路径的丢包/延迟增加，进而导致问题；
- CBR 会存在关键帧后的几帧内容模糊的问题，如果关键帧间隔较短，可以观察到明显的「呼吸效应」；
- WebRTC 使用的方案是 CBR + 长关键帧间隔，这样「呼吸效应」就不是那么明显，而 CBR 确实能增强画面质量；

### 关键帧

MediaCodec 有两种方式触发输出关键帧，一是由配置时设置的 `KEY_FRAME_RATE` 和 `KEY_I_FRAME_INTERVAL` 参数自动触发，二是运行过程中通过 `setParameters` 手动触发输出关键帧。

自动触发实际是按照帧数触发的，例如设置帧率为 25 fps，关键帧间隔为 2s，那就会每 50 帧输出一个关键帧，一旦实际帧率低于配置帧率，那就会导致关键帧间隔时间变长。由于 MediaCodec 启动后就不能修改配置帧率/关键帧间隔了，所以如果希望改变关键帧间隔帧数，就必须重启编码器。

手动触发输出关键帧：

```
Bundle param = new Bundle();
param.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);
mediaCodec.setParameters(param);
```

Capturer/Renderer/Encoder的简要封装demo：https://github.com/Piasy/VideoCRE



# P2P连接过程和 DataChannel 使用

### P2P 连接过程

![](image\p2p_connect_procedure2.jpg)



### SDP

SDP 全称 Session Description Protocol，顾名思义，它是一种描述会话（Session）的协议。一次电话会议，一次网络电话，一次视频流传输等等，都是一次会话。那会话需要哪些描述呢？最基础的有多媒体数据格式和网络传输地址，当然还包括很多其他的配置信息。

一次会话用什么配置，就涉及到一个协商的过程了，会话发起者先提出一些建议（offer），其他人参与者再根据 offer 给出自己的选择（answer），最终意见达成一致后，才能开始会话。

offer 和 answer 其实都是 SDP，而 local/remote 则是相对的，offer 是会话发起者的 local SDP，是会话加入者的 remote SDP，answer 则是会话发起者的 remote SDP，是会话加入者的 local SDP。

SDP 实际上就是一个字符串，它的具体格式定义，可以参考 [RFC 文档](https://tools.ietf.org/html/rfc4566)。它的拼接过程，native 和 Java 代码都有分布，native 代码调用栈还比较深，这里就不展开了，`createOffer` 主要逻辑就是根据创建 `PeerConnection` 对象时指定的 `MediaConstraints`，以及在 `createOffer` 调用前添加的`VideoTrack`/`AudioTrack`/`DataChannel` 情况，拼出初始 SDP，最后在 `PeerConnectionClient.SDPObserver#onCreateSuccess` 中会添加 codec 相关的值。`createAnswer` 则还会参考 offer SDP 的值。

### ICE

ICE 是用于 UDP 媒体传输的 NAT 穿透协议（适当扩展也能支持 TCP 协议），是对 Offer/Answer 模型的扩展，它会利用 STUN、TURN 协议完成工作。ICE 会在 SDP 中增加传输地址记录值（IP + port + 协议），然后对其进行连通性测试，测试通过之后就可以用于发送媒体数据了。

#### candidate

每个传输地址记录值都叫做一个 candidate，candidate 可能有三种：

- 客户端从本机网络接口上获取的地址（host）；
- STUN server 看到的该客户端的地址（server reflexive，缩写为 srflx）；
- TURN server 为该客户端分配的中继地址（relayed）；

两个客户端上述 candidate 的任意组合也许都能连通，但实际上很多组合都不可用。

网络接口地址对应的端口号是客户端自己分配的，如果有多个网络接口地址，那就都要带着。TURN server 可以同时取得 reflexive 和 relayed candidate，而 STUN server 则只能取得 reflexive candidate（*这下就清楚 [coturn](https://github.com/coturn/coturn) 到底是 STUN server 还是 TURN server 了*）。

#### 连通性检查

candidate 收集完毕后，双方的 candidate 两两配对，然后分三步对 candidate 组合进行连通性检查：

- 把 candidates 组合按优先级排序；
- 按顺序发送检查请求（STUN Binding request），源地址是 candidate 组合的本地 candidate，目的地址是对方 candidate；
- 收到对方的检查请求后发出响应（STUN Binding response）；

每次检查实际上是一个四步握手的过程：

![](image\ice_handshake.png)

STUN 请求和 RTP/RTCP 传输数据使用的是完全一样的地址和端口，解多路复用并不是 ICE 的任务，而是 RTP/RTCP 的任务。

客户端收到的 STUN Binding respose 中也会携带对方的公网地址，如果这个地址和发送请求的 request 地址不一致，那 response 里的地址也会作为一个新的 candidate（peer reflexive），参与到连通性检查中。

如果客户端收到了对方的检查请求，除了发送响应外，也会立即对这个 candidate 组合进行检查，以加快完成一次成功的连通性检查。

#### candidate 排序

每个客户端会为自己的 candidate 设置权值，双方 candidate 权值之和将作为组合的权值，用于排序。求和的方式确保了双方排序结果的一致性，这个一致性至关重要，因为通常 NAT 都不会允许外部主机的数据包从某个端口进入内网，除非这个端口有数据包发往过这个主机，因此只有双方都发送了检查请求，数据包才可能通过 NAT。

权值的确定，RFC 里面只说明了基本原则：直接的连接比间接的连接要好。但具体如何设置，并没有具体说明。

### 收集 candidate

candidate 的收集包括两部分：一是 host，二是 srflx 和 relayed。第一部分肯定得在本地网络接口上做文章，第二部分则需要连接 STUN/TURN Server。

candidate 的收集由设备网络连接变化触发：

![img](https://imgs.piasy.com/2017-08-30-init_pc_start_network_monitor.jpg)

![img](https://imgs.piasy.com/2017-08-30-network_change_trigger_ice_candidate_gathering.jpg)

实际收集 candidate 的过程分为几个阶段：Udp，Relay，Tcp，SslTcp。下面重点分析 Udp 和 Relay 这两个阶段，在这两个阶段里，我们会收集 host，server reflexive 和 relayed 这三种 candidate。

![img](https://imgs.piasy.com/2017-08-30-do_allocate.jpg)

![img](https://imgs.piasy.com/2017-08-30-create_udp_ports.jpg)

![img](https://imgs.piasy.com/2017-08-30-create_stun_ports.jpg)

![img](https://imgs.piasy.com/2017-08-30-create_relay_ports.jpg)

三种 candidate 都会汇报到 `BasicPortAllocatorSession::OnCandidateReady` 处，从这里最终到达 Java 层的 listener 又还有好几层关卡呢：

![img](https://imgs.piasy.com/2017-08-30-candidate_to_java.jpg)

上面的过程主要有三个不直接的东西：

- sig slot：简言之就是一个信号处理的框架，A 发一个信号，B 能接收处理，二者完全解耦，具体的可以看看[官方文档](https://github.com/KubaO/sigslot/blob/master/sigslot.pdf)；
- message：类似于 Java 里面的 Handler 机制，也是提交消息，接收者进行相关处理，为啥有了 sig slot 还要 message 机制呢？sig slot 无法发送延迟消息是原因之一；
- 网络：STUN/TURN Server 的访问都是网络请求，为了实现跨平台，网络相关的代码做了不少封装，并且使用的都是操作系统的 C/C++ 接口，这块我也还没有深入看；

另外这里推荐一个 STUN/TURN Server 测试工具：[Trickle ICE](https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice)，用来测试服务器是否正确部署，以便排查问题。

## 使用 candidate

交换了 candidate 之后，WebRTC 会建立连接，发送 STUN ping 检查 candidate 连通性。连通性检查通过后，再交换 DTLS 证书，最后就可以发送音视频数据了。整个过程涉及的代码比较多（*中间的步骤我也还没捋得特别清楚*），这里就只描述几个关键路径了：

![img](https://imgs.piasy.com/2017-08-31-use_candidate_connect.jpg)

## DataChannel 使用

最艰难的部分终于过去了，现在让我们来点轻松的，基于 DataChannel 实现一个 P2P 文字聊天功能。

DataChannel 是 WebRTC 提供的任意数据 P2P 传输的 API，它使用 [SCTP 协议](https://tools.ietf.org/html/rfc4960)，可以灵活配置是否可靠传输。我们可以用它实现文字聊天、文件分享、实时对战游戏等场景下的数据传输，P2P + DTLS 保证了传输数据的安全性。

为了使用 DataChannel，我们先得创建 PeerConnection 对象，而且完成 P2P 连接的建立，具体过程经过上面的分析，我们应该已经了然于胸了，下面只摘录关键代码，完整代码可以查看[这个 GitHub 提交](https://github.com/Piasy/AppRTC-Android/commit/a0c0e11d404645a9886fca7846bed2c82933a13c)。

```
// 初始化并创建 factory
PeerConnectionFactory.initializeAndroidGlobals(mAppContext, true);
mPeerConnectionFactory = new PeerConnectionFactory(null);

// 创建 PC 对象
mPeerConnection = mPeerConnectionFactory.createPeerConnection(rtcConfig,
    new MediaConstraints(), this);

// 创建 DataChannel
DataChannel.Init init = new DataChannel.Init();
init.ordered = true;
init.negotiated = true; // false is ok
init.maxRetransmits = -1;
init.maxRetransmitTimeMs = -1;
init.id = 0; // must be set, and >= 0
mDataChannel = mPeerConnection.createDataChannel("P2P MSG DC", init);
mDataChannel.registerObserver(this);

// A，创建 offer
mPeerConnection.createOffer(MsgPcClient.this, mSdpConstraints);
// 在 onCreateSuccess 回调中 setLocalDescription
// 在 onSetSuccess 回调中把 offer 发出去
mPeerConnection.setLocalDescription(MsgPcClient.this, sdp);

// B，收到 offer 后 setRemoteDescription
mPeerConnection.setRemoteDescription(MsgPcClient.this, sdp);
// 创建 answer
mPeerConnection.createAnswer(MsgPcClient.this, mSdpConstraints);
// 在 onCreateSuccess 回调中 setLocalDescription
// 在 onSetSuccess 回调中把 answer 发出去
mPeerConnection.setLocalDescription(MsgPcClient.this, sdp);

// A，收到 answer 后 setRemoteDescription
mPeerConnection.setRemoteDescription(MsgPcClient.this, sdp);

// 在 onIceCandidate 回调中把 candidate 发出去
// 收到对方的 candidate 后 addIceCandidate
mPeerConnection.addIceCandidate(candidate);

// 在 onDataChannel 回调中注册消息回调
dataChannel.registerObserver(this);

// 发送消息
byte[] msg = message.getBytes();
DataChannel.Buffer buffer = new DataChannel.Buffer(ByteBuffer.wrap(msg), false);
mDataChannel.send(buffer);

// onMessage 回调中处理消息
ByteBuffer data = buffer.data;
final byte[] bytes = new byte[data.capacity()];
data.get(bytes);
String msg = new String(bytes);
Logging.d(TAG, "onMessage " + msg);
```

创建 DataChannel 时可以通过 `DataChannel.Init` 的 `ordered`、`maxRetransmitTimeMs`、`maxRetransmits` 参数配置配置可靠性：

- `ordered`：是否保证顺序传输；
- `maxRetransmitTimeMs`：重传允许的最长时间；
- `maxRetransmits`：重传允许的最大次数；

































相关文章：

1.[视频压缩编码和音频压缩编码的基本原理](https://blog.csdn.net/leixiaohua1020/article/details/28114081)