[Toc]

### HDFS环境搭建

#### 1.安装vmware，共享目录：  vmware版本：10.0.0 build-1295980  ubuntu版本：14.04.4    

```
先安装的vmware workstation自带光盘中vmare tools包，安装完成后没有产生vmhgfs模块；    

可以认为此方法失败，然后根据网上的提示安装open-vm-tools，使用命令行：
sudo apt-get install open-vm-tools    
安装完成后，依然没有成功的vmhgfs模块；

最后根据https://github.com/rasa/vmware-tools-patches中的方法，执行如下命令行成功；
$ git clone https://github.com/rasa/vmware-tools-patches.git  
$ cd vmware-tools-patches 
$ sudo ./patched-open-vm-tools.sh
```

#### 2.Ubuntu14.04下安装并开启SSH服务，这样你可以用MobaXterm进行连接，执行命令了，方便复制粘贴  

```
1).打开终端窗口，输入命令,等待安装完成    
	sudo apt-get install openssh-server
2).查看SSH服务是否启动,输入命令,看到sshd则说明服务已经启动    
	sudo ps -e |grep ssh     
	如果没有启动，输入命令,sudo service ssh start 
3).修改配置文件    \
	sudo gedit /etc/ssh/sshd_config   
	把配置文件中的"PermitRootLogin without-password"加一个"#"号注释掉，再增加一句"PermitRootLogin yes"，保存文件。这样做是为了允许root用户登录。
4）设置 ssh 免密码登陆执行命令    
 (1)ssh-keygen -t rsa    
 (2)cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys     
验证：ssh hostname
```

***\*完成以上步骤之后，克隆2份slave1，slave2，为构建伪分布式系统做准备\****

#### 3.设置主机名     

sudo gedit /etc/ssh/hostname   把里面的默认主机名字删除，并修改为自己的主机名。 

#### 4.设置主机名和ip一致  

```
vi /etc/hosts
//开始下面的操作前，最好创建一个自己的工作目录，我是创建/study目录：
sudo chown yourname:yourname /study
```



#### 5.安装jdk.   解压jdk到你的工作目录下，mv jdk-1.8.0_24 jdk

```
  vi /etc/environment  增加内容如下:   
  JAVA_HOME=/study/jdk   
  PATH=.:$JAVA_HOME/bin:$PATH
  执行: source /etc/environment 命令使得当前终端窗口生效，新开窗口无效，如果想全部窗口生效，需要重启
```



#### 6.安装 hadoop, 执行命令

  1)tar -zxvf hadoop-2.7.3.tar.gz  

  2)mv hadoop-2.7.3 hadoop  

  3)vi /etc/environment 增加内容如下:   

```
 JAVA_HOME=/study/jdk    
 HADOOP_HOME=/usr/local/hadoop
 PATH=.:$HADOOP_HOME/bin:$JAVA_HOME/bin:$PATH  
```

  4)source /etc/environment

  5)修改 conf 目录下的配置文件    

​    (1).hadoop-env.sh 2.7之后不再需要修改    		

```
export JAVA_HOME=/usr/local/jdk/    
```

​    (2).core-site.xml   

```
<configuration> 
     <property>
         <name>fs.defaultFS</name>        
         <value>hdfs://hadoop0:9000</value>    //你的hostname:9000        
     </property>        
     <property>        
         <name>hadoop.tmp.dir</name>        
         <value>/study/hadoop/tmp</value>//你的工作目录下        
     </property>        
     <property>  
         <name>fs.trash.interval</name>  
         <value>1440</value>  
         <description>删除的文件或目录在".Trash"(回收站)中保留的时间.单位为分钟，为0是不保存，立即删除</description>  
     </property>  
</configuration> 
```

​     (3).hdfs-site.xml    

```
 <configuration>
     <property>
           <name>dfs.replication</name>
           <value>1</value>
        </property>
     <property>
            <name>dfs.namenode.name.dir</name>
            <value>/study/hadoop/name</value>
            <description>namenode的位置最好跟datanode不在同一磁盘</description>
     </property>
     <property>
            <name>dfs.datanode.data.dir</name>
            <value>/study/hadoop/data</value>
    </property>
</configuration> 
```

​    (4).mapred-site.xml,如果不存在，可以复制mapred-site.xml.templete改名     

```
<configuration>
     <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
     </property>
</configuration> 
```

   (5)yarn-site.xml     

```
<configuration>
            <property>
                 <name>yarn.nodemanager.aux-services</name>
                 <value>mapreduce_shuffle</value>
            </property>
            <property> 
               <name>yarn.resourcemanager.address</name> 
               <value>master:18040</value> 
               </property> 
            <property> 
               <name>yarn.resourcemanager.scheduler.address</name> 
               <value>master:18030</value> 
               </property> 
            <property> 
               <name>yarn.resourcemanager.resource-tracker.address</name>
               <value>master:18025</value> 
               </property> 
            <property> 
               <name>yarn.resourcemanager.admin.address</name> 
               <value>master:18141</value> 
               </property> 
            <property> 
               <name>yarn.resourcemanager.webapp.address</name> 
               <value>master:18088</value> 
               </property> 
</configuration> 
```

  6)格式化 HDFS：hadoop namenode -format  

  7)启动 Hadoop   可以分步启动      

```
	先启动 HDFS： sbin/start-dfs.sh
	再启动 Yarn: sbin/start-yarn.sh
        或者启动所有：sbin/start-all.sh
    检测是否启动成功：jps，启动hdfs 5个任务，yarn 3个任务  在浏览器查看:
	http://test:50070
	http://test:50030
```

#### 7.windows上的java程序无法操作hdfs处理

报错org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security .AccessControlException: Permission denied: user=Administator, access=EXECUTE, inode="/whb/mkdirs-test":wanghb:supergroup:drw-rw-rw-

用户Administator在hadoop上执行写操作时被权限系统拒绝.

```
1).只要在系统的环境变量里面添加 HADOOP_USER_NAME=wanghb 或者将当前系统的帐号修改为wanghb(你的hadoop账号)
2).也可以修改你要操作的目录，如hadoop fs -chmod 777 /whb  
3).在java代码中设置用户名，System.setProperty("HADOOP_USER_NAME", "xxxx");
```

/***\**\**\**\**\**\**\**\*****分割线*****\**\**\**\**\**\**\**\**\**\**\**\**\**\****/



###  flume配置

#### 1.关闭防火墙

```
  sudo iptables -P INPUT ACCEPT 
  sudo iptables -P FORWARD ACCEPT
  sudo iptables -P OUTPUT ACCEPT
```

#### 2.术语： 

```
  源信息保存在namenode，NameNode为了数据同步，会通过JournalNode相互通信 
  SPOF 单点故障 
  HA 高可用性 
  NN namenode 
  DN datanode
```

#### 3.Flume核心概念： 

```
	Event数据单元，由header和byte array组成 
    Client把log打包成Event发给Agent 
    Agent
    1)Source接收数据放到channel里  
    2)Channel缓存event，可以是文件或内存
    3)Sink 发送Event，成功后从channel移除
```

#### 4.Source监听： 

```
	spooldir:监听目录中文件变化，不检测内容，只支持文本 
	taildir：检测文件内容变化，只支持文本 
	exec：执行命令，将结果作为数据源 
	avro：监听端口，收集数据 等。
```

#### 5.实战：

利用flume检测目录，并写入hdfs 首先，

自己写个属性文件myflume.properties，放到flume的conf目录下      

```
 #Agent名字MyAgent
    MyAgent.sources = mysource
    MyAgent.channels = mychannel
    MyAgent.sinks = mysink

    #监听目录变化
    MyAgent.sources.mysource.type = spooldir
    #监听/tmp/logs目录文件变化
    MyAgent.sources.mysource.channels = mychannel
    MyAgent.sources.mysource.spoolDir =/tmp/logs
    
    #一定注意这里的mysink跟上面是对应的
    MyAgent.sinks.mysink.channel = mychannel
    MyAgent.sinks.mysink.type = hdfs
    MyAgent.sinks.mysink.hdfs.path = hdfs://master:8020/data/logs/%Y%m%d/%H/
    MyAgent.sinks.mysink.hdfs.batchSize = 1000
    MyAgent.sinks.mysink.hdfs.rollSize = 0
    MyAgent.sinks.mysink.hdfs.rollCount = 10000
    MyAgent.sinks.mysink.hdfs.useLocalTimeStamp = true

    #这里的mychannel跟上面是对应的
    MyAgent.channels.mychannel.type = memory
    MyAgent.channels.mychannel.capacity = 10000
    MyAgent.channels.mychannel.transactionCapacity = 100

  启动flume(注意文件名和MyAgent名字)：

    bin/flume-ng agent -n MyAgent -c conf -f \  注：\是换行符，不需要换行的删除
    conf/myflume.properties -Dflume.root.logger=DEBUG,console

```

#### 6.如果收集网络数据，更改sources配置项

    MyAgent.sources.mysource.type = avro
    MyAgent.sources.mysource.channels = mychannel
    MyAgent.sources.mysource.spoolDir =/tmp/logs


### sqoop

1.下载解压sqoop  

```
 export SQOOP_HOME=/opt/hadoop/sqoop-1.99.7
 export PATH=$PATH:$SQOOP_HOME/bin
 export SQOOP_SERVER_EXTRA_LIB=$SQOOP_HOME/extra
 export CATALINA_BASE=$SQOOP_HOME/server
 export LOGDIR=$SQOOP_HOME/logs/
```

2.修改hadoop/etc/hadoop/container-executor.cfg  

​	allowed.system.users=wanghb 运行sqoop的用户名
3.vi core-site.xml

```
<property>
    <name>hadoop.proxyuser.wanghb.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.wanghb.groups</name> 运行sqoop的用户名
    <value>*</value>
</property> 
```

4.修改配置文件sqoop/conf/sqoop.properties 

```
org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/study/hadoop/etc/hadoop
```

打开以下选项：

打开以下选项：

```
org.apache.sqoop.security.authentication.type=SIMPLE
org.apache.sqoop.security.authentication.handler=org.apache.sqoop.security.authentication.SimpleAuthenticationHandler
org.apache.sqoop.security.authentication.anonymous=true
```

然后替换@LOGDIR@ 和@BASEDIR@为logs和base,vi下切换成命令行模式(shift+:)输入：

```
0,$ s/@LOGDIR@/logs/g
0,$ s/@BASEDIR@/base/g
```

5.创建目录

```
mkdir /study/sqoop/extra
mkdir /study/sqoop/logs
```

6.mysql的驱动jar文件复制到这个目录下。

​    cp mysql-connector-java-5.1.36-bin.jar /study/sqoop/extra

7.验证配置是否有效使用sqoop2-tool工具进行验证：

​    bin/sqoop2-tool verify 

8.开启服务器

​    bin/sqoop2-server start

安装配置mysql:

9.安装mysql：  　　

```
sudo apt-get install mysql-server
sudo apt-get install mysql-client
sudo apt-get install libmysqlclient-dev

启动mysql服务：/etc/init.d/mysqld restart

连接数据库：mysql -uroot -p1234 //其中root是数据库的用户不是操作系统的,1234是密码  

创建用户
grant all on *.* to wanghb@'%' identified by 'wanghb';
grant all on *.* to wanghb@'localhost' identified by 'wanghb';
grant all on *.* to wanghb@'master' identified by 'wanghb';
flush privileges;
```

10.创建数据库：

```
	create database hadoop;
	查看： show databases;
	创建表：
	use hadoop;
	//在hadoop数据库中创建或者在创建表时指定库
	create table person(  number INT(11),  name VARCHAR(255)  )ENGINE=hadoop DEFAULT CHARSET=utf8;  插入数据：  INSERT INTO person   (number, name)  VALUES  (1001, "小明");
	查看表单：show tables from hadoop；
	退出quit; 
```

***\*做个试验，使用sqoop从mysql中导出数据到hdfs中\****
11.开启hdfs, 开启sqoop

\----------

### Kafka

1.Kafka 终端和后端存储之间的中间件 

```
由Producer，Broker，Consumer，还有zookeeper组成 
Producer：向broker发送消息，顺序追加消息 
Broker：从producer端接收消息，并可靠保存一段时间，将消息发送给订阅的consumer 
Consumer：取消息并处理，顺序读
```

2.消息组成 

​	topic：切分成1个或多个partition存储在Broker中，是个逻辑上的消息概念 

   key，   value， timestamp

 在一个Partition中同一个Producer发送的消息是顺序的，分配标示是消息关键字hash取模实现，如果数据要求存到同一个Partition，则选用同样的关键字就行

3.生命周期

​    flume 短时间存在，用完即没 

​	kafka 会缓存一段时间，默认一周 

​	hdfs 永久缓存

相似组件对比： 

```
	Kafka    hdfs  
	Broker   datanode 
	partition  block 
	topic    file/dir 
```

4.搭建环境： 

下载解压Kafka 启动 Zookeeper(集团内部可以共享)

```
bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 
```

 启动Kafaka    	

```
bin/kafka-server-start.sh -daemon config/server.properties 
```

 创建 topic，名字为 test

```
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test  
```

启动生产者生产信息，键盘输如测试信息并回车

```
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
```

启动1个终端消费数据，然后观看消费端信息  

```
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 
```

5.Java测试 

```
//这个例子不能在windows和虚拟机间通讯

    //生产者代码
    Properties props = new Properties();
    props.put("bootstrap.servers", "localhost:9092");
    props.put("acks", "all");
    props.put("retries", 0);
    props.put("batch.size", 16384);
    props.put("linger.ms", 1);
    props.put("buffer.memory", 33554432);
    props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
 
    Producer<String, String> producer = new KafkaProducer<String, String>(props);
    producer.send(new ProducerRecord<String, String>("test", "hello"));//test是topic名称

    //消费者代码
    Properties props = new Properties();
    props.put("bootstrap.servers", "localhost:9092");
    props.put("group.id", "testGroup");
    props.put("enable.auto.commit", "true");
    props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
 
    KafkaConsumer<String, String> consumer= new KafkaConsumer<String, String>(props)；
    consumer.subscribe(Arrays.asList(topic));//订阅Topic

    ConsumerRecords<String, String> records = consumer.poll(10000);
    //处理得到的数据
    for (ConsumerRecord<String, String> record : records) {
          System.out.println("topic: " + record.topic() + " ; offset" + record.offset() + " ; key: " + record.key() + " ; value: " + record.value());
    }
```

### Flume和Kafka对比

采集层主要可以使用Flume, Kafka两种技术。

Flume：Flume 是管道流方式，提供了很多的默认实现，让用户通过参数部署，及扩展API.

Kafka：Kafka是一个可持久化的分布式的消息队列。

- Kafka 是一个非常通用的系统。你可以有许多生产者和很多的消费者共享多个主题Topics。相比之下,Flume是一个专用工具被设计为旨在往HDFS,HBase发送数据。它对HDFS有特殊的优化，并且集成了Hadoop的安全特性。所以，Cloudera 建议**如果数据被多个系统消费的话，使用kafka**；如果**数据被设计给Hadoop使用，使用Flume**。 

- 正如你们所知Flume内置很多的source和sink组件。然而，Kafka明显有一个更小的生产消费者生态系统，并且Kafka的社区支持不好。希望将来这种情况会得到改善，但是目前：使用Kafka意味着你准备好了编写你自己的生产者和消费者代码。如果已经存在的Flume Sources和Sinks满足你的需求，并且你更喜欢**不需要任何开发**的系统，请使用Flume。 

- Flume可以使用拦截器实时处理数据。这些对**数据屏蔽或者过量**是很有用的。Kafka需要外部的流处理系统才能做到。 

- Kafka和Flume都是可靠的系统,通过适当的配置能保证零数据丢失。然而，**Flume不支持副本事件**。于是，如果Flume代理的一个节点崩溃了，即使使用了可靠的文件管道方式，你也将丢失这些事件直到你恢复这些磁盘。如果你需要一个高可靠行的管道，那么使用Kafka是个更好的选择。

 Flume和Kafka可以很好地结合起来使用。如果你的设计需要从Kafka到Hadoop的流数据，使用Flume代理并配置Kafka的Source读取数据也是可行的：你没有必要实现自己的消费者。你可以直接利用Flume与HDFS及HBase的结合的所有好处。你可以使用Cloudera Manager对消费者的监控，并且你甚至可以添加拦截器进行一些流处理。

Flume和Kafka可以结合起来使用。通常会使用Flume + Kafka的方式。其实如果为了利用Flume已有的写HDFS功能，也可以使用Kafka + Flume的方式。

----------

### 实验：Flume收集数据到Kafka

1.启动Zookeeper， Kafka，参考上一篇文章，创建topic，启动消费者终端

2.修改Flume的配置文件   

```
MyAgent.sinks.mysink.type = org.apache.flume.sink.kafka.KafkaSink   
MyAgent.sinks.mysink.topic = test   
MyAgent.sinks.mysink.brokerList = localhost:9092   
MyAgent.sinks.mysink.requiredAcks = 1   
MyAgent.sinks.mysink.batchSize = 20 
```

