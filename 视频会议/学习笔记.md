[Toc]

（代码下载见环境搭建文档）

三个问题：

1. 客户端之间如何建立连接？
2. 客户端之间如何实现数据传输？
3. 音视频数据的采集、预览、编码、传输、解码、渲染完整流程。

## 安卓相机采集实现分析

WebRTC-Android 的相机采集主要涉及到以下几个类：Enumerator，Capturer，Session，SurfaceTextureHelper。

其中 Enumerator 创建 Capturer，Capturer 创建 Session，实现对相机的操作，SurfaceTextureHelper 实现用 SurfaceTexture 接收数据。

### Enumerator

`CameraEnumerator` 接口如下：

```
public interface CameraEnumerator {
  public String[] getDeviceNames();
  public boolean isFrontFacing(String deviceName);
  public boolean isBackFacing(String deviceName);
  public List<CaptureFormat> getSupportedFormats(String deviceName);
  public CameraVideoCapturer createCapturer(
      String deviceName, CameraVideoCapturer.CameraEventsHandler eventsHandler);
}
```

主要是获取设备列表、检查朝向、创建 Capturer。

相关类主要是Camera2Enumerator和Camera1Enumerator

### Capturer

WebRTC 视频采集的接口定义为 `VideoCapturer`，其中定义了初始化、启停、销毁等操作，以及接收启停事件、数据的回调。相机采集的实现是 `CameraCapturer`，针对不同的相机 API 又分为 `Camera1Capturer` 和 `Camera2Capturer`。相机采集大部分逻辑都封装在 `CameraCapturer` 中，只有创建 `CameraSession` 的代码在两个子类中有不同的实现。

下面分别看看 `VideoCapturer` 几个重要的 API 实现逻辑。

#### initialize

initialize 比较简单，只是保存一下传入的相关对象。

#### startCapture

startCapture 则会先检查当前是否正在创建 session，或者已有 session 正在运行，这里保证了不会同时存在多个 session 在运行。而众多状态成员的访问都通过 `stateLock` 进行保护，避免多线程安全问题。

如果需要创建 session，则在相机操作线程创建 session，同时在主线程检测相机操作的超时。所有相机的操作都切换到了单独的相机线程，以避免造成主线程阻塞，而检查超时自然不能在相机线程，否则相机线程被阻塞住之后超时回调也不会执行。

我们发现 capturer 中并没有实际相机操作的代码，开启相机、预览的代码都封装在了 `CameraSession` 中，那这样 capturer 的逻辑就得到了简化，切换摄像头、失败重试都只需要创建 session 即可，capturer 可以专注于状态维护和错误处理的逻辑。

**CameraCapturer** 状态维护和错误处理的逻辑还是非常全面的：相机开启状态、相机运行状态、切换摄像头状态、错误重试、相机开启超时，全部都考虑到了。另外相机切换、开关相机、错误事件，统统都有回调通知。这里就充分体现出了 demo 和产品的差别，开启相机预览的 demo 十行代码就能搞定，而要全面考虑各种异常情况，就需要费一番苦心了。

不过这里仍有一点小瑕疵，错误回调的参数是字符串，虽然可以很方便的打入日志，但不利于代码判断错误类型。最好是参数使用错误码，然后准备一个错误码到错误信息的转换函数。

#### stopCapture

`stopCapture` 时会先判断是否正在创建 session，如果正在创建，那就需要等待其创建完毕。通过检查后，如果当前有 session 正在运行，就在相机线程关闭 session。

#### changeCaptureFormat

改变采集格式需要重启采集，即先 `stopCapture`，再 `startCapture`。这俩操作都是异步的，会不会有问题？这就涉及到 Handler 的一点知识了，向 Handler 提交的消息、任务，都会被加入到同一个队列中，提交到队列中的任务会保证按序执行，即先提交一定会先执行，所以这里我们不必担心关闭相机和开启相机顺序错乱。

#### switchCamera

`switchCamera` 也会先停止老的 session，再创建新的 session，只不过还需要检查相机个数、实现切换状态通知逻辑。

这块代码应该有个小问题：`startCapture` 会把 `openAttemptsRemaining` 设置为 `MAX_OPEN_CAMERA_ATTEMPTS`，但切换摄像头时只会将其设置为 1，这个不对称应该没什么道理，所以我认为应该保持一致。

### Session

前面我们已经知道，和相机 API 实际打交道的代码都在 `CameraSession` 中，这里我们就一探其究竟。

开启相机、开启预览、设置事件回调的代码都在创建 session 的工厂方法 `Camera1Session.create` 和 `Camera2Session.create` 中。停止相机和预览则定义了一个 `stop` 接口。

具体的相机 API 使用就比较简单了。

#### Camera1

- 创建 `Camera` 对象：`Camera.open`；
- 设置预览 SurfaceTexture，用来接收帧数据（位于显存中）：`camera.setPreviewTexture`；
- 选择合适的相机预览参数（尺寸、帧率、对焦）：`Parameters` 和 `camera.setParameters`；
- 如果需要获取内存数据回调，则需要设置 buffer 和 listener：`camera.addCallbackBuffer` 和 `camera.setPreviewCallbackWithBuffer`；
- 如果需要相机服务为我们调整数据方向，则可以设置旋转角度：`camera.setDisplayOrientation`；
- 开启预览：`camera.startPreview`；
- 停止预览：`camera.stopPreview` 和 `camera.release`；

#### Camera2

- 创建 `CameraManager` 对象，相机操作始于“相机管家”：`context.getSystemService(Context.CAMERA_SERVICE)`；
- 创建 `CameraDevice` 对象：`cameraManager.openCamera`；
- 和 Camera1 不同，Camera2 的操作都是异步的，调用 `openCamera` 时我们会传入一个回调，在其中接收相机操作状态的事件；
- 创建成功：`CameraDevice.StateCallback#onOpened`；
- 创建相机对象后，开启预览 session，设置数据回调：`camera.createCaptureSession`，同样，这个操作也会传入一个回调；
- session 开启成功：`CameraCaptureSession.StateCallback#onConfigured`；
- 开启 session 后，设置数据格式（尺寸、帧率、对焦），发出数据请求：`CaptureRequest.Builder` 和 `session.setRepeatingRequest`；
- 停止预览：`cameraCaptureSession.stop` 和 `cameraDevice.close`；

#### 图像方向

通常前置摄像头输出的图像方向是逆时针旋转 270° 的，后置摄像头是 90°，但存在一些意外情况，例如 Nexus 5X 前后置都是 270°。

在 Camera1 里我们可以通过 `camera.setDisplayOrientation` 接口来控制相机的输出图像角度，但实际上无论是获取内存数据，还是获取显存数据（SurfaceTexture），这个调用都不会改变数据，它只是影响了相机输出数据时携带的变换矩阵的方向。Camera2 里没有相应的接口，但相机服务会自动为我们合理调整变换矩阵方向，所以相当于我们正确地调用了类似的接口。

如果利用 `camera.setPreviewDisplay` 或者 `camera.setPreviewTexture` 实现预览，那 `camera.setDisplayOrientation` 确实会让预览出来的图像方向发生变化，因为相机服务在渲染到 SurfaceView/TextureView 时会应用变换矩阵，使得预览画面是旋转之后的画面。

除了方向还有一个镜像的问题，Camera1 在前置摄像头时会自动为我们翻转一下画面（当然也只是修改了变换矩阵），例如前置摄像头输出的图像方向是逆时针旋转 270° 时，那就会把图像上下翻转，如果我们再设置一个旋转 90°，把图像旋正，那就相当于是左右翻转，也就达到了镜像的效果，即：前置摄像头我们用左手摸左边的脸，预览里也是显示在屏幕左边。

至于怎么设置 `camera.setPreviewDisplay` 的参数，使得直接预览可以方向正确，可以使用以下代码：

```
private static int getRotationDegree(int cameraId) {
    int orientation = 0;
    WindowManager wm = (WindowManager) applicationContext
        .getSystemService(Context.WINDOW_SERVICE);
    switch (wm.getDefaultDisplay().getRotation()) {
      case Surface.ROTATION_90:
        orientation = 90;
        break;
      case Surface.ROTATION_180:
        orientation = 180;
        break;
      case Surface.ROTATION_270:
        orientation = 270;
        break;
      case Surface.ROTATION_0:
      default:
        orientation = 0;
        break;
    }

    if (cameraInfo.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) {
        return (720 - (cameraInfo.orientation + orientation)) % 360;
    } else {
        return (360 - orientation + cameraInfo.orientation) % 360;
    }
}
```

### SurfaceTextureHelper

`SurfaceTextureHelper` 负责创建 `SurfaceTexture`，接收 `SurfaceTexture` 数据，相机线程的管理。

创建 `SurfaceTexture` 有几点注意事项：

- 创建 OpenGL texture 时所在的线程需要准备好 GL 上下文，WebRTC 中将这部分逻辑封装在 `EglBase` 类中；
- 创建 `SurfaceTexture` 所在的线程，将是其数据回调 `onFrameAvailable` 发生的线程；不过 API 21 引入了一个新的重载版本，支持指定回调所在线程的 Handler；

### 问题

- 利用 SurfaceTexture 接收帧数据，有些机型可能获取到的数据是黑屏（MX5 遇到过）：[需要设置 SurfaceTexture 的 buffer size，`surfaceTexture.setDefaultBufferSize`](http://stackoverflow.com/a/34337226/3077508)

- 利用 SurfaceTexture 接收帧数据，通过 `SurfaceTexture.getTimestamp` 接口获取时间戳，这个时间戳是相对时间，而且前面会有几帧值为 0：相对时间的问题可以在首帧记录下和物理时间的差值，然后计算后续每帧的物理时间戳，但头几帧时间戳为 0，所以我们记下差值就得等到非零时，而头几帧则可以直接使用物理时间作为时间戳；

- `surfaceTexture.updateTexImage` 和 `eglSwapBuffers` 会发生死锁，我们需要自行加锁：

  ```
    // SurfaceTexture.updateTexImage apparently can compete and deadlock with eglSwapBuffers,
    // as observed on Nexus 5. Therefore, synchronize it with the EGL functions.
    // See https://bugs.chromium.org/p/webrtc/issues/detail?id=5702 for more info.
    synchronized (EglBase.lock) {
    surfaceTexture.updateTexImage();
    }
  
    synchronized (EglBase.lock) {
    EGL14.eglSwapBuffers(eglDisplay, eglSurface);
    }
  ```

- 有些机型上，用 `TextureView` 实现预览，`onSurfaceTextureAvailable` 回调不会被调用，导致无法开启预览，这个问题有可能可以通过开启硬件加速得以解决（参考 [StackOverflow 这个问题](https://stackoverflow.com/a/28895727/3077508)），但有可能这个办法也不管用。在一款 OPPO 4.3 的手机上，发现延迟一会儿重设一次 `LayoutParams` 就能触发，所以就先这么搞了；

## 安卓预览实现

### 获取数据

WebRTC 的数据采集由 `VideoCapturer` 完成，`VideoCapturer` 定义了一个 `CapturerObserver` 来接收采集到的数据。而相机数据的输出，无外乎两个途径：`Camera.PreviewCallback`（Camera1） 和 `SurfaceTexture`（Camera1 和 Camera2）。当然，[Camera2 也可以获取 YUV 内存数据](https://github.com/Piasy/CameraCompat/blob/master/CameraCompat/src/main/java/com/github/piasy/cameracompat/compat/Camera2PreviewCallback.java#L45)。

`camera.setPreviewCallbackWithBuffer` 的调用在 `Camera1Session` 中，取得内存数据后将一路回调通知到 `VideoCapturer#onByteBufferFrameCaptured`。

为 `SurfaceTexture` 设置数据回调 `surfaceTexture.setOnFrameAvailableListener` 的调用则在 `SurfaceTextureHelper` 中，显存数据更新后将一路回调通知到`VideoCapturer#onTextureFrameCaptured`。

`CapturerObserver` 只有一个实现类，那就是 `AndroidVideoTrackSourceObserver`，而它在收到数据之后负责把数据抛到 native 层，WebRTC 在 native 层做了很多事情，包括图像处理、编码（软）、传输等，`AndroidVideoTrackSourceObserver` 是帧数据从 Java 层到 native 层的起点。

帧数据在 native 层的旅途我们暂且跳过，下面我们从渲染的终点继续探寻数据流动的线索。

### 渲染

WebRTC 里用的是 `SurfaceView`，没有使用 `GLSurfaceView`。 如果使用 `GLSurfaceView` 我们的渲染模式就只有 continously 和 when dirty 了，而如果我们自己管理 OpenGL 环境，那我们的渲染将是完全自定义的。

WebRTC 的渲染接口定义为 `VideoRenderer`，它用于预览的实现就是 `SurfaceViewRenderer`。

#### OpenGL 和 EGL

OpenGL 环境管理，其实就是 EGL 环境的管理：`EGLContext`，`EGLSurface` 和 `EGLDisplay`。

- `EGLContext` 是一个容器，里面存储着各种内部的状态（view port，texture 等）以及对这个 context 待执行的 GL 指令，可以说它存储着渲染的输入（配置和指令）；
- `EGLSurface` 则是一个 buffer，存储着渲染的输出（a color buffer, a depth buffer, and a stencil buffer），它有两种类型，`EGL_SINGLE_BUFFER` 和 `EGL_BACK_BUFFER`，single 就是只有一个 buffer，在里面画了就立即显示到了 display 上，而 back 则有两个 buffer，一个用于在前面显示，一个用于在后面绘制，绘制完了就用 `eglSwapBuffers` 进行切换；
- `EGLDisplay` 是和“操作系统的窗口系统”的一个连接，它代表了一个显示窗口，我们最常用的是系统默认的显示窗口（屏幕）；

### SurfaceViewRenderer 和 EglRenderer

WebRTC 把 EGL 的操作封装在了 `EglBase` 中，并针对 EGL10 和 EGL14 提供了不同的实现，而 OpenGL 的绘制操作则封装在了 `EglRenderer` 中。视频数据在 native 层处理完毕后会抛出到 `VideoRenderer.Callbacks#renderFrame` 回调中，在这里也就是 `SurfaceViewRenderer#renderFrame`，而 `SurfaceViewRenderer` 又会把数据交给 `EglRenderer` 进行渲染。所以实际进行渲染工作的主角就是 `EglRenderer` 和 `EglBase14`（EGL14 实现）了。

`EglRenderer` 实际的渲染代码在 `renderFrameOnRenderThread` 中，前面已经提到，GLES API 的调用都需要在创建了 `EGLContext` 的线程调用，在 `EglRenderer` 中这个线程就是 RenderThread，也就是 `renderThreadHandler` 对应的线程。

由于这里出现了异步，而且提交的 `Runnable` 并不是每次创建一个匿名对象，所以我们就需要考虑如何传递帧数据，`EglRenderer` 的实现：它先把需要渲染的帧保存在 `pendingFrame` 成员变量中，保存好后异步执行 `renderFrameOnRenderThread`，在其中首先把 `pendingFrame` 的值保存在局部变量中，然后将其置为 null，这样就实现了一个“接力”的效果，利用一个成员变量，把帧数据从 `renderFrame` 的参数传递到了 `renderFrameOnRenderThread` 的局部变量中。当然这个接力的过程需要加锁，以保证多线程安全，一旦完成接力，双方的操作就无需加锁了，这样能有效减少加锁的范围，提升性能。

`renderFrameOnRenderThread` 中会调用 `GlDrawer` 的 `drawOes`/`drawYuv` 来绘制 OES 纹理数据/YUV 内存数据。绘制完毕后，调用 `eglBase.swapBuffers` 交换 Surface 的前后 buffer，把绘制的内容显示到屏幕上。

### GlRectDrawer

`GlDrawer` 的实现是 `GlRectDrawer`，在这里我们终于见到了期待已久的 shader 代码、vertex 坐标和 texture 坐标。

```java
private static final String VERTEX_SHADER_STRING =
      "varying vec2 interp_tc;\n"
    + "attribute vec4 in_pos;\n"
    + "attribute vec4 in_tc;\n"
    + "\n"
    + "uniform mat4 texMatrix;\n"
    + "\n"
    + "void main() {\n"
    + "    gl_Position = in_pos;\n"
    + "    interp_tc = (texMatrix * in_tc).xy;\n"
    + "}\n";

private static final String OES_FRAGMENT_SHADER_STRING =
      "#extension GL_OES_EGL_image_external : require\n"
    + "precision mediump float;\n"
    + "varying vec2 interp_tc;\n"
    + "\n"
    + "uniform samplerExternalOES oes_tex;\n"
    + "\n"
    + "void main() {\n"
    + "  gl_FragColor = texture2D(oes_tex, interp_tc);\n"
    + "}\n";

private static final FloatBuffer FULL_RECTANGLE_BUF = GlUtil.createFloatBuffer(new float[] {
    -1.0f, -1.0f, // Bottom left.
    1.0f, -1.0f, // Bottom right.
    -1.0f, 1.0f, // Top left.
    1.0f, 1.0f, // Top right.
});
```

如果我们需要对图像进行旋转操作，`Matrix.rotateM`搭配 `Matrix.translateM` 才能正常。



## 视频硬编码实现

### MediaCodec 基础

![](image\media_codec_workflow.png)

调用流程：

- 选择编码器：根据 `mimeType` 和 `colorFormat`，以及是否为编码器，选择出一个 `MediaCodecInfo`；
- 创建编码器：`MediaCodec.createByCodecName(codecInfo.getName())`；
- 对于 API 21 以上的系统，我们可以选择异步消费输出：`mVideoCodec.setCallback`；
- 配置编码器：设置各种编码器参数（`MediaFormat`），再调用 `mVideoCodec.configure`（*文档也没有明确说 setCallback 应该在 configure 之前，但既然示例是这样写的，我们还是保持这样好了，[毕竟相机采集也是踩过坑了的](https://blog.piasy.com/2017/07/24/WebRTC-Android-Camera-Capture/#section-1)*）；
- 对于 API 19 以上的系统，我们可以选择 Surface 输入：`mVideoCodec.createInputSurface`；
- 启动编码器：`mVideoCodec.start`；
- 输入数据到编码器：输入数据到来时，Surface 输入模式下（*提前用 Surface 创建 EGLSurface*），调用 GLES API 绘制，最后 `eglSwapBuffers` 即可；普通模式下我们需要 `dequeueInputBuffer`、填入数据、`queueInputBuffer`；
- 消费编码器输出数据：异步模式下，我们在 `onOutputBufferAvailable` 中使用 buffer 内的数据，然后 `releaseOutputBuffer` 即可；同步模式下我们需要 `dequeueOutputBuffer`、使用 buffer 内的数据、`releaseOutputBuffer`；
- 停止并销毁编码器：先告知编码器我们要结束编码，Surface 输入时调用 `mVideoCodec.signalEndOfInputStream`，普通输入则可以为在 `queueInputBuffer` 时指定 `MediaCodec.BUFFER_FLAG_END_OF_STREAM` 这个 flag；告知编码器后我们就可以等到编码器输出的 buffer 带着 `MediaCodec.BUFFER_FLAG_END_OF_STREAM` 这个 flag 了，等到之后我们调用 `mVideoEncoder.release` 销毁编码器；

### 数据送进编码器

WebRTC 的硬编码封装在 `MediaCodecVideoEncoder` 类中, 接口的调用都发生在 native 层，就在 `webrtc/sdk/android/src/jni/androidmediaencoder_jni.cc` 这个文件

- 初始化编码器、EGL 环境：`initEncode`，其中会选择合适的编码器配置并启动，使用 Surface 模式输入时，我们会创建 `EglBase14` 和 `GlRectDrawer`，用于把 texture 数据绘制到编码器的输入 Surface 中；
- 输入 texture 数据到编码器：`encodeTexture`，其中的代码比较简单，GL 绘制 `drawer.drawOes`，交换 buffer `eglBase.swapBuffers`；但这里有一个 `checkKeyFrameRequired` 调用，我们会在流控部分展开讲；
- 输入内存数据到编码器：`dequeueInputBuffer` 和 `encodeBuffer`，其中的代码和官方样例没什么区别；

### 从编码器取数据

- `dequeueOutputBuffer` 取出一帧数据；
- `releaseOutputBuffer` 归还输出 buffer；

但这里有几点值得一提：

- 通常编码传输时每个关键帧头部都需要带上编码配置数据（PPS，SPS），但 MediaCodec 会在首次输出时专门输出编码配置数据，后面的关键帧里是不携带这些数据的，所以需要我们手动做一个拼接；
- 这里调用 `mediaCodec.dequeueOutputBuffer` 时第二个参数 timeout 传的是 0，表示不会等待，由于这里并没有一个单独的线程不停调用，反倒可以防止阻塞，但如果我们单独起了一个线程专门取输出数据，那这就会导致 CPU 资源的浪费了，可以加上一个合适的值，例如 3~10ms；

### 安卓硬编码流控

MediaCodec 流控相关的接口并不多，一是配置时设置目标码率和码率控制模式，二是动态调整目标码率（API 19+）。

**配置时指定目标码率和码率控制模式**：

```
mediaFormat.setInteger(MediaFormat.KEY_BIT_RATE, bitRate);
mediaFormat.setInteger(MediaFormat.KEY_BITRATE_MODE,
        MediaCodecInfo.EncoderCapabilities.BITRATE_MODE_VBR);
// 其他配置

mVideoCodec.configure(mediaFormat, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
```

码率控制模式在 `MediaCodecInfo.EncoderCapabilities` 类中定义了三种，在 [framework 层有另一套名字](http://androidxref.com/7.1.1_r6/xref/frameworks/native/include/media/openmax/OMX_Video.h#237)和它们的值一一对应：

- CQ 对应于 `OMX_Video_ControlRateDisable`，它表示完全不控制码率，尽最大可能保证图像质量；
- CBR 对应于 `OMX_Video_ControlRateConstant`，它表示编码器会尽量把输出码率控制为设定值，即我们前面提到的“不为所动”；
- VBR 对应于 `OMX_Video_ControlRateVariable`，它表示编码器会根据图像内容的复杂度（实际上是帧间变化量的大小）来动态调整输出码率，图像复杂则码率高，图像简单则码率低；

**动态调整目标码率**：

```
Bundle param = new Bundle();
param.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, bitrate);
mediaCodec.setParameters(param);
```

WebRTC 使用的是 CBR，稳定可控是 CBR 的优点

- VBR 在画面内容保持静止时，码率会降得很低，一旦画面内容开始动起来，码率上升会跟不上，就会导致画面质量很差；
- VBR 上调码率后，有可能导致中间网络路径的丢包/延迟增加，进而导致问题；
- CBR 会存在关键帧后的几帧内容模糊的问题，如果关键帧间隔较短，可以观察到明显的「呼吸效应」；
- WebRTC 使用的方案是 CBR + 长关键帧间隔，这样「呼吸效应」就不是那么明显，而 CBR 确实能增强画面质量；

### 关键帧

MediaCodec 有两种方式触发输出关键帧，一是由配置时设置的 `KEY_FRAME_RATE` 和 `KEY_I_FRAME_INTERVAL` 参数自动触发，二是运行过程中通过 `setParameters` 手动触发输出关键帧。

自动触发实际是按照帧数触发的，例如设置帧率为 25 fps，关键帧间隔为 2s，那就会每 50 帧输出一个关键帧，一旦实际帧率低于配置帧率，那就会导致关键帧间隔时间变长。由于 MediaCodec 启动后就不能修改配置帧率/关键帧间隔了，所以如果希望改变关键帧间隔帧数，就必须重启编码器。

手动触发输出关键帧：

```
Bundle param = new Bundle();
param.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);
mediaCodec.setParameters(param);
```

Capturer/Renderer/Encoder的简要封装demo：https://github.com/Piasy/VideoCRE



## 视频数据native 层

### 基本概念

#### PeerConnection stream  track source sink

一个 track 是一个音频或视频数据流（发送或接收）。一个 track 可以归属于一个或多个 stream，通过 stream id 区分

source 则是数据源，可以作为 track 的输入，它的消费者叫 sink，一个 source 可以有多个 sink

四个概念的关系：PC - 一到多个 stream - 零到多个 track - 一个 source。

#### sender receiver transceiver

sender 负责编码、发送；receiver 负责接收、解码；

transceiver 包含一个 sender 和一个 receiver，它们拥有相同的 mid

addTrack 时会为其分配 transceiver/sender（新建或复用已有 transceiver/sender），也可以通过 addTransceiver 来建立 track - sender/receiver - transceiver 的关联。

### 发送端视频数据

#### capturer => source => encoder

#### encoder => sender

### 数据 pipeline 建立过程

RTCCameraVideoCapturer => RTCVideoSource

RTCVideoSource => ObjCVideoTrackSource

AdaptedVideoTrackSource => VideoBroadcaster

> 从 iOS 系统 -> RTCCameraVideoCapturer -> RTCVideoSource -> ObjCVideoTrackSource -> AdaptedVideoTrackSource -> VideoBroadcaster 

VideoBroadcaster => VideoStreamEncoder

> `PeerConnection::AddTrack`：创建 sender, receiver, transceiver, 关联 track 和 sender；
>
> `PeerConnection::SetLocalDescription`：把 track 交给 channel，但 channel 把 track 当 source 用；
>
> `PeerConnection::SetRemoteDescription`：关联 stream 和 source；

VideoStreamEncoder => RTCVideoEncoderH264

### 接收端视频数据

- VideoReceiveStream - VideoSendStream
- VideoReceiver - VideoSender
- VCMGenericDecoder - VCMGenericEncoder
- ObjCVideoDecoder - ObjCVideoEncoder
- RTCVideoDecoderH264 - RTCVideoEecoderH264

#### 跨平台视频处理模块的架构

包括采集、编码、jitter buffer、解码、渲染等功能，还有它们的线程、队列管理，其中采集、（硬件）编解码、渲染都是平台相关的，其他模块以及对平台相关模块的调用和管理，都是平台无关的。

## 混音

### AudioMixer 的使用

AudioMixer 的使用比较简单，只需要三步：

- 创建：`const auto mixer = AudioMixerImpl::Create();`
- 添加声源：`mixer->AddSource(src);`
- 混音并取得结果：`mixer->Mix(output_channel_num, mixed_frame);`

### AudioMixer 的实现原理

AudioMixer 实际做的事情也只需要三步：

- 计算输出采样率：`CalculateOutputFrequency`;
- 从 source 收集音频数据：`GetAudioFromSources`;
- 执行混音操作：`FrameCombiner::Combine`;

**CalculateOutputFrequency**

- 收集所有 source 的 `PreferredSampleRate`，存入 `preferred_rates`；
- 找出 8K, 16K, 32K, 48K 中大于 `preferred_rates` 所有元素的最小者，作为输出采样率；

**GetAudioFromSources**

- 遍历所有的 source，把取到的 `AudioFrame` 添加到 `audio_source_mixing_data_list` 列表中；
- 对 `audio_source_mixing_data_list` 列表排序，没有 mute 的在前面，active 的在前面，energy 大的在前面；
- 从排序后的列表中，最多选出三路没有 mute 的参与混音；
- 参与混音的 frame 可能还要做 Ramp（淡入）处理 `RampAndUpdateGain`；
- 混音路数太多，音质就会太差，WebRTC 客户端则是只混三路

**GetAudioFrameWithInfo**

- 由 `Source` 的子类实现，需要实现重采样功能，根据传入的 `sample_rate_hz` 和自身实际数据的采样率，进行重采样；
- WebRTC 里收流端的实际实现是 `webrtc::voe::Channel`（它没有继承 Source，而是被层层委托过来的）；
- 重采样的逻辑实际实现在 `AcmReceiver::GetAudio` 函数里，通过调用 `ACMResampler::Resample10Msec` 实现，而 ACMResampler 则是调用 `common_audio/resampler` 目录中的代码实现，这个 resampler 比较基础，不支持变声道数，功能不如 FFmpeg 的强大；
- Channel 还实现了音量控制的逻辑，在把数据交给 mixer 之前，它调用 `AudioFrameOperations::ScaleWithSat` 实现音量调整

**FrameCombiner::Combine**

- FrameCombiner 要求所有待合并的 frame 采样率都相同，且样点数都为 10ms 的样点，即采样率除以 100；

- FrameCombiner 允许待合并的 frame 声道数不一致，所以需要先做 `RemixFrame` 操作，即单双声道切换，调用 `AudioFrameOperations::MonoToStereo` 或 `AudioFrameOperations::StereoToMono` 实现；

- 零路、单路其实没什么需要操作的，直接把数据拷贝一份即可；

- 多路就是把样点值直接相加

- 多路时接下来的操作就是限幅了，目前支持两种模式 `apm_agc_limiter_`（`AudioProcessing`）和 `apm_agc2_limiter_`（`FixedGainController`）可供选择

- 限幅之后就是把 float 数据转换为 `int16_t` 并写入输出 frame 里了；

  

## P2P连接过程

### P2P 连接过程

![](image\p2p_connect_procedure2.jpg)

**宏观流程**

- 设置 local sdp；
- 创建一个 transport 对象（启用了 bundle）；
- 收集 local candidates；
- 设置 remote sdp，添加 remote candidates；
- ICE 连通性检查，建立 P2P 连接；
- P2P 数据传输；

### SDP

SDP 全称 Session Description Protocol，顾名思义，它是一种描述会话（Session）的协议。一次电话会议，一次网络电话，一次视频流传输等等，都是一次会话。那会话需要哪些描述呢？最基础的有多媒体数据格式和网络传输地址，当然还包括很多其他的配置信息。

一次会话用什么配置，就涉及到一个协商的过程了，会话发起者先提出一些建议（offer），其他人参与者再根据 offer 给出自己的选择（answer），最终意见达成一致后，才能开始会话。

offer 和 answer 其实都是 SDP，而 local/remote 则是相对的，offer 是会话发起者的 local SDP，是会话加入者的 remote SDP，answer 则是会话发起者的 remote SDP，是会话加入者的 local SDP。

SDP 实际上就是一个字符串，它的具体格式定义，可以参考 [RFC 文档](https://tools.ietf.org/html/rfc4566)。它的拼接过程，native 和 Java 代码都有分布，native 代码调用栈还比较深，这里就不展开了，`createOffer` 主要逻辑就是根据创建 `PeerConnection` 对象时指定的 `MediaConstraints`，以及在 `createOffer` 调用前添加的`VideoTrack`/`AudioTrack`/`DataChannel` 情况，拼出初始 SDP，最后在 `PeerConnectionClient.SDPObserver#onCreateSuccess` 中会添加 codec 相关的值。`createAnswer` 则还会参考 offer SDP 的值。

### P2P 关键类

- PeerConnection: WebRTC 核心类；
- JsepTransportController: 管理 P2P 连接；
- 各种 transport 类：P2P 连接的封装，封装了加解密、mux/demux 等逻辑，提供收发数据的接口；
- BasicPortAllocator, PortAllocator: 保存各种配置，管理 PortAllocatorSession；
- BasicPortAllocatorSession, PortAllocatorSession: 遍历所有网络设备（Network 对象），分配 port；
- AllocationSequence: 负责对单个网络设备（Network 对象）分配 port，分阶段进行；
- 各种 port 类：代表的是一种通讯机制的本地实例，它可以和远端的类似实例一起实现数据通讯；
- Connection: 代表的是一个 local port 和一个 remote port 的通讯链接；

### 各种 transport 类的关系

- PacketTransportInternal, PacketTransportInterface: packet base transport 的基类；
- P2PTransportChannel, IceTransportInternal: 继承自 PacketTransportInternal，负责 ICE 相关的功能，包括：收集本地 candidate、和远端 candidate 做连通性检查、数据传输；
- DtlsTransport, DtlsTransportInternal: 继承自 PacketTransportInternal，负责 DTLS 相关的功能，包括：DTLS 握手、DTLS 加解密；内含一个 IceTransportInternal (P2PTransportChannel)，收发数据通过它实现；

- RtpTransport, RtpTransportInternal, SrtpTransportInterface, RtpTransportInterface: 提供了收发 RTP, RTCP 包的接口，其内部包了两个实际收发 RTP 和 RTCP 数据的 PacketTransportInternal (DtlsTransport)；
- DtlsSrtpTransport, SrtpTransport: DTLS-SRTP, SRTP 的实现类，继承自 RtpTransport； *有了 DTLS，为何还要 SRTP？*
- JsepTransport: JsepTransportController 管理 transport 的辅助类，sdp 里每个 m line 都对应于一个数据流（音频、视频、应用数据），每个数据流都需要一个 transport，但可以通过 bundle 技术复用同一个 transport，m line 里的 attribute 描述了 transport 的属性；

### 关键类的数量关系

一个 PeerConnection - 一个 JsepTransportController - 一个 JsepTransport（启用了 bundle） - 一个 DtlsSrtpTransport - 一个 DtlsTransport - 一个 P2PTransportChannel。

一个 JsepTransportController - 一个 BasicPortAllocator - 多个 BasicPortAllocatorSession，但一次分配过程只会有一个 session。

一个 BasicPortAllocatorSession - 多个 AllocationSequence。

一个 AllocationSequence - 多个 port。

一个 P2PTransportChannel - 多个 Connection，但最终会选出一个 Connection 使用。

### candidate

每个传输地址记录值都叫做一个 candidate，candidate 可能有三种：

- 客户端从本机网络接口上获取的地址（host）；
- STUN server 看到的该客户端的地址（server reflexive，缩写为 srflx）；
- TURN server 为该客户端分配的中继地址（relayed）；

两个客户端上述 candidate 的任意组合也许都能连通，但实际上很多组合都不可用。

网络接口地址对应的端口号是客户端自己分配的，如果有多个网络接口地址，那就都要带着。TURN server 可以同时取得 reflexive 和 relayed candidate，而 STUN server 则只能取得 reflexive candidate（*这下就清楚 [coturn](https://github.com/coturn/coturn) 到底是 STUN server 还是 TURN server 了*）。

### 收集 candidate

candidate 的收集包括两部分：一是 host，二是 srflx 和 relayed。第一部分肯定得在本地网络接口上做文章，第二部分则需要连接 STUN/TURN Server。

candidate 的收集由设备网络连接变化触发：

![img](https://imgs.piasy.com/2017-08-30-init_pc_start_network_monitor.jpg)

![img](https://imgs.piasy.com/2017-08-30-network_change_trigger_ice_candidate_gathering.jpg)

实际收集 candidate 的过程分为几个阶段：Udp，Relay，Tcp，SslTcp。

收集过程有三个不直接的交互：

- sig slot：简言之就是一个信号处理的框架，A 发一个信号，B 能接收处理，二者完全解耦，具体的可以看看[官方文档](https://github.com/KubaO/sigslot/blob/master/sigslot.pdf)；
- message：类似于 Java 里面的 Handler 机制，也是提交消息，接收者进行相关处理，为啥有了 sig slot 还要 message 机制呢？sig slot 无法发送延迟消息是原因之一；
- 网络：STUN/TURN Server 的访问都是网络请求，为了实现跨平台，网络相关的代码做了不少封装，并且使用的都是操作系统的 C/C++ 接口，这块我也还没有深入看；

另外这里推荐一个 STUN/TURN Server 测试工具：[Trickle ICE](https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice)，用来测试服务器是否正确部署，以便排查问题。

### 收集本地 candidate

```
PeerConnection::SetLocalDescription
                ↓
JsepTransportController::MaybeStartGathering
                ↓
P2PTransportChannel::MaybeStartGathering
                ↓
BasicPortAllocatorSession::StartGettingPorts
                ↓
BasicPortAllocatorSession::DoAllocate
```

### DoAllocate

遍历所有网络设备（Network 对象），创建 AllocationSequence 对象，调用其 Init Start 函数，分配 port。

```
BasicPortAllocatorSession::DoAllocate
              ↓
AllocationSequence::Start
              ↓ message
AllocationSequence::OnMessage
```

### UDP phase

UDP phase 会收集两种类型的 candidate：host 和 srflx。

#### host candidate

一旦创建了 AsyncPacketSocket 对象，有了本地 IP 和端口，host 类型的 candidate 也就已经就绪了，而 AsyncPacketSocket 对象在 `AllocationSequence::Init` 里就已经创建好了，所以可以直接发出 host candidate。

```
AllocationSequence::OnMessage
              ↓
AllocationSequence::CreateUDPPorts
              ↓
BasicPortAllocatorSession::AddAllocatedPort
              ↓
UDPPort::PrepareAddress
              ↓
UDPPort::OnLocalAddressReady
              ↓
      Port::AddAddress
              ↓ sig slot (SignalCandidateReady)
BasicPortAllocatorSession::OnCandidateReady
```

#### srflx candidate

向 STUN server 发送一个 UDP 包（叫 STUN Binding request），server 会把这个包里的源 IP 地址、UDP 端口返回给客户端（叫 STUN Binding response），这个 IP 和端口将来可能可以用来和其他客户端建立 P2P 连接

收集 srflx candicate 时可以复用收集 host candidate 时创建的 socket 对象，这一逻辑通过 `PORTALLOCATOR_ENABLE_SHARED_SOCKET` flag 控制，默认是开启的。

复用 socket 的情况下，`AllocationSequence::CreateStunPorts` 函数会直接返回，因为早在 `AllocationSequence::CreateUDPPorts` 函数的执行过程中，就已经执行了 STUN Binding request 的发送逻辑。

发送 STUN Binding request：

```
UDPPort::OnLocalAddressReady
            ↓
UDPPort::MaybePrepareStunCandidate
            ↓
UDPPort::SendStunBindingRequest
            ↓
StunRequestManager::SendDelayed
            ↓ message
StunRequest::OnMessage
            ↓ sig slot (SignalSendPacket)
UDPPort::OnSendPacket
            ↓
AsyncUDPSocket::SendTo
            ↓
PhysicalSocket::SendTo
            ↓
系统 socket sendto
```

收到 STUN Binding response：

```
PhysicalSocketServer::WaitSelect
                ↓
SocketDispatcher::OnEvent
                ↓ sig slot (SignalReadEvent)
AsyncUDPSocket::OnReadEvent
                ↓ sig slot (SignalReadPacket)
AllocationSequence::OnReadPacket
                ↓
UDPPort::HandleIncomingPacket
                ↓
StunRequestManager::CheckResponse
                ↓
StunBindingRequest::OnResponse
                ↓
UDPPort::OnStunBindingRequestSucceeded
                ↓
        Port::AddAddress
                ↓ sig slot (SignalCandidateReady)
BasicPortAllocatorSession::OnCandidateReady
```

### RELAY phase

WebRTC 目前支持两种中继协议：GTURN 和 TURN。现在基本都是使用标准的 TURN 协议。TURN 协议是 STUN 协议的一个扩展，它利用一个中继服务器，使得无法建立 P2P 连接的客户端（NAT 严格限制导致）也能实现通讯。

TURN 协议的工作流程如下：

客户端发送 Allocate request 到 server，server 返回 401 未授权错误（带有 realm 和 nonce），客户端再发送带上认证信息的 Allocate request，server 返回成功分配的 relay address。分配成功后，客户端需要通过发送机制（Send Mechanism）或信道机制（Channels）在 server 上配置和其他 peer 的转发信息。此外 allocation 和 channel 都需要保活。

WebRTC 使用的是信道机制，因为这一机制的数据开销更低

收集 TURN relay candidate 时也可以复用收集 host candidate 时创建的 socket 对象，这一逻辑通过 `PORTALLOCATOR_ENABLE_SHARED_SOCKET` flag 控制，默认开启。

TURN 协议独特的部分：

```
AllocationSequence::OnMessage
              ↓
AllocationSequence::CreateRelayPorts
              ↓
BasicPortAllocatorSession::AddAllocatedPort
              ↓
TurnPort::PrepareAddress
              ↓
TurnPort::SendRequest
              ↓ message
StunRequest::OnMessage
              ↓ 发送请求、接收响应
StunRequestManager::CheckResponse
              ↓
TurnAllocateRequest::OnErrorResponse
              ↓
TurnAllocateRequest::OnAuthChallenge
              ↓
TurnPort::SendRequest
              ↓ 发送请求、接收响应
StunRequestManager::CheckResponse
              ↓
TurnAllocateRequest::OnResponse
              ↓
TurnPort::OnAllocateSuccess
              ↓
        Port::AddAddress
              ↓ sig slot (SignalCandidateReady)
BasicPortAllocatorSession::OnCandidateReady
```

### StunRequest 和 StunRequestManager

StunRequest 类是对 STUN request 的定义和封装，基类里实现了 request 超时管理、重发的逻辑，各种特定类型的逻辑由子类实现，例如 StunBindingRequest 和 TurnAllocateRequest。

StunRequestManager 则实现了 response 和 request 匹配的逻辑：manager 按 transaction id => request 的 hash 保存了所有的 request，收到 response 后，根据 transaction id 即可找到对应的 request，进而可以执行 request 对象的回调。

### OnCandidateReady

OnCandidateReady 里会调用两个重要的函数（通过 sig slot）：`P2PTransportChannel::OnPortReady` 和 `P2PTransportChannel::OnCandidatesReady`。

在 OnPortReady 里，P2PTransportChannel 会把 port 存入 `ports_` 数组，供后续收到 remote candidate 后建立 Connection 用。此外，这里也会立即尝试用这个 port 和每个远端 candidate 建立 Connection。

在 OnCandidatesReady 里，P2PTransportChannel 会把 candidate 一路回调给 APP 层：

```
P2PTransportChannel::OnCandidatesReady
                  ↓
JsepTransportController::OnTransportCandidateGathered_n
                  ↓
PeerConnection::OnTransportControllerCandidatesGathered
                  ↓
PeerConnection::OnIceCandidate
                  ↓
PeerConnectionObserver::OnIceCandidate
```

### 设置远端 candidate

收到远端的 candidate 后，我们调用 `PeerConnection::AddIceCandidate` 接口进行设置，其内部调用栈为：

```
PeerConnection::AddIceCandidate
              ↓
JsepTransportController::AddRemoteCandidates
              ↓
JsepTransport::AddRemoteCandidates
              ↓
P2PTransportChannel::AddRemoteCandidate
              ↓
P2PTransportChannel::CreateConnections
```

#### 连通性检查

candidate 收集完毕后，双方的 candidate 两两配对，然后分三步对 candidate 组合进行连通性检查：

- 把 candidates 组合按优先级排序；
- 按顺序发送检查请求（STUN Binding request），源地址是 candidate 组合的本地 candidate，目的地址是对方 candidate；
- 收到对方的检查请求后发出响应（STUN Binding response）；

每次检查实际上是一个四步握手的过程：

![](image\ice_handshake.png)

STUN 请求和 RTP/RTCP 传输数据使用的是完全一样的地址和端口，解多路复用并不是 ICE 的任务，而是 RTP/RTCP 的任务。

客户端收到的 STUN Binding respose 中也会携带对方的公网地址，如果这个地址和发送请求的 request 地址不一致，那 response 里的地址也会作为一个新的 candidate（peer reflexive），参与到连通性检查中。

如果客户端收到了对方的检查请求，除了发送响应外，也会立即对这个 candidate 组合进行检查，以加快完成一次成功的连通性检查。

STUN Binding request:

```
P2PTransportChannel::AddRemoteCandidate
                  ↓
P2PTransportChannel::SortConnectionsAndUpdateState
                  ↓
P2PTransportChannel::MaybeStartPinging
                  ↓
P2PTransportChannel::PingConnection
                  ↓
StunRequestManager::SendDelayed
                  ↓ 发送请求、接收响应
Connection::OnConnectionRequestResponse
                  ↓
Connection::set_write_state
                  ↓ sig slot (SignalStateChange)
P2PTransportChannel::OnConnectionStateChange
                  ↓
P2PTransportChannel::RequestSortAndStateUpdate
                  ↓ message
P2PTransportChannel::SortConnectionsAndUpdateState
```

创建 Connection 会触发 ping，ping 成功后会触发 Connection 的状态切换（见下文）。排序后，我们最终会选出一个合适的 Connection，通知上层可以进行数据通讯了。

```
P2PTransportChannel::SwitchSelectedConnection
                    ↓ sig slot (SignalReadyToSend)
DtlsTransport::OnReadyToSend
                    ↓ sig slot (SignalReadyToSend)
RtpTransport::OnReadyToSend
```

#### candidate 排序

每个客户端会为自己的 candidate 设置权值，双方 candidate 权值之和将作为组合的权值，用于排序。求和的方式确保了双方排序结果的一致性，这个一致性至关重要，因为通常 NAT 都不会允许外部主机的数据包从某个端口进入内网，除非这个端口有数据包发往过这个主机，因此只有双方都发送了检查请求，数据包才可能通过 NAT。

权值的确定，RFC 里面只说明了基本原则：直接的连接比间接的连接要好。但具体如何设置，并没有具体说明。

### Connection 状态

`write_state_`：一共有四种状态，取值依次变大，其切换逻辑在 `Connection::ReceivedPingResponse` 和 `Connection::UpdateState` 中：

- `STATE_WRITABLE`：只要收到 ping response 就会切换到这个状态；
- `STATE_WRITE_UNRELIABLE`：当前处于 WRITABLE 状态，且最近发生过一定量的 ping 失败/超时，则切换到此状态；
- `STATE_WRITE_INIT`：初始化状态；
- `STATE_WRITE_TIMEOUT`：如果当前处于 UNRELIABLE 或 INIT 状态，且最近超过一定时间没有收到 response，则切换到此状态；

### ICE 状态

ICE 状态 IceConnectionState 定义在 `api/peerconnectioninterface.h` 中，状态定义如下：

- `kIceConnectionNew`: PeerConnection 构造时的初始状态；

- kIceConnectionChecking: ICE 连通性检查状态，包括收集 candidate、ICE 连通性检查；
  - 当 initiator 应用 remote sdp（即 answer）时，如果 sdp 里有 media section，且当前状态为 New，那就切换到 Checking 状态；
  - 当添加 remote candidate 时（从 sdp 中，或者信令通道），如果当前状态是 New 或者 Disconnected，那就切换到 Checking 状态；

- `kIceConnectionConnected`, `kIceConnectionCompleted`, `kIceConnectionFailed`, `kIceConnectionDisconnected` 则根据 JsepTransportController 的 IceConnectionState 来进行设置，基本上一一对应，除了 Disconnected：如果当前处于 Connected 或 Completed，但 JsepTransportController 的 IceConnectionState 切换到了 Connecting，那就认为是发生了断连，故切换到 Disconnected 状态；

- `kIceConnectionClosed`: PC 被关闭时，切换到 Closed 状态；

JsepTransportController 的 IceConnectionState 的计算状态的逻辑在 `JsepTransportController::UpdateAggregateStates_n` 函数中：

- 默认处于 `kIceConnectionConnecting` 状态；
- 遍历所有的 DtlsTransportInternal（即 DtlsTransport，其内部有个 P2PTransportChannel），分别统计以下几种状态：any_failed，all_connected，all_completed，any_gathering，all_done_gathering

DtlsTransport 的 writable 状态

- 如果没有禁用 DTLS，那 writable 就意味着完成了 DTLS 握手，而开始 DTLS 握手的前提就是其底层的 P2PTransportChannel 处于 writable 状态；
- 如果禁用了 DTLS，那 writable 就等同于其底层的 P2PTransportChannel 处于 writable 状态；
- 而 P2PTransportChannel 处于 writable 的条件则是选出了 Connection，且处于 `writable || PresumedWritable` 状态；

P2PTransportChannel 的 IceTransportState

- P2PTransportChannel 创建时处于 `STATE_INIT` 状态，之后的状态更新逻辑都在 `P2PTransportChannel::ComputeState` 函数中，这个函数在 `P2PTransportChannel::SortConnectionsAndUpdateState` 函数的最后调用，这是为了既保证 Connection 更新后状态更新的及时性，又避免同时多个 Connection 更新导致多次状态更新；
- 如果执行过 AddConnection（收到未知地址的 STUN Binding request，或者 CreateConnection），但是当前没有任何 Connection 处于 active 状态，那就处于 `STATE_FAILED` 状态；
- 如果有某个 Network 有多个 active connection，那就处于 `STATE_CONNECTING` 状态；
- 如果所有 active connection 都有不同的 Network，即每个 Network 至多有一个 active connection，那就处于 `STATE_COMPLETED` 状态；

candidate 收集状态：

- P2PTransportChannel 创建时，`gathering_state_` 为 kIceGatheringNew；
- 在 `P2PTransportChannel::MaybeStartGathering` 里开始收集 candidate 时，如果当前未处于 Gathering 状态，则切换到 kIceGatheringGathering 状态；
- 在收到 OnCandidatesAllocationDone 回调时，切换到 kIceGatheringComplete 状态；

### Connection 排序

采用的是 stable sort

 `P2PTransportChannel::CompareConnections`函数

writable、receiving 的排在 non-writable、non-receiving 的前面；nomination 大的排在前面；

收到数据的时间；network cost 和 priority；

### Connection 选择和淘汰

P2PTransportChannel::MaybeSwitchSelectedConnection

- 首先只有处于 writable，或 PresumedWritable，或处于 UNRELIABLE 状态的 Connection 才可能被选中；
- 如果当前没有选中的 Connection，且新的 Connection 可以被选中，那不用犹豫，直接选中它；
- 如果当前已经有了选中的 Connection，那就得二者一较高下了：先比较 network cost，如果新的 Connection cost 更高，且未处于 receiving 状态，那就妥妥的维持现状；
- 否则，就让已选中的 Connection 和新 Connection 重新走一下排序的对比逻辑

那如何确定该 ping 哪个 connection 呢？其逻辑实现在 `P2PTransportChannel::FindNextPingableConnection` 中：

- 每次检查只会选出一个 Connection 执行 ping 操作（注意这不等价于同时只 ping 一个 Connection），此外，已处于 writable 状态的 Connection 也需要定期进行 ping，以确认健康度；
- 如果已有选中的 Connection，且其处于 writable 状态，且需要 ping，那就优先 ping 它；
- 如果没有选中的 Connection，或者其处于 weak 状态，那就每个 Network 选出一个排在最前面的 writable 且 connected 的 Connection（C++ `std::map::insert` 同一 key 只会加一次），对其中需要 ping 的，选出上次 ping 的时间最早的 Connection，进行本次 ping 操作；
- 如果上一步还没选出一个 Connection，那就从那些非 writable、收到 ping 但未回 ping 的 Connection 里，选出收到 ping 的时间最早的 Connection，进行本次 ping 操作；
- 如果还没选到，那就从所有未 ping 过的 Connection 里（如果都 ping 过，那就认为都没 ping 过），选出 most pingable 的 Connection，依次按下述三种指标挑选：最可能连通（两端都是 relay 的 Connection 更可能连通，在此前提下，UDP 更可能连通），最近收到过 ping，排序时排在前面；



### DataChannel 使用

基于 DataChannel 实现一个 P2P 文字聊天功能。

DataChannel 是 WebRTC 提供的任意数据 P2P 传输的 API，它使用 [SCTP 协议](https://tools.ietf.org/html/rfc4960)，可以灵活配置是否可靠传输。我们可以用它实现文字聊天、文件分享、实时对战游戏等场景下的数据传输，P2P + DTLS 保证了传输数据的安全性。

为了使用 DataChannel，我们先得创建 PeerConnection 对象，而且完成 P2P 连接的建立，具体过程经过上面的分析，我们应该已经了然于胸了，下面只摘录关键代码，完整代码可以查看[这个 GitHub 提交](https://github.com/Piasy/AppRTC-Android/commit/a0c0e11d404645a9886fca7846bed2c82933a13c)。

```
// 初始化并创建 factory
PeerConnectionFactory.initializeAndroidGlobals(mAppContext, true);
mPeerConnectionFactory = new PeerConnectionFactory(null);

// 创建 PC 对象
mPeerConnection = mPeerConnectionFactory.createPeerConnection(rtcConfig,
    new MediaConstraints(), this);

// 创建 DataChannel
DataChannel.Init init = new DataChannel.Init();
init.ordered = true;
init.negotiated = true; // false is ok
init.maxRetransmits = -1;
init.maxRetransmitTimeMs = -1;
init.id = 0; // must be set, and >= 0
mDataChannel = mPeerConnection.createDataChannel("P2P MSG DC", init);
mDataChannel.registerObserver(this);

// A，创建 offer
mPeerConnection.createOffer(MsgPcClient.this, mSdpConstraints);
// 在 onCreateSuccess 回调中 setLocalDescription
// 在 onSetSuccess 回调中把 offer 发出去
mPeerConnection.setLocalDescription(MsgPcClient.this, sdp);

// B，收到 offer 后 setRemoteDescription
mPeerConnection.setRemoteDescription(MsgPcClient.this, sdp);
// 创建 answer
mPeerConnection.createAnswer(MsgPcClient.this, mSdpConstraints);
// 在 onCreateSuccess 回调中 setLocalDescription
// 在 onSetSuccess 回调中把 answer 发出去
mPeerConnection.setLocalDescription(MsgPcClient.this, sdp);

// A，收到 answer 后 setRemoteDescription
mPeerConnection.setRemoteDescription(MsgPcClient.this, sdp);

// 在 onIceCandidate 回调中把 candidate 发出去
// 收到对方的 candidate 后 addIceCandidate
mPeerConnection.addIceCandidate(candidate);

// 在 onDataChannel 回调中注册消息回调
dataChannel.registerObserver(this);

// 发送消息
byte[] msg = message.getBytes();
DataChannel.Buffer buffer = new DataChannel.Buffer(ByteBuffer.wrap(msg), false);
mDataChannel.send(buffer);

// onMessage 回调中处理消息
ByteBuffer data = buffer.data;
final byte[] bytes = new byte[data.capacity()];
data.get(bytes);
String msg = new String(bytes);
Logging.d(TAG, "onMessage " + msg);
```

创建 DataChannel 时可以通过 `DataChannel.Init` 的 `ordered`、`maxRetransmitTimeMs`、`maxRetransmits` 参数配置配置可靠性：

- `ordered`：是否保证顺序传输；
- `maxRetransmitTimeMs`：重传允许的最长时间；
- `maxRetransmits`：重传允许的最大次数；

### 附录：NAT 类型与 P2P 连接的可行性

终端所处网络类型

- Opened：开放式，拥有公网 ip，且没有防火墙，可自由与外部通信；
- Full Cone NAT：完全圆锥型 NAT，NAT 规则如下：从主机 UDP 端口 A 发出的数据包都会对应到 NAT 设备出口 ip 和端口 B，并且从任意外部地址（ip port 二元组）发送到该 NAT 设备 UDP 端口 B 的包都会被转到主机端口 A；
- Restricted Cone NAT：地址限制圆锥型 NAT，相比于 Full Cone NAT，只有从之前该主机发出包的目的 ip 发送到该 NAT 设备 UDP 端口 B 的包才会被转到主机端口 A；
- Port Restricted cone NAT：端口限制圆锥型 NAT，相比于 Full Cone NAT，只有从之前该主机发出包的目的 ip 和 port 发送到该 NAT 设备 UDP 端口 B 的包才会被转到主机端口 A；
- Symmetric NAT：对称型 NAT，在 Port Restricted cone NAT 的基础上，即使数据包都从主机 UDP 端 A 发出，但只要目的地址（ip port 二元组）不同，NAT 设备就会为之分配不同的出端口 B；
- Symmetric UDP Firewall：对称型 UDP 防火墙，防火墙规则如下：从主机 UDP 端口 A 发出的数据包保持源地址，但只有从之前该主机发出包的目的 ip 和 port 发送到该主机端口 A 的包才能通过防火墙；
- Blocked：防火墙限制 UDP 通信；

利用有两个固定公网 ip 的 STUN server，客户端可以通过数次测试，探测自己所处的网络类型：

```
                        +--------+
                        |  Test  |
                        |   I    |
                        +--------+
                             |
                             |
                             V
                            /\              /\
                         N /  \ Y          /  \ Y             +--------+
          UDP     <-------/Resp\--------->/ IP \------------->|  Test  |
          Blocked         \ ?  /          \Same/              |   II   |
                           \  /            \? /               +--------+
                            \/              \/                    |
                                             | N                  |
                                             |                    V
                                             V                    /\
                                         +--------+  Sym.      N /  \
                                         |  Test  |  UDP    <---/Resp\
                                         |   II   |  Firewall   \ ?  /
                                         +--------+              \  /
                                             |                    \/
                                             V                     |Y
                  /\                         /\                    |
   Symmetric  N  /  \       +--------+   N  /  \                   V
      NAT  <--- / IP \<-----|  Test  |<--- /Resp\               Open
                \Same/      |   I    |     \ ?  /               Internet
                 \? /       +--------+      \  /
                  \/                         \/
                  |                           |Y
                  |                           |
                  |                           V
                  |                           Full
                  |                           Cone
                  V              /\
              +--------+        /  \ Y
              |  Test  |------>/Resp\---->Restricted
              |   III  |       \ ?  /
              +--------+        \  /
                                 \/
                                  |N
                                  |       Port
                                  +------>Restricted

                 Figure 2: Flow for type discovery process
```

1. STUN 客户端从向 STUN 服务器发送请求，要求得到自身经 NAT 映射后的地址：
   1. 收不到服务器回复，则认为 UDP 被防火墙阻断，不能通信，网络类型：Blocked；
   2. 收到服务器回复，对比本地地址，如果相同，则认为无 NAT 设备，进入第 2 步，否则认为有 NAT 设备，进入第 3 步；
2. （已确认无 NAT 设备）STUN 客户端向 STUN 服务器发送请求，要求服务器从其他 ip 和 port 向客户端回复包：
   1. 收不到服务器从其他 ip 地址的回复，认为包被前置防火墙阻断，网络类型：Symmetric UDP Firewall；
   2. 收到则认为客户端处在一个开放的网络上，网络类型：Opened；
3. （已确认有 NAT 设备）STUN 客户端向 STUN 服务器发送请求，要求服务器从其他 ip 和 port 向客户端回复包：
   1. 收不到服务器从其他 ip 地址的回复，认为包被前置 NAT 设备阻断，进入第 4 步；
   2. 收到服务器回复，则网络类型：Full Cone NAT；
4. STUN 客户端向 STUN 服务器的另外一个 ip 地址发送请求（本地端口不变），要求得到自身经 NAT 映射后的地址，并和第 1 步得到的地址对比：
   1. 地址不相同，则网络类型：Symmetric NAT；
   2. 地址相同，则认为是 Restricted NAT，进入第 5 步，进一步确认类型；
5. （已确认 Restricted NAT 设备）STUN 客户端向 STUN 服务器发送请求，要求服务器从相同 ip 的其他 port 向客户端回复包：
   1. 收不到服务器从其他 port 地址的回复，认为包被前置 NAT 设备阻断，网络类型：Port Restricted Cone NAT；
   2. 收到则认为网络类型：Restricted Cone NAT；



## 音频设备模块 ADM

功能：选择采集/播放音频设备、采集/播放启停控制、采集/播放音量控制、采集/播放静音、双声道采集/播放、获取播放延迟。

Java 类提供了采集/播放静音的接口

### Android ADM

Android 的 ADM 实现类在 `sdk/android/src/jni/audio_device/audio_device_module.cc` 中，其主要接口都是委托给 `AudioInput` 和 `AudioOutput`，而它们分别由 `AudioRecordJni` 和 `AudioTrackJni` 实现，而 `AudioRecordJni` 和 `AudioTrackJni` 的接口则是调用 Java 层的 `WebRtcAudioRecord` 和 `WebRtcAudioTrack` 类。

#### 音频采集

WebRtcAudioRecord（audio 包下）

- 从 AudioRecord 读数据是在一个单独的线程里，这个线程会调用 `Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO)` 以提高线程优先级；
- 音频数据采集到之后，送往 native 层之后，会把数据拷贝一份，交给一个可选的数据回调；这个逻辑有两个小问题，一是修改数据的需求无法满足，二是每次都新建数组，会引发内存抖动；
- 如果 `audioRecord.read` 返回的读得数据大小不等于欲读数据大小，那就不会使用读到的数据，若返回值为 `ERROR_INVALID_OPERATION`，那就会停止采集、报告错误；

数据传递：

- 在采集线程读取到数据后，调用 native 接口执行到 `AudioRecordJni::DataIsRecorded`；
- 调用 `AudioDeviceBuffer::SetRecordedBuffer`，其中会把采集到的数据拷贝到 `rec_buffer_` 中；
- 调用 `AudioDeviceBuffer::DeliverRecordedData`，接下来就是对数据的编码、发送了：       ![](image\音频采集.png)

注：WebRTC Android JNI 接口的 C 层函数定义，都不是手写的，而是用 Python 脚本生成的，生成的代码在 `out/debug/gen/sdk/android/generated_xxx_jni/jni` 目录中，其内则是调用 `sdk/android/src/jni` 目录下的 `XXXJni` 类。

#### 音频播放

WebRtcAudioTrack

- WebRTC 全局只会使用一个 AudioTrack 对象，无论是一个 PC 多路流，还是多个 PC，多路流的音频数据会在 AudioMixer 里混好音，然后用这个 AudioTrack 对象进行播放；
- 和采集类似，播放端也会在 `initPlayout` 里缓存 direct byte buffer 的 native 地址；
- 构造 AudioTrack 对象时，也会捕获 `IllegalArgumentException`，构造完毕后，也会验证其处于 `STATE_INITIALIZED` 状态；
- 在 `startPlayout` 里调用 `audioTrack.play` 时也会捕获 `IllegalStateException`，之后也会验证其处于 `PLAYSTATE_PLAYING` 状态；
- 向 AudioTrack 写数据是在一个单独的线程里，它也会调用 `Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO)` 提高线程优先级；
- 如果 `audioTrack.write` 返回的写入数据大小不等于欲写数据大小，那也不会重试写未被写入的数据，但若返回了错误，那就会停止播放、报告错误；

数据传递：

- Java 层的播放线程不停地调用 native 接口获取待播放的数据，执行到 `AudioTrackJni::GetPlayoutData`；

- 调用 `AudioDeviceBuffer::RequestPlayoutData`，其中会向 audio transport 索要已解码的音频数据，其调用栈为：

  ![](image\音频播放.png)

Android 的声音路由可以通过调用 `AudioManager` 的接口实现，WebRTC 在 demo 的代码里对其进行了一点封装，包括监听音频设备的变化、声音路由的切换与恢复等，在 `AppRTCAudioManager` 类里。

WebRTC 对 `AudioManager` 的主要调用代码如下：

```
audioManager.setMode(AudioManager.MODE_IN_COMMUNICATION);
audioManager.setSpeakerphoneOn(on);
```

## API 概览

### SDP 基本结构

WebRTC 的 SDP 分为几个部分：

- session metadata: v=, o=, s=, t=
- network description: c=, a=candidate
- stream description: m=, a=rtpmap, a=fmtp, a=sendrecv …
- security description: a=crypto, a=ice-frag, a=ice-pwd, a=fingerprint
- QoS, grouping description: a=rtcp-fb, a=group, a=rtcpmux

WebRTC 的 SDP 有三种类型：

- offer: 发起方提供的自己对本次通话的描述；
- answer: 其他方收到 offer 后，给出的回应；
- pranswer: provisional answer

### Plan B v.s. Unified Plan

Plan B 是 SDP 里同类型的媒体流只有一个 m line，同类型的多个媒体流之间通过 msid 区分，而 Unified Plan 则是每个媒体流都有一个 m line，因此如果有两路视频，那就会有两个 video m line。WebRTC 支持Unified Plan，Track/Transceiver API

### Capturer/Source/Track/Sink/Transceiver

- Capturer: 负责数据采集，只有视频才有这一层抽象，它有多种实现，相机采集（Android 还有 Camera1/Camera2 两套）、录屏采集、视频文件采集等；
- Source: 数据源；
- Track: 媒体数据交换的载体，发送端把本地的 Track 发送给远程的接收端，每个 Track 都有自己的 track id，多个关联的 Track 有一个相同的 stream id；
- Sink: Track 数据的消费者，只有视频才有这一层封装，发送端视频的本地预览、接收端收到远程视频后的渲染，都由 Sink 负责；
- Transceiver: 负责收发媒体数据；

### PeerConnectionFactory 接口

#### CreatePeerConnectionFactory

默认的编译选项里，`rtc_use_builtin_sw_codecs = false`，因此 `USE_BUILTIN_SW_CODECS` 未被定义，`CreatePeerConnectionFactory` 只有一个重载版本：接收三个 thread、adm、audio/video encoder/decoder factory、AudioMixer 和 AudioProcessing。

#### CreatePeerConnection

创建 PC 对象，接收 RTCConfiguration 和 PeerConnectionDependencies，前者用来容纳各种配置，后者则用来容纳各种可定制的接口实现，例如 PortAllocator, AsyncResolverFactory, RTCCertificateGeneratorInterface, SSLCertificateVerifier。

Android/iOS 对 dependencies 的支持还未跟上

### PeerConnection 接口

准备工作相关：

- AddTrack: 添加要发送的 track；
- AddTransceiver: 添加 transceiver；
- CreateDataChannel: 添加 DataChannel；
- RemoveTrack: 移除 track；
- GetTransceivers: 获取所有的 transceiver；

建立 P2P 连接相关：

- CreateOffer: 创建 offer；
- CreateAnswer: 创建 answer；
- SetLocalDescription: 设置本地 SDP；
- SetRemoteDescription: 设置远端 SDP；
- AddIceCandidate: 添加 ICE candidate；
- RemoveIceCandidates: 移除 ICE candidate；

其他接口：

- GetStats: 获取统计数据；
- SetBitrate: 设置这个 PC 总的发送码率，包括初始码率、最小码率、最大码率；
- SetBitrateAllocationStrategy: 设置自定义码率分配策略，可以通过这个接口实现针对每个 track 的码率分配策略；

回调接口 PeerConnectionObserver：

- OnSignalingChange: 产生/设置 SDP 后，会触发 signaling state 变化，常见的变化是 `stable -> have-local-offer -> stable` 或 `stable -> have-remote-offer -> stable`，具体可以查看 [SPEC 4.3 State Definitions](https://w3c.github.io/webrtc-pc/#state-definitions)；
- OnRenegotiationNeeded: 需要重新协商（重新建立 P2P 连接）时回调，例如 ICE restart 时会回调；
- OnIceGatheringChange: ICE candidate 收集状态变化后回调；
- OnIceConnectionChange: ICE 连接状态变化后回调；
- OnIceCandidate: 收集到本地 ICE candidate 后回调；
- OnIceCandidatesRemoved: 本地 ICE candidate 被移除后回调；
- OnTrack: 调用 SetRemoteDescription 后，如果 SDP 表明将会创建接收用的 transceiver，就会回调这个接口；
- OnRemoveTrack: 当确定一个 track 不再接收媒体数据后，会回调这个接口，track 不会移除，但 transceiver 的 recv 方向将会被去掉；

### RtpTransceiver

SDP 的 m section 里有一行 `a=mid:`，定义了这种 media 的 id，叫 mid。其中有三种 media: video, audio, application，mid 依次为 0, 1, 2。application 是 DataChannel 的 media type。

offer 和 answer 里同一种 media 的 mid 是相同的。transceiver 表示的就是收发相同 mid 的 sender 和 receiver 的一个组合体。其中 direction 有几种取值：kSendRecv, kSendOnly, kRecvOnly, kInactive。

transceiver 何时与 SDP 里的 m section 关联呢？offer 端在创建 offer 时，会根据已有的 transceiver 创建 m section，并记下每个 transceiver 在 SDP 里对应的 m section 的 index 值，以便在 SetLocalDescription 时，可以为 transceiver 设置正确的 mid；answer 端在 SetRemoteDescription（offer 端发来的 offer）时，如果 offer 里的 m section 有 recv 方向，那就按 media type 来查找已有的 transceiver，如果能找到就可以将其关联起来，否则就创建一个 kRecvOnly 的 transceiver

#### SDP 部分细节

- m line 里会指明传输协议，例如 UDP/TLS/RTP/SAVPF，最后的 SAVPF 还有其他几种值：AVP, SAVP, AVPF, SAVPF
  - AVP 意为 AV profile
  - S 意为 secure
  - F 意为 feedback
- rtpmap 是描述 codec 的，但有特殊的 rtx codec，其实不是 codec，例如 rtx；
- fmtp 补充描述 codec 的参数，format parameters
  - max-fr: maximum framerate
  - profile-level-id: H.264 的 profile level id
- rtx 描述重传策略，由 rtpmap 指明，它的参数由 fmtp 描述
  - apt: associated payload type，指明所描述的 stream；
  - rtx-time: rtp 包在缓冲区保留时间；
- rtcp-fb: RTCP 反馈机制
  - offer 里面列出一些反馈机制，answer 里应移除不理解、不支持的机制，但不能修改；
  - ack rpsi/app
  - nack pli/sli/rpsi/app
  - rpsi: reference picture selection indication
  - app: app 层反馈机制
  - pli: picture loss indication，表明收流端丢失了一幅图像的一些数据，发送端可能会发送一个 I 帧（类似于 FIR），但要考虑拥塞控制
  - sli: slice loss indication
  - ccm fir: codec control message, full intra refresh
- fec 类似于 rtx，也由 rtpmap 指明，它的参数由 fmtp 指明；

## RTP H.264 封包与解包

### RTP 包结构

包头有固定 12 个字节部分，以及可选的 `csrc` 和 `ext` 数据

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|V=2|P|X|  CC   |M|     PT      |       sequence number         |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                           timestamp                           |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           synchronization source (SSRC) identifier            |
+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+
|            contributing source (CSRC) identifiers             |
|                             ....                              |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

接着是载荷数据，载荷长度在包头中有记录。载荷数据的格式，由不同的 profile 单独定义，profile 的 payload type 值，通过 SDP 协商确定。

### H.264 载荷

H.264 载荷数据的第一个字节格式和 NAL 头一样

```
 Table 1.  Summary of NAL unit types and the corresponding packet
           types

 NAL Unit  Packet    Packet Type Name               Section
 Type      Type
 -------------------------------------------------------------
 0        reserved                                     -
 1-23     NAL unit  Single NAL unit packet             5.6
 24       STAP-A    Single-time aggregation packet     5.7.1
 25       STAP-B    Single-time aggregation packet     5.7.1
 26       MTAP16    Multi-time aggregation packet      5.7.2
 27       MTAP24    Multi-time aggregation packet      5.7.2
 28       FU-A      Fragmentation unit                 5.8
 29       FU-B      Fragmentation unit                 5.8
 30-31    reserved                                     -
```

H.264 载荷数据的封包有三种模式：Single NAL unit mode (0), Non-interleaved mode (1), Interleaved mode (2)。它们各自支持的 type 见下表：

```
Table 3.  Summary of allowed NAL unit types for each packetization
          mode (yes = allowed, no = disallowed, ig = ignore)
          
Payload Packet    Single NAL    Non-Interleaved    Interleaved
Type    Type      Unit Mode           Mode             Mode
-------------------------------------------------------------
0      reserved      ig               ig               ig
1-23   NAL unit     yes              yes               no
24     STAP-A        no              yes               no
25     STAP-B        no               no              yes
26     MTAP16        no               no              yes
27     MTAP24        no               no              yes
28     FU-A          no              yes              yes
29     FU-B          no               no              yes
30-31  reserved      ig               ig               ig
```

WebRTC 里实际使用的只有三种封包模式：NAL unit, STAP-A, FU-A。

#### NAL unit

如果 type 为 `[1, 23]`，则该 RTP 包只包含一个 NALU：

```
     0                   1                   2                   3
     0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |F|NRI|  Type   |                                               |
    +-+-+-+-+-+-+-+-+                                               |
    |                                                               |
    |               Bytes 2..n of a single NAL unit                 |
    |                                                               |
    |                               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                               :...OPTIONAL RTP padding        |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Figure 2.  RTP payload format for single NAL unit packet
```

#### 包聚合（Aggregation Packets）

为了体现/应对有线网络和无线网络的 MTU 巨大差异，RTP 协议定义了包聚合策略：

- STAP-A：聚合的 NALU 时间戳都一样，无 DON（decoding order number）；
- STAP-B：聚合的 NALU 时间戳都一样，有 DON；
- MTAP16：聚合的 NALU 时间戳不同，时间戳差值用 16 bit 记录；
- MTAP24：聚合的 NALU 时间戳不同，时间戳差值用 24 bit 记录；
- 包聚合时，RTP 的时间戳是所有 NALU 时间戳的最小值；

```
     0                   1                   2                   3
     0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |F|NRI|  Type   |                                               |
    +-+-+-+-+-+-+-+-+                                               |
    |                                                               |
    |             one or more aggregation units                     |
    |                                                               |
    |                               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                               :...OPTIONAL RTP padding        |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Figure 3.  RTP payload format for aggregation packets
```

#### 包拆分（Fragmentation Units，FUs）

在应用层实现包拆分而不是依赖下层网络的拆分机制，好处有二：

- 可以支持超过 64 KB（IPv4 包最大长度为 64 KB）的 NALU，高清视频文件可能有超大的 NALU；
- 可以利用 FEC（forward error correction）；

每个分包都有一个编号，一个 NALU 拆分的 RTP 包其序列必须顺序且连续，中间不得插入其他数据的 RTP 包序号。FU 只能拆分 NALU，STAP 和 MTAP 不能拆分，FU 也不能嵌套。FU-A 没有 DON，FU-B 有 DON。

FU-A 格式如下：

```
     0                   1                   2                   3
     0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    | FU indicator  |   FU header   |                               |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+                               |
    |                                                               |
    |                         FU payload                            |
    |                                                               |
    |                               +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                               :...OPTIONAL RTP padding        |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Figure 14.  RTP payload format for FU-A
```

FU header 格式如下：

```
      +---------------+
      |0|1|2|3|4|5|6|7|
      +-+-+-+-+-+-+-+-+
      |S|E|R|  Type   |
      +---------------+
```

- S: start bit, 置一表明这是 NALU 的首个 fragment；
- E: end bit, 置一表明是 NALU 的最后一个 fragment；
- R: reserved，必须置零；
- Type: 取值含义与 NALU header 的 type 字段一致；

### WebRTC H.264 封包实现

了解完了理论部分，接下来我们看看 WebRTC 里是如何实现的，WebRTC 把视频数据封装成 RTP packet 的逻辑在 `RTPSenderVideo::SendVideo` 函数中。

### `RTPSenderVideo::SendVideo`

其实封包的过程，就是计算一帧数据需要封多少个包、每个包放多少载荷，为此我们需要知道各种封包模式下，每个包的最大载荷（包大小减去头部大小）。

首先计算一个包的最大容量，这个容量是指可以用来容纳 RTP 头部和载荷的容量，FEC、重传的开销排除在外：

```
// Maximum size of packet including rtp headers.
// Extra space left in case packet will be resent using fec or rtx.
int packet_capacity = rtp_sender_->MaxRtpPacketSize() - fec_packet_overhead -
                    (rtp_sender_->RtxStatus() ? kRtxHeaderSize : 0);
```

`rtp_sender_->MaxRtpPacketSize` [默认会被设置为 1460](https://webrtc.googlesource.com/src/+/6714bf9f18f2514919f7b0cdc305107076cdc65b/modules/rtp_rtcp/source/rtp_rtcp_impl.cc#116)，[但如果需要发送视频，则会被设置为 1200](https://webrtc.googlesource.com/src/+/6714bf9f18f2514919f7b0cdc305107076cdc65b/media/engine/webrtcvideoengine.cc#1546)。（*why ???*）

接着准备四种包的模板：

- `single_packet`: 对应 NAL unit 和 STAP-A 的包；
- `first_packet`: 对应 FU-A 的首个包；
- `middle_packet`: 对应 FU-A 的中间包；
- `last_packet`: 对应 FU-A 的最后一个包；

准备过程包括：

- 在 `RTPSender::AllocatePacket` 里设置 `ssrc` 和 `csrcs` 字段，预留 `AbsoluteSendTime`, `TransmissionOffset` 和 `TransportSequenceNumber` extension 的空间，并且按需设置 `PlayoutDelayLimits` 和 `RtpMid` extension；
- 在 `RTPSenderVideo::SendVideo` 里设置 `payload_type`, `rtp_timestamp` 和 `capture_time_ms` 字段；
- 在 `AddRtpHeaderExtensions` 里按需设置 `VideoOrientation`, `VideoContentTypeExtension`, `VideoTimingExtension` 和 `RtpGenericFrameDescriptorExtension` extension；
- `first_packet`, `middle_packet` 和 `last_packet` 均是拷贝自 `single_packet`，因此代码里只调用了 `AddRtpHeaderExtensions` 设置它们的 extension；

这些模板一是后续封包时可以直接拿来用，二是可以准确地知道 RTP 头部需要多少空间，正如注释所言：

> Simplest way to estimate how much extensions would occupy is to set them.

知道了每种包的头部需要多少空间后，就知道每个包最多可以容纳多少载荷了（为 `RtpPacketizer::PayloadSizeLimits` 的各个字段赋值）：

- `max_payload_len`，最大载荷可用空间：包的最大容量减去中包头部大小；
- `single_packet_reduction_len`，封单包时，载荷可用空间还需要在 `max_payload_len` 的基础上打个折扣：单包与中包头部大小之差；即包的最大容量减去单包头部大小；
- `first_packet_reduction_len`，封多包时，首包载荷可用空间也需要在 `max_payload_len` 的基础上打个折扣：首包与中包头部大小之差；
- `last_packet_reduction_len`，封多包时，末包载荷可用空间也需要在 `max_payload_len` 的基础上打个折扣：末包与中包头部大小之差；

准备好了模板、知道了 limits 之后，就创建 `RtpPacketizer`，通过其 `NumPackets` 接口得知这一帧图像需要封装为多少个包，再调用其 `NextPacket` 封装每个包。调用 `NextPacket` 之后还不算完，还得调用 `RTPSender::AssignSequenceNumber` 分配序列号，如果需要设置 `VideoTimingExtension`，还得设置 `packetization_finish_time_ms`。最后，就是调用 FEC 处理，或直接调用 `RTPSenderVideo::SendVideoPacket` 发送 RTP 报文了。

视频编码为 H.264 时，`RtpPacketizer` 的实现类是 `RtpPacketizerH264`

### `RtpPacketizerH264`

`RtpPacketizerH264` 构造时会根据 `RTPFragmentationHeader` 的内容，生成 `RtpPacketizerH264::Fragment` 数组 `input_fragments_`，`Fragment` 里面包含了每个 NALU 载荷起始字节的指针、NALU 的长度。

`RTPFragmentationHeader` 其实就是这帧图像里面每个 NALU 的信息：载荷在 buffer 里的 offset、载荷长度。这些信息在编码器输出数据之后解析生成，扫描整个 buffer，查找 NALU start code（`001` 或 `0001`），统计每个 NALU 的 offset 和长度。安卓的实现在 `sdk/android/src/jni/videoencoderwrapper.cc` 的 `VideoEncoderWrapper::ParseFragmentationHeader` 中，iOS 的实现在 `sdk/objc/components/video_codec/nalu_rewriter.cc` 的 `H264CMSampleBufferToAnnexBBuffer` 中。

H.264 规范里定义了一幅图像分片为多个 NALU 的功能，但我观察了一下 iPhone 6 编出来的数据，非关键帧都只有一个 NALU，关键帧有两个 NALU，而且前面都添加了 SPS 和 PPS，所以关键帧会有四个 NALU。

有了 `input_fragments_` 后，就会在 `GeneratePackets` 中遍历之，对每个 `Fragment`，根据 `packetization_mode` 执行不同的封包逻辑：

- 如果是 `SingleNalUnit`，那就为这个 `Fragment`（其实就是一个 NALU）生成一个 `PacketUnit`；

- 如果是 `NonInterleaved`（WebRTC Native SDK 实际使用的 mode），那就看这个 `Fragment` 能否放进单个包里，先计算单个包能容纳多少数据：

  ```
    int single_packet_capacity = limits_.max_payload_len;
    if (input_fragments_.size() == 1)
        single_packet_capacity -= limits_.single_packet_reduction_len;
    else if (i == 0)
        single_packet_capacity -= limits_.first_packet_reduction_len;
    else if (i + 1 == input_fragments_.size())
        single_packet_capacity -= limits_.last_packet_reduction_len;
  ```

- 逻辑并不复杂，`max_payload_len` 扣除各种情况的折扣之后，剩下的就是 `single_packet_capacity`；

- 如果 `fragment_len > single_packet_capacity`，就说明无法放进单个包，那就要做 Fragmentation 了，即调用 `PacketizeFuA`，否则说明可以放进单个包，那就可以做 Aggregation，即调用 `PacketizeStapA`；

- `PacketizeFuA` 就是看怎么把一个 `Fragment` 分成多个包了，然后生成每个 `PacketUnit`，这个分的逻辑实现在 `SplitAboutEqually` 函数中，里面处理了不少边界情况，大体思想就是把数据放进尽可能少的包、每个包的大小尽可能相近；它生成的 `PacketUnit` 的 `aggregated` 字段都是 false；

- `PacketizeStapA` 则是看能把多少个 `Fragment` 放进一个包，这里也会为每个 `Fragment` 生成一个 `PacketUnit`，但只会对 `num_packets_left_` 做一次加一操作；它生成的 `PacketUnit` 除了最后一个的 `aggregated` 字段为 false，其他都为 true；

`GeneratePackets` 执行完毕后，就算出了 `num_packets_left_` 的值，即此帧图像需要多少个 RTP包，并且也准备好了 `PacketUnit` 数组。

之后在 `RTPSenderVideo::SendVideo` 里就会调用 `num_packets_left_` 次 `NextPacket` 来实际组装每一个 RTP 包了，我们现在就看看 `NextPacket` 的逻辑：

- 检查首个 `PacketUnit`：
- 如果 `PacketUnit` 的 `first_fragment` 和 `last_fragment` 字段都是 true，那就直接把载荷拷进去；
- 这种情况有可能是 `SingleNalUnit`，也有可能是 `NonInterleaved` 的 STAP-A 包，因为 `NonInterleaved` 时，如果 `Fragment` 可以放进一个包，那就会封为 STAP-A，而如果只生成了一个 `PacketUnit`，那它的 `first_fragment` 和 `last_fragment` 都会是 true；
- 否则，如果`aggregated`字段为 true，那就调用`NextAggregatePacket`封 STAP-A 包；
  - 这里只提一点我看了比较久才看清楚的逻辑：这个函数里通过一个循环不停的消费 `PacketUnit`，退出循环的条件是 `!packet->aggregated` 或 `packet->last_fragment`，由于需要放进一个包的一系列 `PacketUnit` 里只有最后一个 `last_fragment` 字段为 true（这个逻辑在 `PacketizeStapA` 里），因此可以正确退出循环；
- 如果 `aggregated` 字段为 false，就调用 `NextFragmentPacket` 封 FU-A 包；

### WebRTC H.264 解包实现

了解了封包的实现，我们接下来看看解包是怎么实现的，解包比封包稍微复杂一点，关键就在于包的到达可能是乱序的（丢包重传也可以认为是一种乱序）。

解包过程包括两大步：先解析出 RTP 的头部和载荷；再解析载荷部分，根据不同的封包模式，对封包过程做一个逆操作，就能得到一帧完整的数据。前者在 `Call::DeliverRtp` 中调用 `RtpPacket::ParseBuffer` 中实现，后者则比较复杂，因为需要处理乱序问题，逻辑起始点是 `RtpVideoStreamReceiver::ReceivePacket` 函数。

#### `RtpPacket::ParseBuffer`

`ParseBuffer` 的任务有三点：

- 解析 RTP 标准头的各个字段，包括 `payload_type_`, `sequence_number_`, `timestamp_`, `ssrc_` 等；
- 解析 RTP 扩展头的元数据，即偏移量和长度；
- 确定 RTP 载荷的偏移量和长度，完成了第二点后做个减法就可以得到；

#### `RtpVideoStreamReceiver::ReceivePacket`

首先根据不同的 payload type，创建不同的 `RtpDepacketizer` 去解析载荷内容，H.264 的解析逻辑在 `RtpDepacketizerH264::Parse` 中实现，其主要任务就是找到实际数据的位置和大小：

- 检查载荷的第一个字节里的 type 字段（低五位），以判断包类型（NAL unit, STAP-A, FU-A）；
- FU-A 的解析在 `ParseFuaNalu` 里完成；
- NAL unit 和 STAP-A 的解析在 `ProcessStapAOrSingleNalu` 里完成；
- *具体解析代码这里不做展开；*

然后解析 RTP 扩展头的实际数据，包括 `VideoOrientation` 等。

最后构造 `VCMPacket`，并调用 `PacketBuffer::InsertPacket` 放入包缓冲区中。

#### `PacketBuffer::InsertPacket`

`PacketBuffer` 封装了 RTP 包处理乱序到达的逻辑，大体思路就是：

- 收到每个包之后，检查序列号：
  - 确定已经收到过的包，就会直接丢弃；
  - 否则就把包放进 `data_buffer_` 数组里，并在 `sequence_buffer_` 数组里记下这个序列号的一些属性；
  - 每个包在上述两个数组里存放的下标是序列号模以数组大小，因此是按序列号顺序存放的；
- 调用 `FindFrames`，从已收到的包列表中，找出完整的帧；
- 把完整的帧交给 `RtpFrameReferenceFinder::ManageFrame`，由其确保帧可以解码后，再回调出去，进入后续的解码环节；

#### `PacketBuffer::FindFrames`

每次收到包后，会触发 `FindFrames`，我们会从刚收到的包的序列号向后查找：

- 只有一个包满足以下两个条件之一才会进行检查：
  - 该包是「帧起始」包；
  - 该包前一个序号的包是连续的，何谓连续？就是说它是帧起始包，或它之前的所有序列号都已经收到了；
  - 举个例子，假设 1 是帧起始包，那收到 1 的时候肯定会检查，之后收到 2 时，由于 1 是连续的，所以 2 也会检查，但如果收到 4（假设 4 不是帧起始），那 4 就不会检查，再收到 3 时，就会依次检查 3 和 4；
- 我们首先感兴趣的是「帧末尾」包，即有 `packet->is_last_packet_in_frame` 标志；
- 找到帧末尾包后，再反过来向前查找「帧起始」包；
  - VP8/VP9 靠 `frame_begin`（即 `packet->is_first_packet_in_frame`）标志判断帧起始，H.264 则靠时间戳的变化来判断帧起始；
  - 正常情况下，由于我们只检查帧起始包和连续包，所以一旦找到了帧末尾包，向前就一定能找到帧起始包；
- 找到了帧末尾包和帧起始包，就可以构造完整的帧了，不过这里只是记录元数据，不会做帧数据的拷贝；

#### `RtpFrameReferenceFinder::ManageFrame`

从载荷里解析出来的帧数据都是完整的帧，但不一定能解码，比如 H.264 有前向参考（P 帧需要参考前面的 I 帧才能解码），也有后向参考（B 帧需要参考前面的 I/P 帧和后面的 P 帧才能解码），因此需要等这一帧的参考帧都收到之后，才能回调出去。

虽然 `PacketBuffer` 处理了 RTP 报文乱序到达的问题，输出了一个个完整的帧，但并没有保证帧是按序到达的，所以仍需 `RtpFrameReferenceFinder` 来处理帧乱序到达的问题。

*`RtpFrameReferenceFinder` 的代码细节这里就不展开了，有兴趣/需求的朋友可以自行阅读。

### WebRTC RTP 封包解包相关数据结构

最后，我们再总结一下 WebRTC RTP 封包解包相关数据结构：

- `RtpPacket`: RTP 报文的数据结构，里面定义了各种标准头部字段、扩展头部、数据缓冲区等；
- `RtpPacketToSend`: 发送端封包用到的数据结构，继承自 `RtpPacket`，加了一些扩展头部设置逻辑的封装；
- `RtpPacketReceived`: 接收端解包用到的数据结构，也继承自 `RtpPacket`，加了获取扩展头部逻辑的封装；

### 序列号的比较算法

由于序列号可能发生回绕，所以不能直接比较，有一个 RFC 文档专门定义了这个比较算法：[Serial Number Arithmetic](https://tools.ietf.org/html/rfc1982)。

这个 RFC 里首先定义了序列号的定义法：n 位无符号数，最低序列号为 0，最高序列号为 `2^n-1`，序列号没有最大最小值，每个序列号至少需要 n 位来保存。

接着它定义了序列号的加法：在 `[0, 2^n-1]` 范围内的合法序列号值 s，加 m 的值为 `(s+m) % (2^n)`，这里的加法和取模，都是常规定义的加法和取模。

最后它定义了序列号的比较算法（RFC 里为了严谨，引入了另外两个普通正整数，这里简单起见我们就不引入了）：

- 判等：序列号 `s` 和 `s+m`（`m` 为普通正整数），只有 `m` 为 0 时，它们才相等；即给定两个序列号值，完全无法判断其是否相等，不过通常我们不需要判等，而是判断大小；
- 判小：当且仅当 `(s1 < s2 && s2 - s1 < 2^(n-1)) || (s1 > s2 && s1 - s2 > 2^(n-1))` 时，序列号 `s1` 小于 `s2`；即值小不过一半范围，或大过一半范围，例如 n=3，`2-1 < 4`，故 1 比 2 小，`7-2 > 4`，故 7 比 2 小；
- 判大：当且仅当 `(s1 < s2 && s2 - s1 > 2^(n-1)) || (s1 > s2 && s1 - s2 < 2^(n-1))` 时，序列号 `s1` 大于 `s2`；即值小过一半范围，或大不过一半范围，例如 n=3，`7-2 > 4`，故 2 比 7 大，`2-1 < 4`，故 2 比 1 大；

细心的朋友也许会举出一个例子：7 和 3 谁大谁小？它们其实无法区分大小。就像 3 和 3 是否相等一样，无法区分。RFC 里故意不对这种序列号对的大小问题作出定义，因为着实不好定义。

WebRTC 的实现逻辑主要在 `rtc_base/numerics/sequence_number_util.h` 和 `rtc_base/numerics/mod_ops.h` 中：

```c++
template <typename T, T M = 0>
inline bool AheadOf(T a, T b) {
  static_assert(std::is_unsigned<T>::value,
                "Type must be an unsigned integer.");
  return a != b && AheadOrAt<T, M>(a, b);
}

template <typename T, T M>
inline typename std::enable_if<(M == 0), bool>::type AheadOrAt(T a, T b) {
  static_assert(std::is_unsigned<T>::value,
                "Type must be an unsigned integer.");
  const T maxDist = std::numeric_limits<T>::max() / 2 + T(1);
  if (a - b == maxDist)
    return b < a;
  return ForwardDiff(b, a) < maxDist;
}

template <typename T, T M>
inline typename std::enable_if<(M == 0), T>::type ForwardDiff(T a, T b) {
  static_assert(std::is_unsigned<T>::value,
                "Type must be an unsigned integer.");
  return b - a;
}
```

- 首先序列号必须是无符号数；
- 然后 WebRTC 定义了「前向距离」这个概念，即后数加多少能加到前数（考虑无符号数的溢出）；
- 还定义了「最大距离」这个概念，可以理解为两个数之差的绝对值的最大可能取值，也就是最大取值范围的一半（向上取整）；
- 最后，a 领先于 b 的的条件就是：若 ab 前向距离为最大距离，那 a 大于 b 就是领先于 b，否则，若 ab 前向距离小于最大距离，那 a 就领先于 b；

*其实就是通过无符号数减法的溢出，把 RFC 定义的两种或起来的情况统一了，以及对于 RFC 未定义的情况，定义成了值大小的比较*。







相关文章：

1.[视频压缩编码和音频压缩编码的基本原理](https://blog.csdn.net/leixiaohua1020/article/details/28114081)

2.[Interactive Connectivity Establishment (ICE)](https://tools.ietf.org/html/rfc5245)。

3.[Plan B](https://webrtcglossary.com/plan-b), [Unified Plan](https://webrtcglossary.com/unified-plan), [Unified Plan vs Plan B](https://www.douban.com/note/658626071)。